{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m34bnWEjBf4u"
   },
   "source": [
    "# Self-study try-it activity 23.1: Data processing for PCA\n",
    "\n",
    "PCA is a dimensionality reduction technique that captures the highest possible variance from a set number of dimensions. Additionally, it will find the projection that minimises the sum of the squared distances between the original and the projected data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VRZR3M2bO6rw"
   },
   "outputs": [],
   "source": [
    "#Import the necessary libraries\n",
    "import numpy.random as rand\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LeczTxO8PIV6"
   },
   "source": [
    "Below is a simulated data set (shown in blue) that has been projected onto four different components (shown in orange).\n",
    "\n",
    "Which of these projections maximises the variance of the data that is kept and is likely to be the principal component?\n",
    "\n",
    "This activity helps you visually identify the direction that captures the most variance – an essential concept in understanding how PCA selects principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1098
    },
    "id": "RAnBLopvBf4w",
    "outputId": "20938f2d-0411-4ca1-fdf8-d7448daebc61"
   },
   "outputs": [],
   "source": [
    "rand.seed(1234)\n",
    "\n",
    "x1 = 3 * np.outer(rand.normal(0,1,50), np.array([1,-1]))\n",
    "x2 = rand.multivariate_normal([0,0],np.diag([0.1,0.1]), size = 50)\n",
    "\n",
    "X = x1 + x2\n",
    "\n",
    "projection = np.array((1,0)).reshape((2,1))\n",
    "\n",
    "R = np.matrix([[1/np.sqrt(2), -1/np.sqrt(2)], [1/np.sqrt(2), 1/np.sqrt(2)]])\n",
    "\n",
    "fig, axs = plt.subplots(2,2, figsize = (25,25))\n",
    "for i in range(4):\n",
    "    axs[i//2, i % 2 ].set_xlim([-8,8])\n",
    "    axs[i//2, i % 2 ].set_ylim([-8,8])\n",
    "    axs[i//2, i % 2 ].scatter(X[:,0], X[:,1], alpha = 0.7)\n",
    "    axs[i//2, i % 2 ].title.set_text(\"Projection %s\" % (i+1))\n",
    "    axs[i//2, i % 2 ].title.set_size(20)\n",
    "    scores = X @ projection\n",
    "    Xi = np.outer(scores, projection)\n",
    "    axs[i//2, i % 2 ].scatter(Xi[:,0],Xi[:,1])\n",
    "    projection = R @ projection\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-bODmzLPgue"
   },
   "source": [
    " ### To-do 1:\n",
    "\n",
    " Which of these projections maximises the variance of the data that is kept and is likely to be the principal component?\n",
    " \n",
    "* Projection 1\n",
    "* Projection 2\n",
    "* Projection 3\n",
    "* Projection 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-GTci6CRNmD"
   },
   "source": [
    "### Answer:\n",
    "The projection that retains the largest spread of data along its axis is the one that maximises the variance of the data, and that is likely to be the principal axis. In this case, it is projection 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iREf-3xKSy6C"
   },
   "source": [
    "## Dimensionality deduction with PCA\n",
    "\n",
    "PCA is commonly used to reduce the dimensionality of high-dimensional data sets by selecting a subset of components that retain most of the original variance. This technique is particularly valuable when preparing data for machine learning models, as it can improve training efficiency and model generalisation.\n",
    "\n",
    "In this notebook, you will work with a data set comprising S&P 500 stock prices over the past five years. Given the high degree of correlation among stocks in the market, retaining all individual stock features may introduce redundancy and unnecessarily increase model complexity and training time.\n",
    "\n",
    "Instead, PCA allows you to project the data onto a smaller set of orthogonal components that capture the majority of the variance. This approach simplifies the feature space while preserving the underlying structure, making it a more effective input for downstream modelling tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "53Hr8f3tBf4x",
    "outputId": "508fe1da-2a57-4949-a9e9-afbd2962b145"
   },
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"data/all_stocks_5yr.csv\")\n",
    "\n",
    "names = df[\"Name\"].unique()\n",
    "range(len(df[ df[\"Name\"] == names[0]][\"close\"]))\n",
    "l = len(df[ df[\"Name\"] == names[0]][\"close\"])\n",
    "data = pd.DataFrame(index = range(l), columns= names)\n",
    "\n",
    "for name in names:\n",
    "    x = df[ df[\"Name\"] == name][\"close\"]\n",
    "    if x.isnull().any() or len(x) != l:\n",
    "        data = data.drop(columns= name)\n",
    "    else:\n",
    "        data[name] = np.array(x)\n",
    "\n",
    "data.head()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qk9T-vOQTu1y"
   },
   "source": [
    "The above code cell loads five years of S&P 500 stock data and constructs a clean data frame of closing prices, excluding stocks with missing values or inconsistent time series lengths. The result is a well-aligned data set suitable for analysis or modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPmejub7Wqkg"
   },
   "source": [
    "## Interpreting the scree plot\n",
    "\n",
    "The scree plot below displays the proportion of variance explained by each of the first ten principal components derived from the PCA of the stock data set. This visualisation is a key tool for determining the optimal number of components to retain.\n",
    "\n",
    "Your task is to analyse the scree plot and decide how many principal components you would keep to effectively represent the data set while minimising dimensionality. Consider both the individual and cumulative variance explained and identify the 'elbow point' – the point beyond which additional components contribute marginally to the total variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9MuyPwRBf4x"
   },
   "source": [
    "### To-do 2:\n",
    "\n",
    "Based on the scree plot of the first ten principal components from the PCA and any additional analysis you consider appropriate, how many components would you retain to effectively represent this data set, and what is your rationale for that choice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1566
    },
    "id": "VJYeqOaLBf4y",
    "outputId": "7eed1beb-5c49-42cf-ff00-b20bb15e2cf2"
   },
   "outputs": [],
   "source": [
    "pca = PCA(10).fit(data)\n",
    "var = pca.explained_variance_ratio_\n",
    "var_explained = np.zeros(10)\n",
    "for i in range(10):\n",
    "    var_explained[i] = sum(var[:i+1])\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"\\nComponent\", i+1 , \"\\nFraction of total variance explained by this variable:\", var[i],\n",
    "            \"\\n Total fraction of variance explained by the first %s variable(s):\" % (i +1), var_explained[i] )\n",
    "\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.plot(range(1, len(var_explained)+1), var, label = \"variance of i th component\", marker = \"o\")\n",
    "plt.plot(range(1, len(var_explained)+1), var_explained, label = \"variance explained by the first i components\", marker = \"o\")\n",
    "plt.xlabel(\"component i\")\n",
    "plt.ylabel(\"variance\")\n",
    "plt.title(\"Scree plot for S&P500 stocks\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOzCcQQVsbNj"
   },
   "source": [
    "### Answer:\n",
    "\n",
    "Based on the scree plot and variance output from PCA, retaining the first three principal components to represent this data set would be the right choice. This is because:\n",
    "\n",
    "- Component 1 alone explains approximately 78.4 per cent of the total variance, indicating a dominant underlying structure in the data.\n",
    "\n",
    "- Component 2 adds another 9.2 per cent, bringing the cumulative variance to 87.5 per cent.\n",
    "\n",
    "- Component 3 contributes 5.4 per cent, raising the total to 92.96 per cent.\n",
    "\n",
    "Together, these three components capture nearly 93 per cent of the total variance, which is a strong threshold for dimensionality reduction. Beyond this point, each additional component contributes marginally (less than 3 percent combined for components 4–10), suggesting diminishing returns.\n",
    "\n",
    "Retaining three components strikes a balance between model simplicity and information preservation, reducing computational overhead while maintaining the integrity of the data set’s structure. This choice is especially appropriate for highly correlated financial data, where a few latent factors often drive most of the variation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "StjCk1AwtLAJ"
   },
   "source": [
    "## Pre-processing the data\n",
    "\n",
    "When analysing which stocks contribute most to the principal components, it's important to consider how the data was pre-processed. If you apply PCA directly to the raw closing prices, stocks with higher average prices dominate the principal components in raw data because PCA is sensitive to scale. This can lead to misleading interpretations, as variance may reflect price magnitude rather than meaningful movement.\n",
    "\n",
    "In most financial analyses, you are more interested in **relative variability**, which means how much a stock's price fluctuates in proportion to its own value rather than its absolute magnitude. A high variance driven by a large price scale does not necessarily indicate meaningful movement or volatility.\n",
    "\n",
    "To ensure a fair and interpretable PCA, you standardise the data so that each stock has a **mean of zero** and a **variance of one**. This transformation allows PCA to focus on patterns of variation that are comparable across stocks, regardless of their original price levels, leading to more meaningful insights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M4kDAYJZuKQi"
   },
   "source": [
    "## Re-evaluating component selection\n",
    "\n",
    "### To-do 3:\n",
    "\n",
    "Examine the scree plot provided below. Based on its structure and the variance explained by each principal component, can you reasonably assume that the same number of components should be retained as in your previous PCA? Consider whether the distribution of variance has changed and justify your conclusion with reference to the plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(data)\n",
    "\n",
    "\n",
    "data_scaled = scaler.transform(data)\n",
    "\n",
    "#Number of component guards (optional but safer)\n",
    "n_comp = min(10, data_scaled.shape[1])\n",
    "pca_scaled = PCA(n_comp).fit(data_scaled)\n",
    "var_scaled = pca_scaled.explained_variance_ratio_\n",
    "var_scaled_explained = np.cumsum(var_scaled)\n",
    "\n",
    "#Print the scaled values you just computed\n",
    "for i in range(n_comp):\n",
    "    print(\n",
    "            f\"\\nComponent {i+1}\"\n",
    "            f\"\\nFraction of total variance explained by this component:{var_scaled[i]:.4f}\"\n",
    "            f\"\\nTotal fraction of variance explained by the first {i+1} component(s): {var_scaled_explained[i]:.4f}\"\n",
    "        )\n",
    "#Plot using the scaled arrays for both series and axis length\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(range(1, n_comp+1), var_scaled, marker=\"o\", label=\" Explained variance ratio (component i)\")\n",
    "plt.plot(range(1, n_comp+1), var_scaled_explained, marker=\"o\", label=\"Cumulative explained variance\")\n",
    "plt.xlabel(\"component \")\n",
    "plt.ylabel(\"variance ratio\")\n",
    "plt.title(\"Scree plot (standardized data, S&P 500 stocks)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFdB7_tIu0zP"
   },
   "source": [
    "### Answer:\n",
    "\n",
    "No, it is not necessarily safe to assume that the same number of components should be retained as in the previous PCA analysis.\n",
    "\n",
    "This is because, although the variance distribution across components appears similar to the earlier output, this result is based on standardised data, where each feature has been scaled to have a mean of zero and a variance of one. This pre-processing step ensures that the PCA reflects relative variability rather than being dominated by features with larger absolute values.\n",
    "\n",
    "In this standardised context:\n",
    "\n",
    "- The first component still explains a substantial 78.4 per cent of the variance.\n",
    "\n",
    "- The first three components together explain approximately 93 per cent, which is a strong threshold for dimensionality reduction.\n",
    "\n",
    "- Additional components beyond the third contribute marginally to the total variance (less than 2.5 per cent combined for components 4–10).\n",
    "\n",
    "Therefore, while the number of components to retain may coincide with the previous analysis (i.e. three components), this decision is now better justified because the PCA is based on scaled data, making the interpretation more meaningful and fair across all features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbHFrh2ivx9s"
   },
   "source": [
    "## Comparing PCA results: raw vs pre-processed data\n",
    "\n",
    "Applying PCA to raw versus pre-processed data can yield significantly different principal components. This distinction is illustrated below through two key observations:\n",
    "\n",
    "1. **Top contributing stocks**:  \n",
    "   When you examine the five stocks that contribute most to the first principal component, we find that the results differ between the raw and standardised data sets. This indicates that the PCA model identifies different sources of variance depending on whether the data has been scaled.\n",
    "\n",
    "2. **Score distribution across components**:  \n",
    "   The plots of scores along the first two principal components may appear visually similar, but the underlying scales differ substantially. In the standardised data, features are adjusted to have equal variance, which can amplify or compress certain patterns. This transformation leads to a more balanced and interpretable representation of the data structure.\n",
    "\n",
    "These differences highlight the importance of pre-processing when applying PCA, especially in contexts where feature magnitudes vary widely.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3eSHkfIGwVpC"
   },
   "source": [
    "### To-do 4:\n",
    "\n",
    "Imagine you are an investor planning to allocate capital across S&P 500 stocks based on their weights in the first principal component derived from historical data. However, recognising the potential risk due to high variance in returns, you decide to mitigate this by splitting your investment: half based on the first principal component and half based on the second, which is uncorrelated with the first and exhibits lower variance.\n",
    "\n",
    "Given this strategy, would it be more appropriate to derive the principal components from the **raw data** or from **pre-processed (standardized) data**? Justify your answer regarding interpretability, risk, and the nature of PCA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 651
    },
    "id": "yCSfah1FBf4z",
    "outputId": "65f0d199-a726-42f7-83c5-1421bb4e105e"
   },
   "outputs": [],
   "source": [
    "stocks = data.columns[np.argsort(pca.components_[0])[-1:-6:-1]]\n",
    "\n",
    "print(\"Largest 5 contributors to the principal component of unpreprocessed data:\", stocks)\n",
    "\n",
    "stocks_scaled = data.columns[np.argsort(pca_scaled.components_[0])[-1:-6:-1]]\n",
    "\n",
    "print(\"Largest 5 contributors to the principal component of preprocessed data:\", stocks_scaled)\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize = (20,10))\n",
    "axs[0].plot(pca.transform(data)[:,0],pca.transform(data)[:,1])\n",
    "axs[0].scatter(pca.transform(data)[:,0],pca.transform(data)[:,1], c = np.arange(l)/l)\n",
    "axs[1].plot(pca_scaled.transform(data_scaled)[:,0],pca_scaled.transform(data_scaled)[:,1])\n",
    "axs[1].scatter(pca_scaled.transform(data_scaled)[:,0],pca_scaled.transform(data_scaled)[:,1], c = np.arange(l)/l)\n",
    "axs[0].set_title(\"Raw Data\")\n",
    "axs[1].set_title(\"Pre-processed Data\")\n",
    "axs[0].set_xlabel(\"Scores along the first principal component \")\n",
    "axs[0].set_ylabel(\"Scores along the second principal component \")\n",
    "axs[1].set_xlabel(\"Scores along the first principal component \")\n",
    "axs[1].set_ylabel(\"Scores along the secondary principal component \")\n",
    "plt.show()\n",
    "print(np.mean(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QaH6Ljzryj0x"
   },
   "source": [
    "### Answer:\n",
    "\n",
    "When you apply PCA to stock data without pre-processing, the results are influenced mainly by stocks with high price values and not necessarily those that behave interestingly or vary substantially. For example, expensive stocks such as AMZN or GOOGL might dominate the analysis because their prices are large, not because they're more volatile or informative.\n",
    "\n",
    "However, in finance, you usually care more about how much a stock moves relative to its value and not just its raw price. That's why you standardise the data before applying PCA. This means adjusting all stocks so they have the same average and spread, allowing PCA to focus on actual movement patterns, not just price scale.\n",
    "\n",
    "So if you're using PCA to guide investment decisions, such as choosing stocks based on how they contribute to principal components, then pre-processing is essential. It gives you a fair comparison across all stocks and helps you make decisions based on meaningful variation, not just size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "interpreter": {
   "hash": "7812ea015bdcee6f23a998adcdd2ef97c151c0c241b7b7070987d9313e41299d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
