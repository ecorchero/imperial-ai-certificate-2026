{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLON2gz8Eibi"
   },
   "source": [
    "# Self-study try-it activity 24.1: Grid world experiment in Python\n",
    "\n",
    "A grid world is a simplified environment used to study how intelligent agents make decisions over time. It’s a grid-based map where each cell represents a state, and the agent can move in four directions: up, down, left, or right. Some cells offer rewards, others penalties, and some are terminal states where the episode ends.\n",
    "\n",
    "This set-up is ideal for understanding key concepts in Markov decision processes (MDPs) and reinforcement learning, such as:\n",
    "\n",
    "- Value iteration and policy iteration\n",
    "\n",
    "- Discounting future rewards\n",
    "\n",
    "- Stochastic transitions (noise)\n",
    "\n",
    "- Optimal policy computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "XEb7_NnpG3li",
    "outputId": "22686972-5071-408d-8d61-47180138f3b5"
   },
   "outputs": [],
   "source": [
    "#Install aima3 if it hasn't been installed initially\n",
    "!pip install aima3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "r6GKGe7VHTUh",
    "outputId": "ed4e9958-a995-4fce-9f95-3e828d1a0ae2"
   },
   "outputs": [],
   "source": [
    "#Import the necessary libraries\n",
    "import aima3\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import inspect\n",
    "import aima3.mdp\n",
    "\n",
    "print(inspect.getfile(aima3.mdp.value_iteration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hswGOhosHdzy"
   },
   "outputs": [],
   "source": [
    "from aima3 import mdp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2aFTfz5oPXea"
   },
   "source": [
    "A grid world environment is created using the aima3 library to model sequential decision-making. The agent navigates a 3 × 4 grid, aiming to maximise cumulative rewards while avoiding penalties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Co3CuDZSHikk"
   },
   "outputs": [],
   "source": [
    "#Sequencial decision environment\n",
    "grid_world = mdp.GridMDP([[-0.04, -0.04, -0.04, +1],\n",
    "                          [-0.04, None, -0.04, -1],\n",
    "                          [-0.04, -0.04, -0.04, -0.04]],\n",
    "                          terminals=[(3, 2), (3, 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bdlgfoZP1FG"
   },
   "source": [
    "Once the grid world environment is defined, you can apply two classic algorithms to compute optimal strategies for the agent.\n",
    "\n",
    "Value iteration computes the value of each state by iteratively updating expected rewards. Policy iteration computes the optimal policy, which is the best action to take in each state. These methods allow the agent to make informed decisions that maximize cumulative rewards while navigating the grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RpQ5BFBYH4qh"
   },
   "outputs": [],
   "source": [
    "values = mdp.value_iteration(grid_world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q5_pQs56H57t"
   },
   "outputs": [],
   "source": [
    "policy = mdp.policy_iteration(grid_world)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kymQnBkXVmKT"
   },
   "source": [
    "When working with Grid World environments, the results of value_iteration and policy_iteration are stored as dictionaries. To visualize these results as a grid, we need to convert them into a matrix format that matches the layout of the grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sV6zN2XZH9BD"
   },
   "outputs": [],
   "source": [
    "def convert_to_grid(policy, base_grid, dtype=float):\n",
    "    grid_shape = n, m = len(base_grid), len(base_grid[0]) #Corrected grid_shape calculation\n",
    "    mat = np.full(grid_shape, fill_value=np.nan).astype(dtype)\n",
    "    for k, v in policy.items():\n",
    "        #Adjust key indexing for grid representation from (column, row) to (row, column)\n",
    "        mat[k[1], k[0]] = v #Keep indexing as (row, col)\n",
    "    #Rotate the grid to match the visual representation\n",
    "    return np.rot90(mat, k=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1cC1aMEV7d5"
   },
   "source": [
    "After computing the values or policy for a grid world, it's helpful to visualise them as a heat map. The `plot_grid()` function uses Matplotlib to display a grid with colour-coded values and numeric labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wFTbD1qpIBRa"
   },
   "outputs": [],
   "source": [
    "def plot_grid(grid):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.axis('off')\n",
    "    img = ax.imshow(grid, cmap=plt.get_cmap(\"viridis\"), animated=True)\n",
    "    for (i, j), z in np.ndenumerate(grid):\n",
    "        ax.text(j, i, '{:0.2f}'.format(z), ha='center', va='center')\n",
    "    cbar = fig.colorbar(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qlQUJKuhWWlV"
   },
   "source": [
    "After computing the state values using value iteration, you convert the resulting dictionary into a 2D grid format for visualisation. This helps align the values with the spatial layout of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qyNXgj-SIFe3"
   },
   "outputs": [],
   "source": [
    "values_mat = convert_to_grid(values, grid_world.grid, dtype=float);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420
    },
    "id": "ZoLcjgT7IJNY",
    "outputId": "f446f73a-b411-4a6b-d46c-89d97e3a1c5e"
   },
   "outputs": [],
   "source": [
    "plot_grid(values_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2iYRKbsWpJN"
   },
   "source": [
    "After computing the optimal policy using policy iteration, let's convert the result into a 2D grid layout that matches the environment. This makes it easier to interpret and visualize the agent’s recommended actions for each state.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "CMb_YvGaIRMa",
    "outputId": "2bb33c02-e128-4112-de7b-ad78f35676f2"
   },
   "outputs": [],
   "source": [
    "policy_mat = convert_to_grid(policy, grid_world.grid, dtype=object)\n",
    "policy_mat.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juWC751GT7E2"
   },
   "source": [
    "Recall that your grid conventions are:\n",
    "- Move up: `(0, 1)`\n",
    "- Move down: `(0, -1)`\n",
    "- Move left: `(-1, 0)`\n",
    "- Move right: `(1, 0)`\n",
    "- Do nothing: `None`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sDXq8r_gIS8t"
   },
   "outputs": [],
   "source": [
    "from aima3.mdp import value_iteration\n",
    "from IPython.core.getipython import get_ipython\n",
    "get_ipython().run_line_magic('psource', 'value_iteration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WwSEhyuTXDjF"
   },
   "source": [
    "### To-do 1:\n",
    "\n",
    "-  Define the cliff world MDP, which is a 4 × 12 grid with rewards of:\n",
    "\t- `−1` for each step\n",
    "\t- `−100` for falling off the cliff (states in the third row, excluding the start and goal)\n",
    "\t- `+100` for reaching the goal\n",
    "- Provide a start state of `(0, 2)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1409e3c6"
   },
   "outputs": [],
   "source": [
    "#Write your code here\n",
    "cliff_world_big = mdp.GridMDP(\n",
    "    [[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "     [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "     [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 100]],\n",
    "    terminals=[(11, 2)],\n",
    "    init=(0, 2)  #Start state defined\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOfAbJTlJero"
   },
   "source": [
    "Once the cliff world environment is defined, you use value iteration to compute the optimal value for each state. This tells you how good it is for the agent to be in a particular state, assuming it follows the best possible policy.\n",
    "\n",
    "- Use `mdp.value_iteration()` to compute `cliff_values_big`.\n",
    "- Use `mdp.policy_iteration()` to compute `policy_values_big`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lo2hA_geU3sD"
   },
   "outputs": [],
   "source": [
    "#Write your code here\n",
    "cliff_values_big = mdp.value_iteration(cliff_world_big)\n",
    "policy_values_big = mdp.policy_iteration(cliff_world_big)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZwRgke7bcI-"
   },
   "source": [
    "### To-do 2:\n",
    "\n",
    "- Use the `convert_to_grid()` to convert the result into a 2D grid layout that matches the environment.\n",
    "- Assign it to `values_mat_big`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R-wpSAzmbTDI"
   },
   "outputs": [],
   "source": [
    "#Write your code here\n",
    "values_mat_big = convert_to_grid(values, cliff_world_big.grid, dtype=float);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRRPrhQwoTVn"
   },
   "source": [
    "## Part 1: varying discount rates\n",
    "\n",
    "You’ll test how different discount rates affect the agent’s trajectory. Try `gamma = [0.9, 0.95, 0.99]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "nfbhcqS9n5vw",
    "outputId": "0f00f7c1-7e00-48af-ee86-8c9e2f72bafc"
   },
   "outputs": [],
   "source": [
    "gammas = [0.9, 0.95, 0.99]\n",
    "for gamma in gammas:\n",
    "    cliff_world_big.gamma = gamma #Corrected line\n",
    "    values = mdp.value_iteration(cliff_world_big)\n",
    "    policy = mdp.best_policy(cliff_world_big, values)\n",
    "    print(f\"\\nDiscount Rate: {gamma}\")\n",
    "    print(cliff_world_big.to_arrows(policy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Woi4YWRYheGf"
   },
   "source": [
    "The discount factor(gamma) `γ` determines how much the agent values future rewards compared to immediate rewards.\n",
    "\n",
    "- A lower `γ`(e.g. 0.9) makes the agent more short-sighted, prioritising immediate gains.\n",
    "\n",
    "- A higher `γ` (e.g. 0.99) makes the agent more far-sighted, planning for long-term rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2L7aR60o1gW"
   },
   "source": [
    "## Part 2: adding noise to transitions\n",
    "\n",
    "In real-world environments, actions may not always lead to predictable outcomes. To simulate this uncertainty, let's define a custom MDP class called NoisyCliffMDP, which adds action noise to the cliff world setup. We’ll simulate stochastic transitions by modifying the transition model to include noise. Try noise levels of 0.1 and 0.25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "i7HyhHhOo9Kh",
    "outputId": "d17b7f87-2c05-40ff-b469-0448ac4c381a"
   },
   "outputs": [],
   "source": [
    "class NoisyCliffMDP(mdp.GridMDP):\n",
    "    def __init__(self, grid, terminals, init=(0, 0), gamma=0.9, noise=0.1):\n",
    "        self.noise = noise\n",
    "        #Pass the underlying grid (list of lists) to the superclass constructor\n",
    "        super().__init__(grid.grid, terminals, init, gamma)\n",
    "\n",
    "\n",
    "    def T(self, state, action):\n",
    "        if action is None:\n",
    "            return [(0.0, state)]\n",
    "        return [(1 - self.noise, self.go(state, action)),\n",
    "                (self.noise / 2, self.go(state, mdp.turn_right(action))),\n",
    "                (self.noise / 2, self.go(state, mdp.turn_left(action)))]\n",
    "\n",
    "noise_levels = [0.1, 0.25]\n",
    "for noise in noise_levels:\n",
    "    #Pass the grid from cliff_world_big, not the GridMDP object itself\n",
    "    noisy_mdp = NoisyCliffMDP(cliff_world_big, terminals=[(11, 2)], noise=noise)\n",
    "    values = mdp.value_iteration(noisy_mdp)\n",
    "    policy = mdp.best_policy(noisy_mdp, values)\n",
    "    print(f\"\\nNoise Level: {noise}\")\n",
    "    print(noisy_mdp.to_arrows(policy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i4LiEuCQlUpe"
   },
   "source": [
    "As noise increases:\n",
    "\n",
    "- The agent becomes more risk-averse, avoiding paths near the cliff.\n",
    "\n",
    "- Policies shift to favour safer routes, even if they are longer.\n",
    "\n",
    "This demonstrates how uncertainty affects decision-making in sequential environments.\n",
    "\n",
    "### To-do 3:\n",
    "\n",
    "In a cliff world environment, how does increasing the noise parameter influence the agent’s optimal policy?\n",
    "\n",
    "Choose the most accurate explanation:\n",
    "\n",
    "A. The agent becomes more aggressive, taking riskier paths to reach the goal faster.\n",
    "\n",
    "B. The agent ignores the cliff and treats all paths equally.\n",
    "\n",
    "C. The agent becomes more cautious, preferring longer but safer routes that avoid the cliff.\n",
    "\n",
    "D. The agent always chooses the shortest path, regardless of noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SruRz5J9uvdF"
   },
   "outputs": [],
   "source": [
    "#Input your choice (A/B/C/D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZhE8Ornu4GK"
   },
   "source": [
    "**The correct answer is: C.**\n",
    "\n",
    "The agent becomes more cautious, preferring longer but safer routes that avoid the cliff.\n",
    "\n",
    "In a cliff world environment, the agent must navigate near dangerous cliff edges that result in large negative rewards (or termination) if entered. The noise parameter introduces stochasticity — meaning the agent might not move exactly in the intended direction.\n",
    "\n",
    "Higher noise means a greater chance of unintended movement (e.g. slipping into the cliff).\n",
    "\n",
    "To avoid falling off, the agent learns to favour safer paths, even if they are longer.\n",
    "\n",
    "This behaviour reflects risk aversion due to increased uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vhMAKii_pBhe"
   },
   "source": [
    "### Visualising Value Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "brJP31ZtpFuI",
    "outputId": "a3a9229e-1f67-47d5-9865-274c044b8307"
   },
   "outputs": [],
   "source": [
    "#values_mat = convert_to_grid(values, cliff_grid, dtype=float)\n",
    "values_mat_big = convert_to_grid(values, cliff_world_big.grid, dtype=float);\n",
    "plot_grid(values_mat_big)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_78tNuIjxb_-"
   },
   "source": [
    "`plot_policy_grid()` visualises a grid world policy by mapping directional actions to arrows and displaying them on a Matplotlib axis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nn0rsvNopz49"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_policy_grid(policy_mat, title, ax):\n",
    "    ax.set_title(title)\n",
    "    ax.axis('off')\n",
    "    for (i, j), action in np.ndenumerate(policy_mat):\n",
    "        if isinstance(action, tuple):\n",
    "            dx, dy = action\n",
    "            arrow = {\n",
    "                (1, 0): '→', (-1, 0): '←',\n",
    "                (0, 1): '↑', (0, -1): '↓'\n",
    "            }.get((dx, dy), '.')\n",
    "        elif action is None:\n",
    "            arrow = 'G'\n",
    "        else:\n",
    "            arrow = '.'\n",
    "        ax.text(j, i, arrow, ha='center', va='center', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h8VIGFsdp9VV"
   },
   "outputs": [],
   "source": [
    "cliff_grid = [[-1]*12 for _ in range(3)] + [[-1]*12]\n",
    "cliff_grid[2][:11] = [-100]*11\n",
    "cliff_grid[2][11] = 100\n",
    "terminals = [(11, 2)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQ6wOsXP0WU8"
   },
   "source": [
    "###  Policy Comparison in a Cliff World MDP\n",
    "\n",
    "Let's visualise how different discount rates (`gamma`) and noise levels affect the optimal policy in a cliff world environment using value iteration. It creates a 2 × 2 subplot grid:\n",
    "\n",
    "- **Top left**: low noise (0.1), moderate discount (0.9)\n",
    "- **Top right**: high noise (0.25), moderate discount (0.9)\n",
    "- **Bottom left**: low noise (0.1), high discount (0.99)\n",
    "- **Bottom right**: high noise (0.25), high discount (0.99)\n",
    "\n",
    "Each subplot shows the agent’s preferred action in each state, revealing how increased noise leads to more cautious policies and higher discounting encourages long-term planning. The custom `NoisyCliffMDP` class overrides the transition model to simulate stochastic movement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1219
    },
    "id": "vXCOFwEkp-9G",
    "outputId": "87155ae6-963c-4197-b59d-83bbe83cb536"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "#Top left: gamma=0.9, noise=0.1\n",
    "mdp1 = mdp.GridMDP(cliff_grid, terminals=terminals, init=(0, 0), gamma=0.9)\n",
    "values1 = mdp.value_iteration(mdp1)\n",
    "policy1 = mdp.best_policy(mdp1, values1)\n",
    "mat1 = convert_to_grid(policy1, cliff_grid, dtype=object) #Changed dtype to object\n",
    "plot_policy_grid(mat1, \"Discount Rate: 0.9\", axes[0, 0])\n",
    "\n",
    "#Top right: gamma=0.9, noise=0.25\n",
    "class NoisyCliffMDP(mdp.GridMDP):\n",
    "    def __init__(self, grid, terminals, init=(0, 0), gamma=0.9, noise=0.25):\n",
    "        self.noise = noise\n",
    "        super().__init__(grid, terminals, init, gamma)\n",
    "\n",
    "    def T(self, state, action):\n",
    "        if action is None:\n",
    "            return [(0.0, state)]\n",
    "        return [(1 - self.noise, self.go(state, action)),\n",
    "                (self.noise / 2, self.go(state, mdp.turn_right(action))),\n",
    "                (self.noise / 2, self.go(state, mdp.turn_left(action)))]\n",
    "\n",
    "mdp2 = NoisyCliffMDP(cliff_grid, terminals, gamma=0.9, noise=0.25)\n",
    "values2 = mdp.value_iteration(mdp2)\n",
    "policy2 = mdp.best_policy(mdp2, values2)\n",
    "mat2 = convert_to_grid(policy2, cliff_grid, dtype=object) #Changed dtype to object\n",
    "plot_policy_grid(mat2, \"Noise Level: 0.25\", axes[0, 1])\n",
    "\n",
    "#Bottom left: gamma=0.99, noise=0.1\n",
    "mdp3 = mdp.GridMDP(cliff_grid, terminals=terminals, init=(0, 0), gamma=0.99)\n",
    "values3 = mdp.value_iteration(mdp3)\n",
    "policy3 = mdp.best_policy(mdp3, values3)\n",
    "mat3 = convert_to_grid(policy3, cliff_grid, dtype=object) #Changed dtype to object\n",
    "plot_policy_grid(mat3, \"Discount Rate: 0.99\", axes[1, 0])\n",
    "\n",
    "#Bottom right: gamma=0.99, noise=0.25\n",
    "mdp4 = NoisyCliffMDP(cliff_grid, terminals, gamma=0.99, noise=0.25)\n",
    "values4 = mdp.value_iteration(mdp4)\n",
    "policy4 = mdp.best_policy(mdp4, values4)\n",
    "mat4 = convert_to_grid(policy4, cliff_grid, dtype=object) #Changed dtype to object\n",
    "plot_policy_grid(mat4, \"Noise Level: 0.25\", axes[1, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vyyqcRxU0_CF"
   },
   "source": [
    "In this notebook, you explored how intelligent agents make decisions in uncertain environments using grid world and the aima3 library. You:\n",
    "\n",
    "- Defined deterministic and stochastic MDPs, including a cliff world scenario\n",
    "- Applied **value iteration** and **policy iteration** to compute optimal strategies\n",
    "- Visualised state values and policies using heat maps and directional arrows\n",
    "- Investigated how varying the **discount factor** (γ) affects short-term vs long-term planning\n",
    "- Observed how increasing **noise** in transitions makes the agent more risk-averse, preferring safer paths\n",
    "\n",
    "These experiments deepen your understanding of how agents balance reward, risk and uncertainty — core ideas in reinforcement learning. You're now equipped to explore more advanced topics such as Q-learning, policy gradients and real-world applications in robotics and recommendation systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LF1vU7HC1Exl"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
