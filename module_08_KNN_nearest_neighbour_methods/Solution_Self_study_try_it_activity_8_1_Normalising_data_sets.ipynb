{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MvUynzIn3kE6"
   },
   "source": [
    "# Self-study try-it activity 8.1: Normalising data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UBR9w09l4KaR"
   },
   "source": [
    "### Data preprocessing: normalisation and scaling\n",
    "\n",
    "In machine learning, scaling and normalisation are fundamental preprocessing techniques that ensure all features contribute proportionally to the model. These steps adjust the range and distribution of feature values, which improves model performance, speeds up convergence and enhances interpretability.\n",
    "\n",
    "Normalisation (min-max scaling) rescales features to a fixed range, preserving the shape of the original distribution.\n",
    "$$\n",
    "x' = \\frac{x - \\min(x)}{\\max(x) - \\min(x)}\n",
    "$$\n",
    "\n",
    "Standardisation (z-score scaling) transforms features to have zero mean and unit variance, making them suitable for algorithms that are sensitive to feature variance.\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{mean} = \\frac{1}{n} \\sum_{i=1}^n x_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{std} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_i - \\text{mean})^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "z = \\frac{x - \\text{mean}}{\\text{std}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "spDCMQkupre3"
   },
   "source": [
    "This activity consists of two parts:\n",
    "\n",
    "- Using a manual approach to compute normalisation and the z-score\n",
    "\n",
    "- Using built-in functions in `sklearn` to compute normalisation and the z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gw8dKF7IpWdf"
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8tpzQgJw75g"
   },
   "source": [
    "### Part one: Using a manual approach to compute normalisation and the z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bnC7Cs76e7qo"
   },
   "outputs": [],
   "source": [
    "# Original data\n",
    "data = [\n",
    "    [1.3, 5.1, 3.1],\n",
    "    [5.2, 3.8, 2.9],\n",
    "    [100.8, 4.2, 1.4],\n",
    "    [2.7, 1.1, 4.1],\n",
    "    [3.1, -123.2, 3.0]\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B00qjdCCwYDZ",
    "outputId": "ba91e584-3432-4b3a-a65f-6ff11422a84b"
   },
   "outputs": [],
   "source": [
    "# Manual normalisation\n",
    "features = list(zip(*data))\n",
    "\n",
    "normalized_features = []\n",
    "for feature in features:\n",
    "    min_val = min(feature)\n",
    "    max_val = max(feature)\n",
    "    normalized_col = [(x - min_val) / (max_val - min_val) for x in feature]\n",
    "    normalized_features.append(normalized_col)\n",
    "\n",
    "normalized_data = list(zip(*normalized_features))\n",
    "\n",
    "print(\"Normalized Data (Min-Max Scaling):\")\n",
    "for i, row in enumerate(normalized_data, 1):\n",
    "    print(f\"i={i}\\t\" + \"\\t\".join(f\"{v:.6f}\" for v in row))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fi7AlXOCwo8g",
    "outputId": "d4931c2b-82d8-468d-8694-7b94db75a749"
   },
   "outputs": [],
   "source": [
    "# Manual z-score standardisation\n",
    "zscore_features = []\n",
    "for feature in features:\n",
    "    mean = sum(feature) / len(feature)\n",
    "    variance = sum((x - mean) ** 2 for x in feature) / len(feature)\n",
    "    std_dev = variance ** 0.5\n",
    "    zscore_col = [(x - mean) / std_dev for x in feature]\n",
    "    zscore_features.append(zscore_col)\n",
    "\n",
    "# Transpose it back to the original row-wise format\n",
    "zscore_data = list(zip(*zscore_features))\n",
    "\n",
    "print(\"\\nZ-score Standardized Data:\")\n",
    "for i, row in enumerate(zscore_data, 1):\n",
    "    print(f\"i={i}\\t\" + \"\\t\".join(f\"{v:.6f}\" for v in row))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GIUttSsLxIWM"
   },
   "source": [
    "### Part two: Using built-in functions in `sklearn` to compute normalisation and the z-score\n",
    "\n",
    "In `sklearn`, `MinMaxScaler` normalises data to a fixed range, preserving the original distribution's shape but not mitigating the impact of outliers.\n",
    "\n",
    "`StandardScaler` standardises features by removing the mean and scaling to unit variance, ensuring that no feature dominates due to scale differences. It is particularly useful for algorithms that assume normally distributed data or are sensitive to feature scales.\n",
    "\n",
    "Both scalers are implemented as classes in `sklearn.preprocessing` and can be integrated into machine learning pipelines for efficient preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fmrnsl3VxHrd"
   },
   "outputs": [],
   "source": [
    "# Create the data set as a pandas DataFrame\n",
    "data = {\n",
    "    'Feature1': [1.3, 5.2, 100.8, 2.7, 3.1],\n",
    "    'Feature2': [5.1, 3.8, 4.2, 1.1, -123.2],\n",
    "    'Feature3': [3.1, 2.9, 1.4, 4.1, 3.0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-FPLtdgcoGSk",
    "outputId": "12112025-6d91-4702-8bf2-1ccb21a03645"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df = pd.DataFrame(data, index=[1, 2, 3, 4, 5])\n",
    "print(\"Original Data:\")\n",
    "print(df)\n",
    "\n",
    "# --- Normalisation (min-max scaling) ---\n",
    "scaler_minmax = MinMaxScaler()\n",
    "normalized_data = scaler_minmax.fit_transform(df)\n",
    "df_normalized = pd.DataFrame(normalized_data, columns=df.columns, index=df.index)\n",
    "\n",
    "print(\"\\nNormalized Data (Min-Max Scaling):\")\n",
    "print(df_normalized)\n",
    "\n",
    "# --- Z-score standardisation ---\n",
    "scaler_standard = StandardScaler()\n",
    "zscore_data = scaler_standard.fit_transform(df)\n",
    "df_zscore = pd.DataFrame(zscore_data, columns=df.columns, index=df.index)\n",
    "\n",
    "print(\"\\nZ-score Standardized Data:\")\n",
    "print(df_zscore)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOtqmxH30gUa"
   },
   "source": [
    "### To-do:\n",
    "\n",
    "1. Generate a random 10 by 2 data set with values in the range [0,1000].\n",
    "\n",
    "2. Initialise the scalars and assign it to `minmax_scaler` and `standard_scaler`, respectively.\n",
    "\n",
    "3. Apply the scalar functions, and display the original data and scaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GMwLIXtEzED2",
    "outputId": "de6963fc-8262-44a1-f122-48399748a623"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Generate a random 10 by 2 data set with values in the range [0, 1000)\n",
    "np.random.seed(42)  # for reproducibility\n",
    "data = np.random.randint(0, 1000, size=(10, 2))\n",
    "\n",
    "print(\"Original Data:\\n\", data)\n",
    "\n",
    "# Initialise the scalers\n",
    "minmax_scaler = MinMaxScaler()\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "# Apply MinMaxScaler\n",
    "data_minmax = minmax_scaler.fit_transform(data)\n",
    "print(\"\\nMinMax Scaled Data:\\n\", data_minmax)\n",
    "\n",
    "# Apply StandardScaler\n",
    "data_standard = standard_scaler.fit_transform(data)\n",
    "print(\"\\nStandard Scaled Data:\\n\", data_standard)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KaDdZG_860zt"
   },
   "source": [
    "### To-do:\n",
    "\n",
    "Generate a new data point and predict its scalar values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7WRj-OLF1X1B",
    "outputId": "ae7c3661-a9ad-4a2a-8643-5773c0d05ed4"
   },
   "outputs": [],
   "source": [
    "# Example new unseen sample (two features)\n",
    "new_sample = np.array([[450, 600]])\n",
    "\n",
    "# Transform it using MinMaxScaler fitted on the original data\n",
    "new_sample_minmax = minmax_scaler.transform(new_sample)\n",
    "print(\"New sample after MinMax scaling:\", new_sample_minmax)\n",
    "\n",
    "# Transform it using StandardScaler fitted on the original data\n",
    "new_sample_standard = standard_scaler.transform(new_sample)\n",
    "print(\"New sample after Standard scaling:\", new_sample_standard)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
