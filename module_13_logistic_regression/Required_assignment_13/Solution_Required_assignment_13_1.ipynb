{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAN7RcUNgKhY"
   },
   "source": [
    "# Required assignment 13.1: Optimising a logistic function in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D7vdQZ6FxEiR"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-R7jPtW2Z0iz"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, you will explore how to use logistic regression to predict the onset of diabetes using the [Pima Indians diabetes data set](https://www.kaggle.com/uciml/pima-inidans-diabetes-database). This notebook will walk you through the process of data preparation, model building, parameter selection and model evaluation with a focus on practical interpretation. \n",
    "\n",
    "Specifically, you will:\n",
    "\n",
    "- Work with the Pima Indians diabetes data set. \n",
    "\n",
    "- Build a logistic regression model.\n",
    "\n",
    "- Choose the best parameters.\n",
    "\n",
    "- Select the probability threshold based on the false positive rate (FPR) and the false negative rate (FNR)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TnSu23jhOVZA"
   },
   "source": [
    "## Download and prepare the data\n",
    "\n",
    "### Question 1\n",
    "\n",
    "- Review the `diabetes.csv` data set and store it in the variable `data`.\n",
    "\n",
    "- Display the columns of the data set and assign them to `columns`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eYNRqkcqyH0M",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ab77d11027a31a42",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "741ff4f0-b304-4601-b290-7eee913a2ecc"
   },
   "outputs": [],
   "source": [
    "###GRADED CELL\n",
    "data = ...\n",
    "columns = ...\n",
    "\n",
    "###BEGIN SOLUTION\n",
    "data = pd.read_csv('data/diabetes.csv')\n",
    "columns = data.columns\n",
    "###END SOLUTION\n",
    "\n",
    "print(\"The columns in the dataset are given by \", columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BYzf8roj3_kc"
   },
   "source": [
    "## Preprocess the data\n",
    "\n",
    "Check for the null values in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L3ZYp4V54QJd",
    "outputId": "ac7ecd5b-aa5b-4127-ede0-78db60c8cbe7"
   },
   "outputs": [],
   "source": [
    "print (data.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0qemfflyYSb"
   },
   "source": [
    "Split the data into inputs and outputs.\n",
    "\n",
    "### Question 2\n",
    "\n",
    "- Identify the inputs and outputs of the data. Assign them to variables `inputs` and `outputs`, respectively. \n",
    "\n",
    "- Assign the inputs to `X`.\n",
    "\n",
    "- Assign the outputs to `Y`.\n",
    "\n",
    "Hint: Ensure that the output is a 1D array of shape (`n_samples`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U6z2SNgVyWJX",
    "outputId": "b372ac2b-6be7-41ee-8045-dc6afd8771cf"
   },
   "outputs": [],
   "source": [
    "###GRADED CELL\n",
    "inputs = ...\n",
    "outputs = ...\n",
    "X = ...\n",
    "Y = ...\n",
    "\n",
    "###BEGIN SOLUTION\n",
    "inputs = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n",
    "outputs = ['Outcome']\n",
    "\n",
    "\n",
    "X = data[inputs]\n",
    "Y = data[outputs].to_numpy().reshape(-1)\n",
    "\n",
    "###END SOLUTION\n",
    "print(\"The inputs are given by :\", X.head())\n",
    "print(\"The outputs are given by :\", Y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqX_Zn3Hxhfd"
   },
   "source": [
    "Preprocess the data using the `StandardScaler()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RdH34bJixuRh"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X)\n",
    "X = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iyPlsKPP608P"
   },
   "source": [
    "Divide the data set into a 70/30 training and testing split.\n",
    "\n",
    "### Question 3\n",
    "\n",
    "- Use `train-test-split()` to split the data into 70 per cent training and 30 per cent testing sets.\n",
    "\n",
    "- Use `random_state =42`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XrzVsMpryuxB",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2defd733ff04a316",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "5e214765-0ac8-430b-b232-5fd4086c16e5"
   },
   "outputs": [],
   "source": [
    "###GRADED CELL\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = None, None, None, None\n",
    "\n",
    "###BEGIN SOLUTION\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 42)\n",
    "###END SOLUTION\n",
    "print(\"The shape of X_train is \", X_train.shape)\n",
    "print(\"The shape of X_test is \", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAmjUYZjxPVF"
   },
   "source": [
    "## Build a logistic regression model\n",
    "\n",
    "Recall that logistic regression models the relationship between one or more independent variables and a binary dependent variable by applying a sigmoid (logistic) function to produce an output between 0 and 1, representing the estimated probability of the event. It is a simple, interpretable model that is commonly used for binary classification problems.\n",
    "\n",
    "### Question 4\n",
    "\n",
    "- Create a `LogisticRegression` model with `penalty=None`  and assign it to `model`.\n",
    "\n",
    "- Use the `.fit()` to train the model.\n",
    "\n",
    "- Make predictions and assign them to `Y_train_pred` and `Y_test_pred`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dPmra3QPikH9",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-674225e98a12764b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "6fc2911c-191e-4d63-a554-84552d4a588a"
   },
   "outputs": [],
   "source": [
    "###GRADED CELL\n",
    "model = None\n",
    "Y_train_pred = None\n",
    "Y_test_pred = None\n",
    "\n",
    "###BEGIN SOLUTION\n",
    "model = LogisticRegression(penalty=None)\n",
    "model.fit(X_train, Y_train)\n",
    "Y_train_pred = model.predict(X_train)\n",
    "Y_test_pred = model.predict(X_test)\n",
    "###END SOLUTION\n",
    "print(\"The shape of Y_train_pred is \", Y_train_pred.shape)\n",
    "print(\"The shape of Y_test_pred is \", Y_test_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CALVvumtqK9r"
   },
   "source": [
    "### Question 5\n",
    "\n",
    "- Compute the confusion matrices for the training and testing data sets and assign them to `confusion_matrix_train` and `confusion_matrix_test`, respectively. \n",
    "\n",
    "- Compute the `train_accuracy` and `test_accuracy` using the `accuracy_score()` of `sklearn_metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "czPv6MgAvoKN",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6fb19ec5660fa91d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "50f58784-f4e1-4deb-e255-4cab612fce9a"
   },
   "outputs": [],
   "source": [
    "###GRADED CELL\n",
    "confusion_matrix_train = None\n",
    "confusion_matrix_test = None\n",
    "train_accuracy = None\n",
    "test_accuracy = None\n",
    "\n",
    "###BEGIN SOLUTION\n",
    "confusion_matrix_train = confusion_matrix(Y_train, Y_train_pred)\n",
    "confusion_matrix_test = confusion_matrix(Y_test, Y_test_pred)\n",
    "\n",
    "train_accuracy = accuracy_score(Y_train, Y_train_pred)\n",
    "test_accuracy = accuracy_score(Y_test, Y_test_pred)\n",
    "###END SOLUTION\n",
    "\n",
    "print('Training Confusion Matrix', confusion_matrix_train)\n",
    "\n",
    "print()\n",
    "print('Testing Confusion Matrix', confusion_matrix_test)\n",
    "\n",
    "print()\n",
    "print('Training accuracy', train_accuracy)\n",
    "\n",
    "print()\n",
    "print('Testing accuracy', test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BCvHNIle66HT"
   },
   "source": [
    "## Choose the best parameters\n",
    "\n",
    "Regularisation involves adding a penalty to the loss function to reduce model complexity and help prevent overfitting. Ideally, this leads to better generalisation.\n",
    "\n",
    "- Analyse the effect of L2 regularisation by examining how testing accuracy changes with different values of $C$.\n",
    "\n",
    "- Create a plot that shows how $C$ varies, starting at $10^{-6}$ and ending at $10^{-2}$.\n",
    "\n",
    "### Question 6\n",
    "\n",
    "- Create a `LogisticRegression` model with `penalty='l2` and `max_iter=1000`.\n",
    "\n",
    "- Use `GridSearchCV` with cross-validation `cv=5` and `scoring=accuracy`.\n",
    "\n",
    "- Compute the `mean_test_score` of  `grid_search.cv_results_` and assign it to `mean_accuracies`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UPEiaWNkuQVr",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a42879b2282e309a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "8841a6eb-1986-481f-bb1c-a8b0ad22f324"
   },
   "outputs": [],
   "source": [
    "###GRADED CELL\n",
    "\n",
    "C_space = np.linspace(10e-6, 10e-2, 200)\n",
    "param_grid = {'C': C_space}\n",
    "logreg = None\n",
    "grid_search = None\n",
    "mean_accuracies = None\n",
    "\n",
    "###BEGIN SOLUTION\n",
    "\n",
    "logreg = LogisticRegression(penalty='l2', max_iter=1000)\n",
    "\n",
    "grid_search = GridSearchCV(logreg, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, Y_train.ravel())\n",
    "\n",
    "\n",
    "mean_accuracies = grid_search.cv_results_['mean_test_score']\n",
    "###END SOLUTION\n",
    "print('The mean_accuracies is given by ', mean_accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "pBPOUXI7AbH8",
    "outputId": "fffb350a-9d82-4a35-9563-3993e27c905f"
   },
   "outputs": [],
   "source": [
    "plt.plot(C_space, mean_accuracies)\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('Cross-validated Accuracy')\n",
    "plt.title('Analysis of Regularisation')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_KCmcsNDCPYt"
   },
   "source": [
    "### Question 7\n",
    "\n",
    "- Identify the `best_C` value and its corresponding cross-validated accuracy.\n",
    "\n",
    "- Print the `best_C` and `best_accuracy` by computing the `best_index`.\n",
    "\n",
    "- Print the `test_accuracy` on the `best_model`.\n",
    "\n",
    "Hint: `best_model` can be obtained using `grid_search.best_estimator_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ZR9HKIhBXP7",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9810dde19c512351",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "e8610da3-4689-463d-f19d-5cdefb90c405"
   },
   "outputs": [],
   "source": [
    "###GRADED CELL\n",
    "best_C = None\n",
    "best_accuracy = None\n",
    "best_model = None\n",
    "test_accuracy = None\n",
    "###BEGIN SOLUTION\n",
    "best_index = np.argmax(mean_accuracies)\n",
    "best_C = C_space[best_index]\n",
    "best_accuracy = mean_accuracies[best_index]\n",
    "best_model = grid_search.best_estimator_\n",
    "test_accuracy = best_model.score(X_test, Y_test)\n",
    "###END SOLUTION\n",
    "print(f'Best cross-validated accuracy was achieved with C = {best_C}, giving an accuracy of {best_accuracy:.4f}.')\n",
    "\n",
    "\n",
    "\n",
    "print(f'Accuracy of the best model on the test set: {test_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pv_JxY-R_n0r"
   },
   "source": [
    "## Select the probability threshold based on the FPR and FNR\n",
    "\n",
    "Recall that you're not estimating $Y$ directly; you're estimating the probability of $Y = 1 | X$.\n",
    "\n",
    "So far, classification has been based on selecting the class with the higher probability. In other words, you've been using the following classifier:\n",
    "$$\n",
    "  \\hat{Y}(x) =\n",
    "    \\begin{cases}\n",
    "      0 & \\text{if } \\hat{p}(x) < 0.5 \\\\\n",
    "      1 & \\text{if } \\hat{p}(x) \\geq 0.5 \\\\\n",
    "    \\end{cases}       \n",
    "$$\n",
    "\n",
    "However, you also need to consider that the probability threshold of 0.5 may not be optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jmizp5n_CZba"
   },
   "source": [
    "### Question 8\n",
    "\n",
    "- Implement a function `predict_with_threshold` that takes a trained logistic regression model, a feature matrix X and a probability threshold. It should return class predictions (0 or 1) based on whether the predicted probability for class 1 is greater than or equal to the threshold. \n",
    "\n",
    "- For thresholds ranging from 0 to 1, compute and store the FNR and FPR on the test set.\n",
    "\n",
    "- Plot the FNR and FPR as functions of the threshold to observe how these metrics change with varying thresholds.\n",
    "\n",
    "Hint: Use the `confusion_matrix` function from `sklearn.metrics` to compute the FNR and FPR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u95hKtPUEITK",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c6e7f4875b001595",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "###GRADED CELL\n",
    "\n",
    "model = LogisticRegression(penalty = 'l2', C = best_C)\n",
    "model.fit(X_train, Y_train)\n",
    "#Define predict_with_threshold(model,X,threshold):\n",
    "#Calculate the probabilities and use the second column, which represents the probabilities\n",
    "#Build an array of Boolean variables, checking whether the probabilities are larger than or equal to the threshold\n",
    "#Compute predictions by transforming the Boolean variables and making them integers\n",
    "#Return predictions\n",
    "\n",
    "###BEGIN SOLUTIONS\n",
    "def predict_with_threshold(model, X, threshold):\n",
    "  probabilities = model.predict_proba(X)\n",
    "  probabilities = probabilities[:, 1]\n",
    "  boolean_threshold = (probabilities >= threshold)\n",
    "  predictions = boolean_threshold.astype(int)\n",
    "\n",
    "  return predictions\n",
    "###END SOLUTIONS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XDebFllpQgjC"
   },
   "source": [
    "### Question 9\n",
    "\n",
    "- Compute the number of true positives and assign it to `num_of_positives`.\n",
    "\n",
    "- Compute the number of true negatives and assign it to `num_of_negatives`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EgV_9FKIN197",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-54b2104d9892a4e6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "c46a34a1-a2e9-43df-fe4b-553c2f6a4241"
   },
   "outputs": [],
   "source": [
    "###GRADED CELL\n",
    "thresholds = np.linspace(0, 1, 101)\n",
    "\n",
    "false_negatives = []\n",
    "false_positives = []\n",
    "\n",
    "###BEGIN SOLUTION\n",
    "\n",
    "#Calculate the number of true positives and true negatives\n",
    "num_of_positives = np.sum(Y_test)\n",
    "num_of_negatives = len(Y_test) - num_of_positives\n",
    "\n",
    "###END SOLUTION\n",
    "\n",
    "print(\"The number of true positives is \", num_of_positives)\n",
    "print(\"The number of true negatives is \", num_of_negatives)\n",
    "for tp in thresholds:\n",
    "  #Predict\n",
    "  Y_pred = predict_with_threshold(model, X_test, tp)\n",
    "  #Calculate predictions\n",
    "  cm = confusion_matrix(Y_test, Y_pred)\n",
    "  #Calculate the FNR and the FPR\n",
    "  fnr = cm[1, 0] / num_of_positives\n",
    "  fpr = cm[0, 1] / num_of_negatives\n",
    "  #Append to lists\n",
    "  false_negatives.append(fnr)\n",
    "  false_positives.append(fpr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RiBzxQzmS4f2"
   },
   "source": [
    "The FNR and FPR are plotted against the probability threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 619
    },
    "id": "jsAWWPalSXmA",
    "outputId": "dcddc864-9172-4af0-9c56-91785e98df86"
   },
   "outputs": [],
   "source": [
    "#Create a plot to see how the rates change with the threshold\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "fig.set_figheight(6)\n",
    "fig.set_figwidth(10)\n",
    "fig.suptitle('Investigation of the FNR and FPR against Probability Threshold')\n",
    "\n",
    "ax.plot(thresholds, false_negatives, 'r', label = 'FNR')\n",
    "ax.plot(thresholds, false_positives, 'b', label = 'FPR')\n",
    "ax.set_ylabel('Performance Metrics')\n",
    "ax.set_xlabel('Probability Thresholds')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxm6rCpbf68M"
   },
   "source": [
    "When the threshold is lowered, the FPR increases, while the FNR decreases. Conversely, increasing the threshold decreases the FPR and increases the FNR. The optimal threshold depends on the context of the problem and the relative costs of false positives and false negatives. \n",
    "\n",
    "The point where the FPR and FNR curves intersect on the probability threshold plot is a reasonable candidate as the 'best threshold', assuming that false positives and false negatives are equally costly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "interpreter": {
   "hash": "d37abda7630e259e5026a5079657683a09f6e3d11473720762ebe7250c494840"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
