{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOdFs3CiTjng"
   },
   "source": [
    "# Self-study try-it activity 13.1: Changing the parameters of a logistic function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-R7jPtW2Z0iz"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, you'll analyse logistic regression using the [Pima Indians diabetes data set](https://www.kaggle.com/uciml/pima-indians-diabetes-database). The goal is to predict the onset of diabetes based on various diagnostic measurements.\n",
    "\n",
    "Start by downloading and importing the data set.\n",
    "\n",
    "Then, review a brief recap of logistic regression and its training procedure. You’ll fit a logistic regression model to the data set.\n",
    "\n",
    "Next, you'll explore how to choose the best model for classification. This includes a short introduction to regularisation. The second set of exercises focuses on selecting the best regularisation constant and examining its effect on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D7vdQZ6FxEiR"
   },
   "outputs": [],
   "source": [
    "#Importing the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TnSu23jhOVZA"
   },
   "source": [
    "### Download and preprocess the data\n",
    "\n",
    "- Download the `diabetes.csv` data set and store it in the variable `data`. \n",
    "\n",
    "- Display the first five rows of the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2WbV63QOfzqq"
   },
   "outputs": [],
   "source": [
    "data = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "cxMZ-EX3f50D",
    "outputId": "e262ec05-56bf-4326-c27e-969286bb80b5"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/diabetes.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bJQ1n2lrgV2L"
   },
   "source": [
    "Identify the `inputs` and `outputs` and assign them to the variables `X` and `Y`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jzAPoDSegPh6"
   },
   "outputs": [],
   "source": [
    "outputs = ...\n",
    "inputs = ...\n",
    "X = ...\n",
    "Y = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U6z2SNgVyWJX"
   },
   "outputs": [],
   "source": [
    "outputs = ['Outcome']\n",
    "inputs = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n",
    "\n",
    "X = data[inputs]\n",
    "Y = data[outputs].to_numpy().reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqX_Zn3Hxhfd"
   },
   "source": [
    "Scale the data and explain why this step needs to be performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RdH34bJixuRh"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X)\n",
    "X = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLSiS5Z1h0hr"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7FnBYQMh0jm"
   },
   "source": [
    "Standardisation transforms each feature so that it has a mean of 0 and a standard deviation of 1. The `diabetes.csv` data set has different units and scales. Without scaling, features with larger numeric ranges could dominate the learning process, leading to biased model performance. \n",
    "\n",
    "The `fit` method computes the mean and standard deviation from the data, and `transform` applies this scaling to the centre and scales all features accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iyPlsKPP608P"
   },
   "source": [
    "Finally, divide the data set into an 80/20 training and testing split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XrzVsMpryuxB"
   },
   "outputs": [],
   "source": [
    "num_of_points = len(Y)\n",
    "\n",
    "idx = list(range(num_of_points))\n",
    "np.random.shuffle(idx)\n",
    "idx_train = idx[:int(num_of_points * 0.8)]\n",
    "idx_train.sort()\n",
    "idx_test = idx[int(num_of_points * 0.8):]\n",
    "idx_test.sort()\n",
    "\n",
    "X_train = X[idx_train, :]\n",
    "X_test = X[idx_test, :]\n",
    "\n",
    "Y_train = Y[idx_train]\n",
    "Y_test = Y[idx_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08jCdcuftEYg"
   },
   "source": [
    "Note: `sklearn` uses `train-test-split` in `sklearn.model_selection` to split the data. Try splitting the data using the `train-test-split` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAmjUYZjxPVF"
   },
   "source": [
    "## Logistic regression\n",
    "\n",
    "Logistic regression is a binary classifier that predicts the probability of an input that belongs to the positive class.\n",
    "\n",
    "Assume you have a set of predictors $x \\in \\mathcal{X}$ and a binary output $y \\in \\{0, 1 \\}$. You want to estimate the probability of an input belonging to the positive class, i.e. you want to build an estimator $\\hat{p}(x)$ such that:\n",
    "\n",
    "$$\n",
    "\\hat{p}(x) = \\mathbb{P}( Y = 1 | X = x )\n",
    "$$\n",
    "\n",
    "You might first consider using a linear regression model:\n",
    "\n",
    "$$\n",
    "\\hat{p}(x) = \\beta_0 + x \\beta_1\n",
    "$$\n",
    "\n",
    "However, this leads to a problem: probabilities must lie between $0$ and $1$, and the linear model does not guarantee this.\n",
    "\n",
    "To address this issue, you can wrap the linear model in a function that constrains the output to the range [0, 1]. Here, you can use the sigmoid function:\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "Applying the sigmoid to the linear model gives you the logistic regression model:\n",
    "\n",
    "$$\n",
    "\\hat{p}(x) = \\sigma(\\beta_0 + x \\beta_1)\n",
    "$$\n",
    "\n",
    "which can also be written as:\n",
    "\n",
    "$$\n",
    "\\hat{p}(x) = \\frac{e^{\\beta_0 + x \\beta_1}}{1 + e^{\\beta_0 + x \\beta_1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9LRuT3WCn7J"
   },
   "source": [
    "## Train the function\n",
    "\n",
    "The parameters of a linear regression model are typically estimated using least squares. However, this method isn’t well suited for logistic regression because you're not estimating $Y$ directly. Instead, you're estimating $\\textit{the probability}$ of Y, which makes maximum likelihood estimation a more appropriate choice.\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\prod_{i : y_i = 1} \\mathbb{P}(Y = 1 | X = x_i) \\prod_{i' : y_{i'} = 0} (1 - \\mathbb{P}(Y = 0 | X = x_{i'}))\n",
    "$$\n",
    "\n",
    "Rather than maximising the likelihood directly, it’s more common to minimise the negative log-likelihood:\n",
    "\n",
    "$$\n",
    "\\ell(\\beta) = - \\log \\mathcal{L}(\\beta) = - \\sum_{i : y_i = 1} \\log\\sigma(x_i^T \\beta) - \\sum_{i' : y_{i'} = 0}\\log \\sigma(x_{i'}^T\\beta)\n",
    "$$\n",
    "\n",
    "This leads to the parameter estimate defined as $\\hat{\\beta}$:\n",
    "$$\n",
    "\\hat{\\beta} = \\arg\\min_{\\beta} \\ell(\\beta)\n",
    "$$\n",
    "\n",
    "You can solve this using any gradient-based optimiser. Additionally, you can incorporate regularisation to reduce overfitting. You’ll explore this concept in more detail in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R90i_2UNu_0l"
   },
   "source": [
    "### Question 1\n",
    "\n",
    "Prove that the sigmoid function always produces an output between 0 and 1.\n",
    "Hint: Consider the limits as $x \\rightarrow \\pm \\infty$ and show that the function is increasing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JrPqXOjM_KTT"
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M4rNG6xDvfd0"
   },
   "source": [
    "The sigmoid function is $\\sigma(x) = \\frac{1}{1 + e^{-x}}$. Its output is always between 0 and 1.\n",
    "\n",
    "**Limits as $( x \\to -\\infty)$ and $( x \\to +\\infty)$**\n",
    "\n",
    "**As $x \\to -\\infty$:**\n",
    "\n",
    "- $-x$ becomes very large and positive.\n",
    "\n",
    "- Therefore, $e^{-x}$ becomes very **large**.\n",
    "\n",
    "- The denominator $1 + e^{-x}$ is also **very large**.\n",
    "\n",
    "- Therefore:\n",
    "  $$\n",
    "  \\sigma(x) = \\frac{1}{\\text{very large number}} \\approx 0\n",
    "  $$\n",
    "\n",
    "**As $x \\to +\\infty$:**\n",
    "\n",
    "- $-x$ becomes very large and **negative**.\n",
    "\n",
    "- Therefore, $e^{-x} \\to 0$.\n",
    "\n",
    "- The denominator $1 + e^{-x} \\to 1$.\n",
    "\n",
    "- Therefore:\n",
    "  $$\n",
    "  \\sigma(x) = \\frac{1}{1 + 0} = 1\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZEaaQZOFzh9m"
   },
   "source": [
    "**The output is always between 0 and 1.**\n",
    "\n",
    "The exponential function $e^{-x}$ is always **positive** for any real $x$.\n",
    "\n",
    "Thus, the denominator $1 + e^{-x}$ is always **greater than 1**.\n",
    "\n",
    "Therefore, the sigmoid function:\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "is always:\n",
    "\n",
    "- **Greater than 0** because the numerator is 1 and the denominator is always positive.\n",
    "- **Less than 1** because the denominator is always greater than 1.\n",
    "\n",
    "Hence, the output of $\\sigma(x)$ always lies **strictly between 0 and 1**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vSFTl0kuzoZ1"
   },
   "source": [
    "**The function always increases.**\n",
    "\n",
    "The sigmoid function is **monotonically increasing**: as $x$ increases, $\\sigma(x)$ also increases.\n",
    "\n",
    "This means the function **never decreases** but smoothly transitions from values near 0 to values near 1 as $x$ goes from $-\\infty$ to $+\\infty$.\n",
    "\n",
    "In other words:\n",
    "\n",
    "$$\n",
    "x_1 < x_2 \\quad \\Rightarrow \\quad \\sigma(x_1) < \\sigma(x_2)\n",
    "$$\n",
    "\n",
    "The output of $\\sigma(x)$ **steadily rises**, making it a smooth and useful activation function for binary classification and neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28zASiCBvS9q"
   },
   "source": [
    "### Question 2\n",
    "\n",
    " Using the `LogisticRegression` class from `scikit.learn`, train a model on the data set above. \n",
    " \n",
    " Make sure you are not regularising. For more information, review the sklearn documentation (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). \n",
    " \n",
    " Show the model's training and testing accuracy and build a confusion matrix for each set.\n",
    " \n",
    " (Hint: The methods and functions required are $ \\texttt{.fit()} $, $\\texttt{.predict()}$, $\\texttt{confusion\\_matrix()}$, and $\\texttt{accuracy\\_score()}$.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ULSvbgau0sn5"
   },
   "outputs": [],
   "source": [
    "model = ...\n",
    "Y_train_pred = ...\n",
    "Y_test_pred = ...\n",
    "confusion_matrix_train = ...\n",
    "confusion_matrix_test = ...\n",
    "train_accuracy = ...\n",
    "test_accuracy = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "czPv6MgAvoKN",
    "outputId": "c57c1b97-b296-4681-cd9e-5dcb32105a02"
   },
   "outputs": [],
   "source": [
    "#Define the model\n",
    "model = LogisticRegression(penalty = None)\n",
    "#Train the model\n",
    "model.fit(X_train, Y_train)\n",
    "#Create the predictions\n",
    "Y_train_pred = model.predict(X_train)\n",
    "Y_test_pred = model.predict(X_test)\n",
    "#Build the confusion matrices\n",
    "confusion_matrix_train = confusion_matrix(Y_train, Y_train_pred)\n",
    "confusion_matrix_test = confusion_matrix(Y_test, Y_test_pred)\n",
    "#Calculate the accuracies\n",
    "train_accuracy = accuracy_score(Y_train, Y_train_pred)\n",
    "test_accuracy = accuracy_score(Y_test, Y_test_pred)\n",
    "#Display the results\n",
    "print('Training Confusion Matrix')\n",
    "print(confusion_matrix_train)\n",
    "print()\n",
    "print(f'Training accuracy = {train_accuracy}')\n",
    "print()\n",
    "print('Testing Confusion Matrix')\n",
    "print(confusion_matrix_test)\n",
    "print()\n",
    "print(f'Testing accuracy = {test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BCvHNIle66HT"
   },
   "source": [
    "## Regularisation\n",
    "\n",
    "Regularisation involves adding a penalty term to the loss function to reduce model complexity and help prevent overfitting. Ideally, this leads to better generalisation.\n",
    "\n",
    "In the case of L2 regularisation, you can modify the optimisation objective as follows:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = \\arg\\min_\\beta \\{ C \\cdot \\ell(\\beta) + \\frac{1}{2}\\beta^T \\beta \\}\n",
    "$$\n",
    "\n",
    "This penalty encourages the parameters $\\beta$ to stay closer to 0 and effectively simplifies the model by reducing the influence of any single feature. \n",
    "\n",
    "### Question 3\n",
    "\n",
    "- Analyse the effect of L2 regularisation. In particular, focus on how the testing accuracy changes for different values of $C$. \n",
    "\n",
    "- Create a plot that shows how $C$ varies, starting at $10^{-6}$ and ending at $10^{-2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4yMFaxR98wr"
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "id": "IdyH1TBvzcKw",
    "outputId": "101366eb-7ee2-44e3-a30e-47c6ad5477a2"
   },
   "outputs": [],
   "source": [
    "\n",
    "C_space = np.linspace(10e-6, 10e-2, 200)\n",
    "accuracies = []\n",
    "\n",
    "for C in C_space:\n",
    "  model = LogisticRegression(penalty = 'l2', C = C)\n",
    "  model.fit(X_train, Y_train)\n",
    "  Y_test_pred = model.predict(X_test)\n",
    "  accuracies.append(accuracy_score(Y_test, Y_test_pred))\n",
    "\n",
    "plt.plot(C_space, accuracies)\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('Testing Accuracy')\n",
    "plt.title('Analysis of Regularisation')\n",
    "\n",
    "best_model = np.argmax(accuracies)\n",
    "best_accuracy = accuracies[best_model]\n",
    "best_C = C_space[best_model]\n",
    "\n",
    "print(f'Best test accuracy was achieved with C = {best_C}, giving an accuracy of {best_accuracy}.')\n",
    "print(f'Without regularisation, we achieved an accuracy of {test_accuracy}.')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Ut5IZIH1npd"
   },
   "source": [
    "### Question 4\n",
    "\n",
    "- What behaviour do you observe as regularisation increases (i.e. as $C$ decreases)?\n",
    "\n",
    "- From your analysis, which value of the regularisation constant gives the best result? How does the corresponding testing accuracy compare with your earlier results?\n",
    "\n",
    "- For which values of $C$ does the model recover the previous training accuracy? Why do you think this occurs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWUAbWC8_GKa"
   },
   "source": [
    "### Solutions\n",
    "\n",
    "- When $C$ is very small in logistic regression, it implies strong regularisation, which heavily shrinks the model coefficients towards 0. This simplification can help prevent overfitting but may also lead to underfitting when the model fails to capture important patterns in the data. As a result, generalisation to unseen data may improve, but training performance can decline due to increased bias and the model's inability to represent more complex relationships.\n",
    "\n",
    "- There may be some improvement in test performance with regularisation, but this is not guaranteed, as it depends on the data and the severity of overfitting in the unregularised model.\n",
    "\n",
    "- You can recover the previous training accuracy when $C$ is large, which corresponds to weak regularisation. In this case, the penalty on the model coefficients is minimal, allowing them to grow larger and fit the training data more closely. This often leads to high training accuracy but at the risk of overfitting, meaning it captures noise or specific idiosyncrasies in the training set that don’t generalise well to new data. As a result, validation or test performance may degrade despite strong training results."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "interpreter": {
   "hash": "d37abda7630e259e5026a5079657683a09f6e3d11473720762ebe7250c494840"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
