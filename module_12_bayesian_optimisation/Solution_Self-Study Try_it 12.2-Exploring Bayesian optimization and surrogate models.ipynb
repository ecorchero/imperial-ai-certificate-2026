{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-study try-it activity 12.2: Applying surrogate models to guide decision-making in Bayesian optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4SwBZjG_GUyH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYwyi0m3zELD"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, you’ll revisit key ideas from the module with a focus on black box optimisation. By manually selecting queries, you’ll build intuition for how Bayesian optimisation works.\n",
    "\n",
    "You'll begin with the Bayesian optimisation problem, where you take on the optimiser’s role. This helps reveal why the problem is hard and what kinds of strategies might help solve it. You’ll then explore surrogate models as tools that represent your belief and uncertainty about the black box function. With a surrogate in place, choosing the next query becomes more manageable. As you work through the examples, you’ll likely start forming your own rules for picking queries. These rules can be formalised into acquisition functions.\n",
    "\n",
    "By the end, you'll define and analyse two new acquisition functions, testing how they trade off between exploration and exploitation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JRRb5fGpQq0A"
   },
   "source": [
    "## The Bayesian optimisation problem\n",
    "\n",
    "Bayesian optimisation addresses the problem of maximising expensive black box functions. In particular, your goal is to find:\n",
    "\n",
    "$$x_* = \\arg\\max f(x)$$\n",
    "\n",
    "You don’t know anything about the function in advance, but you can query it at any input $x$. The challenge is to get as close as possible to the maximum using as few queries as possible.\n",
    "\n",
    "To build intuition for the problem and how you might solve it, try the following exercise.\n",
    "\n",
    "When running the cell, you’ll be asked to enter a value between zero and one. You have ten attempts to maximise an unknown function. Because it’s a black box, you’ll only see the inputs you’ve chosen, \n",
    "$x_i$, and the outputs, $f(x_i)$. You can rerun the cell as many times as you like. As you go, consider the following:\n",
    "\n",
    "1. How is this different from classical optimisation? (Hint: can you use gradients?)\n",
    "\n",
    "2. What are the main difficulties you encounter when optimising?\n",
    "\n",
    "3. What strategies help you improve over multiple runs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vXUaNq-bGqaQ"
   },
   "outputs": [],
   "source": [
    "#Draw random function parameters\n",
    "modes = np.random.randint(1, 5)\n",
    "std = np.random.uniform(low = 0.005, high = 0.05, size = modes)\n",
    "means = np.random.uniform(size = modes)\n",
    "amps = np.random.uniform(size = modes) * (2 - 1) + 1\n",
    "\n",
    "#Define function\n",
    "def calc_function(x):\n",
    "  exp = -(x - means) ** 2 / std\n",
    "  y = amps * np.exp(exp)\n",
    "  return np.sum(y)\n",
    "\n",
    "#Initialise query lists and maximum observations\n",
    "X, Y = [], []\n",
    "max_obs = 0\n",
    "\n",
    "#Number of queries in the optimisation loop\n",
    "num_queries = 10\n",
    "\n",
    "for i in range(0, num_queries):\n",
    "  #Clear the outputs to keep the interface clean\n",
    "  clear_output(wait = True)\n",
    "  #Initialise plots\n",
    "  fig, ax = plt.subplots(figsize = (15, 7))\n",
    "  #Set the x and y limits, labels and dynamic title\n",
    "  ax.set_xlim(0, 1)\n",
    "  ax.set_ylim(0, max(max_obs + 1, 3))\n",
    "  ax.set_ylabel('f(x)')\n",
    "  ax.set_xlabel('x')\n",
    "  ax.set_title('So far you have selected ' + str(i) + ' queries, you have ' + str(10 - i) + ' left.' )\n",
    "  #Plot queries and show the plot\n",
    "  ax.scatter(X, Y, c = 'r', marker='x', s = 100)\n",
    "  plt.show()\n",
    "  #Initialise x\n",
    "  x = -1\n",
    "  #Select a display format for X and Y\n",
    "  X_format =  ['%.2f' % query for query in X] #Two sig figs\n",
    "  Y_format = ['%.4f' % obs for obs in Y] #Four sig figs\n",
    "\n",
    "  while not (0 <= x <= 1): #Condition to ensure a number between zero and one is chosen\n",
    "    data = [(query, obs) for query, obs in zip(X_format, Y_format)]\n",
    "    print('Data so far (sorted by descending observations): ')\n",
    "    print('\\n'.join('{}: (x, f(x)) = {}'.format(*k) for k in enumerate(data, start = 1))) #Display data\n",
    "    x = float(input('Pick a number between 0 and 1: '))\n",
    "  #Append data, calculate function and sort lists according to observation values\n",
    "  X.append(x)\n",
    "  y = calc_function(x)\n",
    "  Y.append(y)\n",
    "  X = [x for _, x in sorted(zip(Y, X), reverse = True)]\n",
    "  Y.sort(reverse = True)\n",
    "  max_obs = max(max_obs, y)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "#Calculate function in the grid [0, 0.01, 0.02, ..., 0.98, 0.99, 1]\n",
    "x_grid = np.linspace(0, 1, 1001)\n",
    "y_real = []\n",
    "best_obs_grid = 0\n",
    "for x in x_grid:\n",
    "  y = calc_function(x)\n",
    "  y_real.append(y)\n",
    "  best_obs_grid = max(best_obs_grid, y) #Keep track of the best observation\n",
    "\n",
    "\n",
    "#Final plot and display\n",
    "fig, ax = plt.subplots(figsize = (15, 7))\n",
    "# Plot GP posterior mean and ~95% confidence band (beta=1.96, not Upper Confidence Bound)\n",
    "ax.plot(x_grid, y_real, 'k', label = 'f(x)')\n",
    "ax.scatter(X, Y, c = 'r', marker = 'x', label = 'Queries', s = 100)\n",
    "ax.set_ylabel('f(x)')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_title('Real function and all queries')\n",
    "plt.show()\n",
    "print('Maximum (by Grid-Search):')\n",
    "print(best_obs_grid)\n",
    "print('Best by Yourself:')\n",
    "print(max_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. How is this different from classical optimisation?**\n",
    "\n",
    "In classical optimisation, such as gradient descent, you usually have access to the function itself or its gradients. This lets you move step by step toward the optimum by following the slope\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. What are the main difficulties you encounter when optimising?**\n",
    "\n",
    "When optimising a black box function, you face noisy or limited information, lack access to analytical forms or derivatives and are restricted to a small number of queries, requiring efficiency and strategic sampling. Additionally, the function’s unknown shape and the need to balance exploration with exploitation further complicate the search for the global maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. What strategies help you improve over multiple runs?**\n",
    "\n",
    "To maximise your chances of finding the global maximum, you can balance the exploration of new regions and the exploitation of promising areas while tracking and sorting observations to guide future queries. Additionally, you can use interactive feedback, heuristic selection and continuous learning from each query to visually assess and adapt your optimisation strategy as you progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "Use the following quadratic function and rerun the code cell above for the Bayesian optimisation problem:\n",
    "\n",
    "$$-4(x - 0.5)^2 + 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######Draw random function parameters\n",
    "modes = np.random.randint(1, 5)\n",
    "std = np.random.uniform(low = 0.005, high = 0.05, size = modes)\n",
    "means = np.random.uniform(size = modes)\n",
    "amps = np.random.uniform(size = modes) * (2 - 1) + 1\n",
    "\n",
    "#Define function\n",
    "def calc_function(x):\n",
    "    return -4 * (x - 0.5)**2 + 1\n",
    "\n",
    "#Initialise query lists and maximum observations\n",
    "X, Y = [], []\n",
    "max_obs = 0\n",
    "\n",
    "#Number of queries in the optimisation loop\n",
    "num_queries = 10\n",
    "\n",
    "for i in range(0, num_queries):\n",
    "  #Clear the outputs to keep the interface clean\n",
    "  clear_output(wait = True)\n",
    "  #Initialise plots\n",
    "  fig, ax = plt.subplots(figsize = (15, 7))\n",
    "  #Set the x and y limits, labels and dynamic title\n",
    "  ax.set_xlim(0, 1)\n",
    "  ax.set_ylim(0, max(max_obs + 1, 3))\n",
    "  ax.set_ylabel('f(x)')\n",
    "  ax.set_xlabel('x')\n",
    "  ax.set_title('So far you have selected ' + str(i) + ' queries, you have ' + str(10 - i) + ' left.' )\n",
    "  #Plot queries and show the plot\n",
    "  ax.scatter(X, Y, c = 'r', marker='x', s = 100)\n",
    "  plt.show()\n",
    "  #Initialise x\n",
    "  x = -1\n",
    "  #Select a display format for X and Y\n",
    "  X_format =  ['%.2f' % query for query in X] #Two sig figs\n",
    "  Y_format = ['%.4f' % obs for obs in Y] #Four sig figs\n",
    "\n",
    "  while not (0 <= x <= 1): #Condition to ensure a number between zero and one is chosen\n",
    "    data = [(query, obs) for query, obs in zip(X_format, Y_format)]\n",
    "    print('Data so far (sorted by descending observations): ')\n",
    "    print('\\n'.join('{}: (x, f(x)) = {}'.format(*k) for k in enumerate(data, start = 1))) # display data\n",
    "    x = float(input('Pick a number between 0 and 1: '))\n",
    "  #Append data, calculate function and sort lists according to observation values\n",
    "  X.append(x)\n",
    "  y = calc_function(x)\n",
    "  Y.append(y)\n",
    "  X = [x for _, x in sorted(zip(Y, X), reverse = True)]\n",
    "  Y.sort(reverse = True)\n",
    "  max_obs = max(max_obs, y)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "#Calculate function in the grid [0, 0.01, 0.02, ..., 0.98, 0.99, 1]\n",
    "x_grid = np.linspace(0, 1, 1001)\n",
    "y_real = []\n",
    "best_obs_grid = 0\n",
    "for x in x_grid:\n",
    "  y = calc_function(x)\n",
    "  y_real.append(y)\n",
    "  best_obs_grid = max(best_obs_grid, y) #Keep track of the best observation\n",
    "\n",
    "\n",
    "#Final plot and display\n",
    "fig, ax = plt.subplots(figsize = (15, 7))\n",
    "ax.plot(x_grid, y_real, 'k', label = 'f(x)')\n",
    "ax.scatter(X, Y, c = 'r', marker = 'x', label = 'Queries', s = 100)\n",
    "ax.set_ylabel('f(x)')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_title('Real function and all queries')\n",
    "plt.show()\n",
    "print('Maximum (by Grid-Search):')\n",
    "print(best_obs_grid)\n",
    "print('Best by Yourself:')\n",
    "print(max_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlaRFIklVITK"
   },
   "source": [
    "## Surrogate models\n",
    "\n",
    "By the end of the previous exercise, you may have started to develop a strategy that balances exploration and exploitation. You explore regions where you know little about the function, and you exploit areas where you’ve already observed high values by refining your search nearby.\n",
    "\n",
    "Surrogate models allow you to design algorithms that follow this kind of logic. They offer a way to represent both your belief about the unknown function and your uncertainty. The most common example is the Gaussian process (GP).\n",
    "\n",
    "A GP defines a distribution over functions. It’s specified by a mean function and a positive semi-definite kernel. To formalise this, consider a space $\\mathcal{X}$, a function $\\mu: \\mathcal{X} \\rightarrow \\mathbb{R}$ and a *positive semi-definite* function $\\kappa : \\mathcal{X} \\times \\mathcal{X} \\rightarrow \\mathbb{R}$. \n",
    "\n",
    "You can say that $f$ follows a GP prior with mean function $\\mu$ and kernel $\\kappa$ if, for any $n \\in \\mathbb{N}$ and any set of distinct points $(x_1, x_2, ..., x_n) := x$, $x_i \\in \\mathcal{X}$, the random vector $(f(x_1), ..., f(x_n))$ is normally distributed with mean vector $m(x)$ and covariance matrix $K(x)$, where:\n",
    "\n",
    "$$\n",
    "m(x)_i = \\mu(x_i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "K(x)_{i, j} = \\kappa(x_i, x_j)\n",
    "$$​\n",
    "\n",
    "This can be written compactly as: $f \\sim \\mathcal{GP}(\\mu(x), \\kappa(x))$\n",
    "\n",
    "One major advantage of GPs is that you can compute the posterior distribution analytically, even when the observations are noisy. Suppose your observed data follows the model:\n",
    "\n",
    "$$\n",
    " y = f(x) + \\epsilon\n",
    "$$\n",
    "\n",
    "where $\\epsilon \\sim \\text{Normal}(0, \\sigma_e^2)$ is Gaussian noise. \n",
    "\n",
    "Assume $f \\sim \\mathcal{GP}(\\mu_0, \\kappa_0)$, and you are given a data set of queries and observations, $D_n = \\{ (x_i, y_i) : i = 1, ..., n \\}$. Then the posterior distribution $f \\ | \\ D_n$ is also a GP, with the following posterior mean and covariance functions:\n",
    "\n",
    "\n",
    "$$\n",
    "    m_n(x) = \\mu_0(x) + k(x)^T( K + \\sigma^2 I )^{-1} (Y - m)\n",
    "$$\n",
    "\n",
    "$$\n",
    "    s^2_n(x, x') = \\kappa_0(x, x') - k(x)^T ( K + \\sigma^2 I ) ^{-1} k(x')\n",
    "$$\n",
    "\n",
    "where: $m_i = \\mu_0(x_i), Y_i = y_i, k(x)_i = \\kappa_0(x, x_i), K_{ij} = \\kappa_0(x_i, x_j)$\n",
    "\n",
    "You’ll now repeat the earlier exercise, this time with a GP surrogate fitted to your observed queries. The model uses a radial basis function (RBF) kernel, which depends on a length scale parameter. You’ll have the chance to adjust this and see how it influences the behaviour of the model.\n",
    "\n",
    "As you work through the exercise, consider the following:\n",
    "\n",
    "1. How do you interpret the model? How does it update in response to new observations? Why might zero be a sensible choice for the prior mean?\n",
    "\n",
    "2. Can you define a rule for selecting the next query using only the GP posterior (without manual input)?\n",
    "\n",
    "3. Try changing the model parameters (see the top of the code cell). How does the length scale affect the model’s confidence and predictions? How does it respond to noisy data?\n",
    "\n",
    "Note: In practice, hyperparameters such as the kernel length scale are often learned using maximum likelihood. Here, they’re fixed to help you focus on understanding their effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PFWrjzlqLkpX"
   },
   "outputs": [],
   "source": [
    "#Parameters of the problem. Feel free to change them and play around with them.\n",
    "real_noise_std = 1e-10 #Real noise needs to be positive for code to work instead of zero set 1e-10\n",
    "noise_assumption = 1e-10 #Noise assumption, a hyperparameter\n",
    "\n",
    "rbf_lengthscale = 0.1 #Length scale parameter\n",
    "\n",
    "#Draw random function parameters\n",
    "modes = np.random.randint(1, 5)\n",
    "std = np.random.uniform(low = 0.005, high = 0.05, size = modes)\n",
    "means = np.random.uniform(size = modes)\n",
    "amps = np.random.uniform(size = modes) * (2 - 1) + 1\n",
    "\n",
    "#Define function\n",
    "def calc_function(x):\n",
    "  exp = -(x - means) ** 2 / std\n",
    "  y = amps * np.exp(exp)\n",
    "  return np.sum(y)\n",
    "\n",
    "#Define the kernel of the GP\n",
    "kernel = RBF(length_scale=rbf_lengthscale, length_scale_bounds='fixed')\n",
    "model = GaussianProcessRegressor(kernel = kernel, alpha=noise_assumption)\n",
    "#Standard deviation for plot\n",
    "beta = 1.96\n",
    "\n",
    "#Define function\n",
    "def calc_function(x):\n",
    "  exp = -(x - means) ** 2 / std\n",
    "  y = amps * np.exp(exp)\n",
    "  return np.sum(y)\n",
    "#Initialise query lists and maximum observations\n",
    "X, Y = [], []\n",
    "max_obs = 0\n",
    "#Initialise grid for plots\n",
    "x_grid = np.linspace(0, 1, 101).reshape(-1, 1)\n",
    "\n",
    "#Number of queries in the optimisation loop\n",
    "num_queries = 10\n",
    "\n",
    "for i in range(0, num_queries):\n",
    "  #Clear the outputs to keep the interface clean\n",
    "  clear_output(wait = True)\n",
    "  model = GaussianProcessRegressor(kernel = kernel)\n",
    "  #Fit the model\n",
    "  if i != 0:\n",
    "    model.fit(np.array(X).reshape(-1, 1), np.array(Y).reshape(-1, 1))\n",
    "  #Calculate mean and standard deviation and make\n",
    "  post_mean, post_std = model.predict(x_grid, return_std=True)\n",
    "  post_mean, post_std = post_mean.squeeze(), post_std.squeeze()\n",
    "\n",
    "  #Initialise plots\n",
    "  fig, ax = plt.subplots(figsize = (15, 7))\n",
    "  #Set the x and y limits, labels and dynamic title\n",
    "  ax.set_xlim(0, 1)\n",
    "  ax.set_ylim(0, max(max_obs + 1, 3))\n",
    "  ax.set_ylabel('f(x)')\n",
    "  ax.set_xlabel('x')\n",
    "  ax.set_title('So far you have selected ' + str(i) + ' queries, you have ' + str(10 - i) + ' left.' )\n",
    "  #Plot queries\n",
    "  ax.scatter(X, Y, c = 'r', marker='x', s = 100)\n",
    "  #Plot mean and standard deviations\n",
    "  ax.plot(x_grid.squeeze(), post_mean, label = 'GP Posterior Mean')\n",
    "  ax.fill_between(x_grid.squeeze(), post_mean - beta*post_std, post_mean + beta*post_std, alpha = 0.2, label = str(beta) + ' Standard Deviations')\n",
    "  ax.legend()\n",
    "  plt.show()\n",
    "  #Initialise x\n",
    "  x = -1\n",
    "  #Select a display format for X and Y\n",
    "  X_format =  ['%.2f' % query for query in X] #Two sig figs\n",
    "  Y_format = ['%.4f' % obs for obs in Y] #Four sig figs\n",
    "\n",
    "  while not (0 <= x <= 1): #Condition to ensure a number between zero and one is chosen\n",
    "    data = [(query, obs) for query, obs in zip(X_format, Y_format)]\n",
    "    print('Data so far (sorted by descending observations): ')\n",
    "    print('\\n'.join('{}: (x, f(x)) = {}'.format(*k) for k in enumerate(data, start = 1))) #Display data\n",
    "    x = float(input('Pick a number between 0 and 1: '))\n",
    "  #Append data, calculate function and sort lists according to observation values\n",
    "  X.append(x)\n",
    "  y = calc_function(x) + np.random.normal(scale = real_noise_std)\n",
    "  Y.append(y)\n",
    "  X = [x for _, x in sorted(zip(Y, X), reverse = True)]\n",
    "  Y.sort(reverse = True)\n",
    "  max_obs = max(max_obs, y)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "#Calculate function in the grid [0, 0.01, 0.02, ..., 0.98, 0.99, 1]\n",
    "x_grid = np.linspace(0, 1, 1001)\n",
    "y_real = []\n",
    "best_obs_grid = 0\n",
    "for x in x_grid:\n",
    "  y = calc_function(x)\n",
    "  y_real.append(y)\n",
    "  best_obs_grid = max(best_obs_grid, y) #Keep track of the best observation\n",
    "\n",
    "\n",
    "#Final GP posterior\n",
    "model.fit(np.array(X).reshape(-1, 1), np.array(Y).reshape(-1, 1))\n",
    "post_mean, post_std = model.predict(x_grid.reshape(-1, 1), return_std=True)\n",
    "post_mean, post_std = post_mean.squeeze(), post_std.squeeze()\n",
    "\n",
    "\n",
    "#Final plot and display\n",
    "fig, ax = plt.subplots(figsize = (15, 7))\n",
    "ax.plot(x_grid, y_real, 'k', label = 'f(x)')\n",
    "ax.scatter(X, Y, c = 'r', marker = 'x', label = 'Queries', s = 100)\n",
    "ax.set_ylabel('f(x)')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(bottom = 0)\n",
    "ax.set_title('Real function and all queries')\n",
    "ax.plot(x_grid.squeeze(), post_mean, label = 'GP Posterior Mean')\n",
    "ax.fill_between(x_grid.squeeze(), post_mean - beta*post_std, post_mean + beta*post_std, alpha = 0.2, label = str(beta) + ' Standard Deviations')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "print('Maximum (by Grid-Search):')\n",
    "print(best_obs_grid)\n",
    "print('Best by Yourself:')\n",
    "print(max_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. How do you interpret the model? How does it update in response to new observations? Why might zero be a sensible choice for the prior mean?**\n",
    "\n",
    "You can think of the surrogate model as a flexible map, meaning it shows its best guess of the function’s value at each point, along with how confident it is in that guess. As you add more observations, the model becomes more certain. Using zero as the prior mean is common because it keeps the initial assumptions neutral, allowing the data rather than prior beliefs to shape the predictions. This is especially useful when you have no reason to assume anything about the function’s typical value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Can you define a rule for selecting the next query using only the GP posterior (without manual input)?**\n",
    "\n",
    "Automatic rules for selecting the next point in Bayesian optimisation rely on the surrogate model’s posterior, which captures both the predicted value and the uncertainty across the space. These rules, called acquisition functions, guide the search by balancing exploration (where uncertainty is high) and exploitation (where the predicted value is promising).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters of the problem. Feel free to change them and play around with them.\n",
    "real_noise_std = 1e-10 #Real noise needs to be positive for code to work instead of zero set 1e-10\n",
    "noise_assumption = 1e-10 #Noise assumption, a hyperparameter\n",
    "\n",
    "rbf_lengthscale = 0.5 #Length scale parameter\n",
    "\n",
    "#Draw random function parameters\n",
    "modes = np.random.randint(1, 5)\n",
    "std = np.random.uniform(low = 0.005, high = 0.05, size = modes)\n",
    "means = np.random.uniform(size = modes)\n",
    "amps = np.random.uniform(size = modes) * (2 - 1) + 1\n",
    "\n",
    "#Define function\n",
    "def calc_function(x):\n",
    "  exp = -(x - means) ** 2 / std\n",
    "  y = amps * np.exp(exp)\n",
    "  return np.sum(y)\n",
    "\n",
    "#Define the kernel of the GP\n",
    "kernel = RBF(length_scale=rbf_lengthscale, length_scale_bounds='fixed')\n",
    "model = GaussianProcessRegressor(kernel = kernel, alpha=noise_assumption)\n",
    "#Standard deviation for plot\n",
    "beta = 1.96\n",
    "\n",
    "#Define function\n",
    "def calc_function(x):\n",
    "  exp = -(x - means) ** 2 / std\n",
    "  y = amps * np.exp(exp)\n",
    "  return np.sum(y)\n",
    "#Initialise query lists and maximum observations\n",
    "X, Y = [], []\n",
    "max_obs = 0\n",
    "#Initialise grid for plots\n",
    "x_grid = np.linspace(0, 1, 101).reshape(-1, 1)\n",
    "\n",
    "#Number of queries in the optimisation loop\n",
    "num_queries = 10\n",
    "\n",
    "for i in range(0, num_queries):\n",
    "  #Clear the outputs to keep the interface clean\n",
    "  clear_output(wait = True)\n",
    "  model = GaussianProcessRegressor(kernel = kernel)\n",
    "  #Fit model\n",
    "  if i != 0:\n",
    "    model.fit(np.array(X).reshape(-1, 1), np.array(Y).reshape(-1, 1))\n",
    "  #Calculate mean and standard deviation and make them one-dimensional for plotting\n",
    "  post_mean, post_std = model.predict(x_grid, return_std=True)\n",
    "  post_mean, post_std = post_mean.squeeze(), post_std.squeeze()\n",
    "\n",
    "  #Initialise plots\n",
    "  fig, ax = plt.subplots(figsize = (15, 7))\n",
    "  #Set the x and y limits, labels and dynamic title\n",
    "  ax.set_xlim(0, 1)\n",
    "  ax.set_ylim(0, max(max_obs + 1, 3))\n",
    "  ax.set_ylabel('f(x)')\n",
    "  ax.set_xlabel('x')\n",
    "  ax.set_title('So far you have selected ' + str(i) + ' queries, you have ' + str(10 - i) + ' left.' )\n",
    "  #Plot queries\n",
    "  ax.scatter(X, Y, c = 'r', marker='x', s = 100)\n",
    "  #Plot mean and standard deviations\n",
    "  ax.plot(x_grid.squeeze(), post_mean, label = 'GP Posterior Mean')\n",
    "  ax.fill_between(x_grid.squeeze(), post_mean - beta*post_std, post_mean + beta*post_std, alpha = 0.2, label = str(beta) + ' Standard Deviations')\n",
    "  ax.legend()\n",
    "  plt.show()\n",
    "  #Initialise x\n",
    "  x = -1\n",
    "  #Select a display format for X and Y\n",
    "  X_format =  ['%.2f' % query for query in X] #Two sig figs\n",
    "  Y_format = ['%.4f' % obs for obs in Y] #Four sig figs\n",
    "\n",
    "  while not (0 <= x <= 1): #Condition to ensure a number between zero and one is chosen\n",
    "    data = [(query, obs) for query, obs in zip(X_format, Y_format)]\n",
    "    print('Data so far (sorted by descending observations): ')\n",
    "    print('\\n'.join('{}: (x, f(x)) = {}'.format(*k) for k in enumerate(data, start = 1))) #Display data\n",
    "    x = float(input('Pick a number between 0 and 1: '))\n",
    "  #Append data, calculate function and sort lists according to observation values\n",
    "  X.append(x)\n",
    "  y = calc_function(x) + np.random.normal(scale = real_noise_std)\n",
    "  Y.append(y)\n",
    "  X = [x for _, x in sorted(zip(Y, X), reverse = True)]\n",
    "  Y.sort(reverse = True)\n",
    "  max_obs = max(max_obs, y)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "#Calculate function in the grid [0, 0.01, 0.02, ..., 0.98, 0.99, 1]\n",
    "x_grid = np.linspace(0, 1, 1001)\n",
    "y_real = []\n",
    "best_obs_grid = 0\n",
    "for x in x_grid:\n",
    "  y = calc_function(x)\n",
    "  y_real.append(y)\n",
    "  best_obs_grid = max(best_obs_grid, y) #Keep track of the best observation\n",
    "\n",
    "\n",
    "#Final GP posterior\n",
    "model.fit(np.array(X).reshape(-1, 1), np.array(Y).reshape(-1, 1))\n",
    "post_mean, post_std = model.predict(x_grid.reshape(-1, 1), return_std=True)\n",
    "post_mean, post_std = post_mean.squeeze(), post_std.squeeze()\n",
    "\n",
    "\n",
    "#Final plot and display\n",
    "fig, ax = plt.subplots(figsize = (15, 7))\n",
    "ax.plot(x_grid, y_real, 'k', label = 'f(x)')\n",
    "ax.scatter(X, Y, c = 'r', marker = 'x', label = 'Queries', s = 100)\n",
    "ax.set_ylabel('f(x)')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(bottom = 0)\n",
    "ax.set_title('Real function and all queries')\n",
    "ax.plot(x_grid.squeeze(), post_mean, label = 'GP Posterior Mean')\n",
    "ax.fill_between(x_grid.squeeze(), post_mean - beta*post_std, post_mean + beta*post_std, alpha = 0.2, label = str(beta) + ' Standard Deviations')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "print('Maximum (by Grid-Search):')\n",
    "print(best_obs_grid)\n",
    "print('Best by Yourself:')\n",
    "print(max_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Try changing the model parameters (see the top of the code cell). How does the length-scale affect the model’s confidence and predictions? How does it respond to noisy data?**\n",
    "\n",
    "Increasing the length-scale to 0.5 made the surrogate model smoother, helping it ignore local noise. As a result, your optimisation tracked the true maximum more closely and found a stronger overall solution.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "IMP-PCMLAI-M12-BayesianOptimNotebook.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "d37abda7630e259e5026a5079657683a09f6e3d11473720762ebe7250c494840"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
