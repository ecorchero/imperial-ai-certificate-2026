{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCQARWTCOEah"
   },
   "source": [
    "# Required assignment 15.1: Developing neural networks in Tensorflow\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this assignment, you will learn to develop basic neural networks using **TensorFlow**, a powerful and widely-used open-source machine learning framework developed by Google. TensorFlow provides a flexible and scalable platform for building and training machine learning models, with a high-level API called **Keras** that simplifies the creation of neural networks.\n",
    "\n",
    "You will start by understanding fundamental concepts such as tensors (multi-dimensional data arrays), layers, activation functions, and the model training process. Using TensorFlow’s Keras Sequential API, you will build simple feedforward neural networks, train them on sample datasets, and evaluate their performance.\n",
    "\n",
    "This hands-on assignment aims to demystify the process of neural network development by guiding you through building your first models, helping you gain practical experience with concepts like model layers, forward propagation, loss functions, and optimization techniques. By the end, you will have a foundational understanding of how to implement and train neural networks in TensorFlow.\n",
    "\n",
    "This foundation will prepare you for more advanced neural network architectures and tasks in machine learning and deep learning.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "87aZ86X-g0yW"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVIRGA4aRzVf"
   },
   "source": [
    "### Question 1:\n",
    "Create a simple TensorFlow Keras setup where you:\n",
    "\n",
    "Define a single Dense (fully connected) layer named L_1 with the following specifications:\n",
    "\n",
    "Number of output units: 2\n",
    "\n",
    "Input shape: vectors of length 2\n",
    "\n",
    "Include bias terms (use_bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "-OF5nMUUR_rs",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "38f3347965ecc5551ed4a9e00dfd9feb",
     "grade": false,
     "grade_id": "cell-8779e809cdf4aa96",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "3897bf26-1c7b-4590-9a16-ddd7107eceac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Dense name=dense_2, built=False>\n"
     ]
    }
   ],
   "source": [
    "###GRADED CELL\n",
    "L_1 = ...\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "\n",
    "L_1 = Dense(\n",
    "    units=2,\n",
    "    use_bias=True\n",
    ")\n",
    "\n",
    "print(L_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "cYusHEebSefu",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c8b44b6dbc0fba76df65e2932a51b9ab",
     "grade": true,
     "grade_id": "cell-d77ce945b3e22e95",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WBRpD1DqUiwM"
   },
   "source": [
    "### Question 2:\n",
    "\n",
    "You have already defined a Dense layer L_1 in TensorFlow Keras. In order to initialize the layer's weights and biases so that you can access and inspect them, write the code to build the layer by passing a dummy input tensor.\n",
    "\n",
    "Create a dummy input tensor filled with zeros.\n",
    "\n",
    "The dummy input should have a batch size of 1 and the same number of features as the input shape expected by L_1 (in this case, 2).\n",
    "\n",
    "Pass this dummy input to the layer L_1 to trigger weight initialization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "GgZO0dfaU1BI",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e2b7719d6a39809778d0691892cd8d14",
     "grade": false,
     "grade_id": "cell-6c5c49e02026f4c1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "50af558b-b03a-418b-ad92-84018ef395f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_2: [[ 0.4601302   0.11065233]\n",
      " [ 1.0109085  -0.08141661]] \n",
      " b_2: [0. 0.]\n"
     ]
    }
   ],
   "source": [
    "###GRADED CELL\n",
    "dummy_input = ...\n",
    "_ = ...\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "\n",
    "dummy_input = tf.zeros((1, 2))   # batch size = 1, input dimension = 2\n",
    "_ = L_1(dummy_input)             # run the layer once to build it\n",
    "\n",
    "print('A_2:', L_1.kernel.numpy(), '\\n', 'b_2:', L_1.bias.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Ip08VUSEVFfQ",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c58d3582184e368ab229d7eeacf53158",
     "grade": true,
     "grade_id": "cell-ec198426682d705e",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MrAXtptcVTTw"
   },
   "source": [
    "### Question 3: \n",
    "Given a TensorFlow Keras Dense layer L_1, define a lambda function named F_1 that takes an input tensor x and applies the layer L_1 to it, returning the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "id": "3gaErC55hoky",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "79cbe36c8fbda452abca520b97a98411",
     "grade": false,
     "grade_id": "cell-accebedeaa6281e2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### GRADED CELL\n",
    "F_1 = ...\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "\n",
    "F_1 = lambda x: L_1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "B1-Jol2OX_pH",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "67f9329931d937f0c7e4c332d012167b",
     "grade": true,
     "grade_id": "cell-e8c6f94783a486fb",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eq1NliSe2M4n"
   },
   "source": [
    "### Question 4:\n",
    "Define a TensorFlow Keras Dense layer named F_2 with 2 output units using default parameters. Then, write a function named apply_F2 that takes an input tensor x and returns the output of applying the layer F_2 to x.\n",
    "\n",
    "Your code should:\n",
    "\n",
    "Create the layer F_2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "id": "bNX2o6_9h3eb",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "16e666162a3e16023fa24fc2fd553648",
     "grade": false,
     "grade_id": "cell-6e77fd26295f83a0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "###GRADED CELL\n",
    "F_2 = ...\n",
    "apply_F2 = ...\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "\n",
    "F_2 = Dense(2) # Layer with 2 output units\n",
    "\n",
    "def apply_F2(x):\n",
    "    return F_2(x)  # Call the layer directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "sKRKQs38293m",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d378639e9611040e7836c378bc05d0eb",
     "grade": true,
     "grade_id": "cell-cccc3e3abeb4aae4",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CV5urZT4Pqn"
   },
   "source": [
    "### Question 5: \n",
    "Write code using TensorFlow 2.x and Keras that does the following:\n",
    "\n",
    "- Create a TensorFlow tensor x with float32 values [1.0, 2.0] and reshape it to shape (1, 2)—representing a batch size of 1 and 2 input features per example.\n",
    "\n",
    "- Define a Keras Dense layer named dense_layer with 3 output units and bias enabled.\n",
    "\n",
    "- Build the layer by passing the input tensor x once to initialize its weights and biases.\n",
    "\n",
    "- Define a lambda function F_2 that takes an input tensor and applies the dense_layer followed immediately by the ReLU activation function (tf.nn.relu).\n",
    "\n",
    "- Compute the output of F_2 on the input x and print the result.\n",
    "\n",
    "NOTE:- Use TensorFlow’s default float32 data type.\n",
    "\n",
    "Remember that Dense layers expect inputs shaped (batch_size, input_features).\n",
    "\n",
    "Building the layer by passing input data is required before accessing weights or using the layer meaningfully.\n",
    "\n",
    "The ReLU activation function is applied element-wise to introduce non-linearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "yXe61XS4jaQM",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "06d7ef327571d37337b4adb6c53e5850",
     "grade": false,
     "grade_id": "cell-79c1b4796454e890",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "a77afb1c-6876-4b47-f5ca-636acb99a312"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[1.3028107  0.97487974 0.        ]], shape=(1, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "###GRADED CELL\n",
    "x = ...\n",
    "dense_layer = ...\n",
    "_ = ...\n",
    "F_2 = ...\n",
    "output = ...\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "\n",
    "x = tf.constant([[1.0, 2.0]], dtype=tf.float32)   # shape (1, 2)\n",
    "dense_layer = Dense(3, use_bias=True)            # 3 output units, bias enabled\n",
    "_ = dense_layer(x)                               # build the layer (init weights & biases)\n",
    "\n",
    "F_2 = lambda x: tf.nn.relu(dense_layer(x))       # apply layer, then ReLU\n",
    "output = F_2(x)                                  # compute output on x\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "JWgY5XMp5RN3",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1581268d23f3d2660d8aec8eedfdca1a",
     "grade": true,
     "grade_id": "cell-c21a4493ce8c187e",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hN5QDsPJ6Y3v"
   },
   "source": [
    "### Question 6: \n",
    "Given the weight matrix weights and bias vector biases from a neural network layer, perform the following tasks using NumPy:\n",
    "\n",
    "- Transpose the weight matrix so that its shape becomes (units, input_dim). Store it as weights_T.\n",
    "\n",
    "- Create a column vector x_vect from the input feature vector [1., 2.] with shape (2,1) and data type float32.\n",
    "\n",
    "- Compute the affine transformation result by performing the matrix multiplication of weights_T and x_vect, then add the bias vector (reshaped as a column vector). Store the result as input_of_L2.\n",
    "\n",
    "- Apply the ReLU activation function element-wise to input_of_L2 (use np.maximum(0, ...)) and store it as output_of_L2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "NEEI1t9ajmiE",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "73ed1a659c42f3aa5b99f307c00fb739",
     "grade": false,
     "grade_id": "cell-14fce2f90fc07d4f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "c9afbe23-561a-47ae-e13c-9180eb78d4ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.3028107 ]\n",
      " [0.97487974]\n",
      " [0.        ]]\n"
     ]
    }
   ],
   "source": [
    "### GRADED CELL\n",
    "_ = dense_layer(x)  # Make sure the layer is built and initialized\n",
    "weights, biases = dense_layer.get_weights()\n",
    "weights_T = ...\n",
    "x_vect = ...\n",
    "input_of_L2 = ...\n",
    "output_of_L2 = ...\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "\n",
    "_ = dense_layer(x)                     # ensure layer built\n",
    "weights, biases = dense_layer.get_weights()\n",
    "\n",
    "weights_T = weights.T                  # transpose\n",
    "x_vect = np.array([[1.], [2.]], dtype=np.float32)\n",
    "\n",
    "input_of_L2 = weights_T @ x_vect + biases.reshape(-1, 1)\n",
    "\n",
    "output_of_L2 = np.maximum(0, input_of_L2)\n",
    "\n",
    "print(output_of_L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "TG1YJ_WL9ImS",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6637138f854d29d4e43aae14fba06d8c",
     "grade": true,
     "grade_id": "cell-815d41576e697205",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "c2aac5aa-a36f-4224-e6fc-3d6fcb3fc7d2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sm_8GEJ2-F5h"
   },
   "source": [
    "Using TensorFlow Keras, define a Dense layer named dense_layer_3 with the following specifications:\n",
    "\n",
    "- Accepts input tensors with 3 features (input shape (3,)).\n",
    "\n",
    "- Outputs 2 features (units=2).\n",
    "\n",
    "- Includes a bias term (use_bias=True).\n",
    "\n",
    "- Does not apply any activation function (activation=None).\n",
    "\n",
    "- To initialize the weights and biases, pass a dummy input tensor of ones with shape (1, 3) through the layer.\n",
    "\n",
    "- Define a lambda function named F_3 that takes an input tensor x and applies the dense_layer_3 layer to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "mYGH4zE8k7T8"
   },
   "outputs": [],
   "source": [
    "dense_layer_3 = tf.keras.layers.Dense(units=2, use_bias=True, activation=None)\n",
    "\n",
    "# Use dummy input tensor separately to build weights\n",
    "dummy_input = tf.ones((1, 3), dtype=tf.float32)\n",
    "_ = dense_layer_3(dummy_input)\n",
    "\n",
    "# Define forward function\n",
    "F_3 = lambda x: dense_layer_3(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwJz0RcpBlf0"
   },
   "source": [
    "### Question 7: \n",
    "Given three previously defined TensorFlow Keras layers (or layer-applying functions) named F_1, F_2, and F_3, define a lambda function F that composes these layers sequentially, applying F_1 to input x, then passing its output to F_2, and finally passing the result to F_3.\n",
    "\n",
    "- Write a single-line lambda function named F that, for any input tensor x, returns the result of applying F_3 on the output of F_2 on the output of F_1(x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "id": "59flfqbglXaE",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d54b172f64342cf72e805593415e2119",
     "grade": false,
     "grade_id": "cell-af609e58e898bd92",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[1.6332716 2.749324 ]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "###GRADED CELL\n",
    "F = ...\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "\n",
    "F = lambda x: F_3(F_2(F_1(x)))\n",
    "\n",
    "dummy = tf.constant([[1.0, 2.0]])\n",
    "print(F(dummy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "cSAb28s4CO_M",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fd3db51fb87f083833b28a0285b8be91",
     "grade": true,
     "grade_id": "cell-e75dadd24416c886",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDk8FPt_CqyE"
   },
   "source": [
    "Then a NumPy array [-2, 5] is converted into a TensorFlow tensor with datatype float32.  The tensor is then reshaped to add a batch dimension, resulting in a shape of (1, 2), which is the expected input shape for most neural network layers in TensorFlow.\n",
    "\n",
    "Next, the composed function F—which typically chains multiple layers or operations—is applied to this input tensor x. Finally, the computed output tensor y is printed.\n",
    "\n",
    "This process demonstrates how to prepare raw input data, convert and reshape it appropriately for TensorFlow models, and perform inference using a composed neural network function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CepxASW1l43P",
    "outputId": "b87a34d4-f65e-4fd4-e749-3ccc8923e32d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[2.5793755 4.0623713]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert numpy array to TensorFlow tensor, specifying float32 to match PyTorch's .float()\n",
    "x = tf.convert_to_tensor(np.array([-2, 5], dtype=np.float32))  # [2][6][1]\n",
    "\n",
    "# Reshape the input to include a batch dimension (1, 2)\n",
    "x = tf.reshape(x, (1, 2))\n",
    "\n",
    "# Assuming F is your composed TensorFlow function/layer as discussed earlier\n",
    "y = F(x)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2nH_DQZaH7wn"
   },
   "source": [
    "The model created can be visualized using computational graph using `tf.keras.utils.plot_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "o4DS6h74mQUX",
    "outputId": "94969a93-3eb3-47b0-b828-763b01fa62dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[ 0.864764  -1.0787934]], shape=(1, 2), dtype=float32)\n",
      "You must install pydot (`pip install pydot`) for `plot_model` to work.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ W1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ W2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ relu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ W3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ W1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                 │             \u001b[38;5;34m6\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ W2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)                 │             \u001b[38;5;34m9\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ relu (\u001b[38;5;33mReLU\u001b[0m)                     │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)                 │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ W3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                 │             \u001b[38;5;34m8\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23</span> (92.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m23\u001b[0m (92.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23</span> (92.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m23\u001b[0m (92.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# !pip install pydot\n",
    "from IPython.display import Image, display\n",
    "# Sequential model construction\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Dense(2, name=\"W1\"),\n",
    "    layers.Dense(3, name=\"W2\"),\n",
    "    layers.ReLU(name=\"relu\"),\n",
    "    layers.Dense(2, name=\"W3\")\n",
    "])\n",
    "\n",
    "# Input tensor (batch of shape (1, 2))\n",
    "x = tf.convert_to_tensor(np.random.randn(1, 2).astype(np.float32))\n",
    "\n",
    "# Forward pass\n",
    "y = model(x)\n",
    "print(y)\n",
    "# (Optional) Visualize the computational graph\n",
    "tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True)\n",
    "# Display in Jupyter Notebook\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
