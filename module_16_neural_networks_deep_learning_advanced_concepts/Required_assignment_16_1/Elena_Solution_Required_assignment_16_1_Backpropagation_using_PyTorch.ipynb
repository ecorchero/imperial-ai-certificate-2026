{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "IROq4j4-sjH8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "IROq4j4-sjH8",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2bdee213442fe595f20d9032e873ae60",
     "grade": false,
     "grade_id": "cell-927a4fda810c580b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Required assignment 16.1: Backpropagation using PyTorch\n",
    "\n",
    "In this activity, you explore the fundamental concepts of backpropagation, a core technique used in training neural networks. At its heart, deep learning is an optimisation problem where you adjust the network's weights to minimise a loss function that measures how well the network‚Äôs predictions match the target values.\n",
    "\n",
    "To effectively minimise this loss, you rely on calculus, specifically the computation of gradients (partial derivatives) of the loss with respect to the weights. Backpropagation efficiently computes these gradients using the chain rule, breaking down complex computations into simpler steps visualised through a computational graph.\n",
    "\n",
    "You will learn about two modes of differentiation ¬≠‚Äì forward mode and backward mode ‚Äì each suited to different network input‚Äìoutput configurations. Understanding these concepts is critical for implementing and optimising neural networks through algorithms such as stochastic gradient descent.\n",
    "\n",
    "This activity will guide you through the mechanics of backpropagation and the chain rule, equipping you with a solid foundation for neural network training and optimisation.\n",
    "To work on this assignment, you require `aima3` and `torchviz`. If these are not installed, please ensure to use `pip install aima3` and `pip install torchviz` to install these packages before importing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cbb1049",
   "metadata": {
    "id": "2cbb1049"
   },
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import sys\n",
    "from aima3 import learning\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "from torch.autograd import Variable\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2434d2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "cb2434d2",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5c80799208ed72bc741b5ed578274fd4",
     "grade": false,
     "grade_id": "cell-9d0d1cb1c527f2ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Backpropagation and the chain rule\n",
    "\n",
    "\n",
    "*Foreword: In this notebook, we use slightly different terminology. An arbitrary training instance is denoted as $(v, y) \\in E$ where $v$ is the collection of predictors, $y$ is the target and $E$ is the training set. Moreover, the network weights are denoted by $x$.*\n",
    "\n",
    "Deep learning is fundamentally a giant problem in optimisation. You are choosing numerical 'weights' to minimise a loss function $L$ (which depends on those weights). **This is the learning part.** In other words,\n",
    "$$L(x) = \\sum_{(v, y) \\in \\boldsymbol{E}} \\text{loss}(F(x, v) - y).$$\n",
    "Calculus tells us that the minimiser of $L$ satisfies the following system of equations (there may be many solutions that satisfy this, hence you do not necessarily obtain the minimiser ‚Äì you just hope it's something 'good enough'):\n",
    "\n",
    "> **The partial derivatives of L with respect to the weights $x$ should be zero**: $$\\boxed{\\frac{\\partial L}{\\partial x} = 0 }$$\n",
    "\n",
    "You solve the equation above, iteratively, using a modification of the gradient descent method called **stochastic gradient descent**.\n",
    "\n",
    "*Backpropagation* is a method to compute derivatives quickly, using the chain rule:\n",
    "\n",
    "$$\\frac{dF}{dx} = \\frac{d}{dx}(F_3(F_2(F_1 (x))) = \\frac{dF_3}{dF_2}\\vert_{F_2=F_2(F_1(x))} \\frac{dF_2(F_1(x))}{dF_1}\\vert_{F_1 = F_1(x)} \\frac{dF_1(x)}{dx}\\vert_x.$$\n",
    "\n",
    "A convenient way to visualise how the function $F$ is computed from the weights $x_i$ is to use a **computational graph**. It separates the big computation into small steps, and you can find the derivative of each step (each computation) on the graph.\n",
    "\n",
    "**Backpropagation** is a technique for optimising parameters in a neural network. There are two types of backpropagation, depending on the relation between the number of inputs and outputs in the neural network:\n",
    "- *Forward mode*: $F$ has few inputs but many outputs\n",
    "- *Backward mode*: $F$ has many inputs but few outputs\n",
    "\n",
    "*Forward mode* differentiation tracks how one input affects every node. *Reverse mode* differentiation tracks how every node affects one output. That is, forward mode differentiation applies the operator $\\dfrac{\\partial (.)}{\\partial x}$ to every node, while reverse mode differentiation applies the operator $\\dfrac{\\partial F}{\\partial (.)}$ to every node. The general rule is to sum over all possible paths from one node to the other, multiplying the derivatives on each edge of the path together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RQbe5WpXw-vx",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "RQbe5WpXw-vx",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5ef189d97bf94f409d816e041dc2bb16",
     "grade": false,
     "grade_id": "cell-e2d2224348282a4a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Question 1:\n",
    "\n",
    "Consider the computational graph shown below:\n",
    "<img src=\"images/image1.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "$$\n",
    "c = x^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "s = x + y\n",
    "$$\n",
    "\n",
    "$$\n",
    "F = c x s\n",
    "$$\n",
    "where the initial values of x = 2 and y = 3.\n",
    "\n",
    "- Compute the derivative $$‚àÇF/‚àÇx$$ using forward-mode differentiation.\n",
    "- Compute the same derivative $$‚àÇF/‚àÇx$$\n",
    "  using backward mode automatic differentiation implemented in PyTorch.\n",
    "- Verify your backward mode derivative by printing the gradient stored in $$x.grad$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_yeErJ-O0Sv7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "_yeErJ-O0Sv7",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d776ade898a0a98e65d68b4c68dbdf62",
     "grade": false,
     "grade_id": "cell-f8a65fb7a98715a1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Question 1a:\n",
    "\n",
    "Implement the example in PyTorch.\n",
    "- Provide the inputs `x` and `y` using `Variable(torch.tensor(2.), requires_grad=True)` and `Variable(torch.tensor(3.),requires_grad=True)` respectively.\n",
    "- From the computation graph given above, input the values of `c`, `s` and `F`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b085919f",
   "metadata": {
    "deletable": false,
    "id": "b085919f",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ebe1917a68351d9a801f1b773d2c3788",
     "grade": false,
     "grade_id": "cell-afb61e4ac7cc7c09",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### GRADED CELL\n",
    "x = None\n",
    "y = None\n",
    "c = None\n",
    "s = None\n",
    "F = None\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()\n",
    "\n",
    "# inputs with gradients enabled\n",
    "x = Variable(torch.tensor(2.0), requires_grad=True)\n",
    "y = Variable(torch.tensor(3.0), requires_grad=True)\n",
    "\n",
    "# computational graph\n",
    "c = x ** 2      # c = x^2\n",
    "s = x + y       # s = x + y\n",
    "F = c * s       # F = c * s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "I0EX3jfl49o-",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "I0EX3jfl49o-",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7d30c8f0792fb6697682ce9951967336",
     "grade": true,
     "grade_id": "cell-fa905f501be01a26",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e0TmRCf5_cI",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5e0TmRCf5_cI",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4621452498bf9c2dd36ca64a110f6056",
     "grade": false,
     "grade_id": "cell-d623f2d0e5711a88",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Question 1b:\n",
    "Write the PyTorch command that computes the gradients of \\( F \\) using backward propagation **while keeping the computational graph intact** for further gradient computations.\n",
    "\n",
    "\n",
    "\n",
    "*Hint:* Use the appropriate argument in the `backward()` method to retain the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a329c3b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "5a329c3b",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "81a41fa08ab3fa7571c111f428c362ea",
     "grade": false,
     "grade_id": "cell-db91868eb0f5e32f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "d7ccbe82-4062-46c0-f008-a41fb31536ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.0\n"
     ]
    }
   ],
   "source": [
    "###GRADED CELL\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "\n",
    "# compute gradients of F w.r.t. x and y\n",
    "F.backward(retain_graph=True)\n",
    "\n",
    "print(x.grad.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a84c6e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "d0a84c6e",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e56b62831a118cbc7f69a8f5a8695ac5",
     "grade": true,
     "grade_id": "cell-049caece19d256ff",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e18e2a05",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "e18e2a05",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b648d7f5ab671aef4622baf1ab3961ab",
     "grade": false,
     "grade_id": "cell-1b047368ca565688",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Question 2:\n",
    "\n",
    "Let $F = \\log(x) + x^2 y + y^2$.\n",
    "\n",
    "Evaluate $\\dfrac{\\partial{F}}{\\partial{x}}$ and $\\dfrac{\\partial{F}}{\\partial{y}}$ at the point $x = 2$, $y = 3$ (both in forward and backward modes)\n",
    " by using `torch`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe6c890",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5fe6c890",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "76729239eb2990b834cf627c9894da46",
     "grade": false,
     "grade_id": "cell-76ae7fe99da30383",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Answer:**\n",
    "\n",
    "You have $\\dfrac{\\partial F}{\\partial x} = 1/x + 2xy$, which evaluates to $12.5$ at $(x=2, y=3)$. Moreover, you also have $\\dfrac{\\partial F}{\\partial y} = x^2 + 2y$, which evaluates to $10$ at $(x=2, y=3)$. To compute this, three ways are demonstrated below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfad2d1a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "dfad2d1a",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a706868323b4aceba59697d45483d904",
     "grade": false,
     "grade_id": "cell-6f3089763e64b2d3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, you can solve this problem simply by using `torch`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Xl64-kUaEWp7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Xl64-kUaEWp7",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "497297ca929c8294e3abc26e98ec1be4",
     "grade": false,
     "grade_id": "cell-7b881fe73598ad88",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You are given:\n",
    "$$\n",
    "F(x, y) = \\ln x + x^2y + y^2\n",
    "$$\n",
    "\n",
    "You need to:\n",
    "1. **Break the function into intermediate variables**:  \n",
    "   Let  \n",
    "  $$\n",
    "   a = \\ln x,\\quad b = x^2y,\\quad c = y^2\n",
    "   $$\n",
    "   and  \n",
    " $$\n",
    "   F = a + b + c.\n",
    "   $$\n",
    "\n",
    "2. **Find the local derivatives** for each intermediate variable with respect to its inputs:  \n",
    "   \n",
    "\n",
    "3. **Apply the chain rule** to compute:  \n",
    "  $$\n",
    "   \\frac{\\partial F}{\\partial x}\n",
    "   \\quad\\text{and}\\quad\n",
    "   \\frac{\\partial F}{\\partial y}.\n",
    "   $$\n",
    "\n",
    "4. **Evaluate the derivatives** at:\n",
    "  $$\n",
    "   x = 2, y = 3\n",
    "  $$\n",
    "\n",
    "5. **Verify the results** using PyTorch ‚Äì The solved answer in pen and paper is provided above.\n",
    "\n",
    "###Instructions:\n",
    "- Assign input variables to `x` and `y` using `Variable(torch.tensor())`. HINT: Refer to Question 1.\n",
    "- Assign `torch.log(x)` to `a`.\n",
    "- Refer to the equation and assign value to `b`.\n",
    "- Refer to the equation and assign value to `c`.\n",
    "- Compute `F` which is the summation of `a`, `b` and `c`.\n",
    "- Compute `F.backward()`.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26d58681",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "26d58681",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9083f3e8139683538f9e6d8d2bd3af65",
     "grade": false,
     "grade_id": "cell-5ddb3e57910096b6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "33a19111-3d12-4208-cb31-8b37377390fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "derivative WRT x: 12.5 WRT y: 10.0\n"
     ]
    }
   ],
   "source": [
    "###GRADED CELL\n",
    "x = None\n",
    "y = None\n",
    "a = None\n",
    "b = None\n",
    "c = None\n",
    "F = None\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "\n",
    "# inputs\n",
    "x = Variable(torch.tensor(2.0), requires_grad=True)\n",
    "y = Variable(torch.tensor(3.0), requires_grad=True)\n",
    "\n",
    "# intermediate values\n",
    "a = torch.log(x)      # a = ln(x)\n",
    "b = x**2 * y          # b = x^2 * y\n",
    "c = y**2              # c = y^2\n",
    "\n",
    "# final expression\n",
    "F = a + b + c\n",
    "\n",
    "# compute gradients\n",
    "F.backward()\n",
    "\n",
    "print(\"derivative WRT x:\", x.grad.numpy(), \"WRT y:\", y.grad.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mxEak56EMgt-",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "mxEak56EMgt-",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "89ff4d33d48f131820f84722893ed9aa",
     "grade": true,
     "grade_id": "cell-057d2c73176ce914",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ced715c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "2ced715c",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0f823483f0fdb658c8e7ab6ab1073ee8",
     "grade": false,
     "grade_id": "cell-97434b3091d2f628",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Stochastic Gradient Descent (SGD)\n",
    "\n",
    "1. Weight initialization:\n",
    "the starting weights $$ùë•_0$$\n",
    "  in gradient descent strongly affect training. Poor initialisation (e.g. all zeros or wrong variance) can cause exploding or vanishing weights, preventing convergence. Layer width and variance together control stability.\n",
    "\n",
    "2. Gradient descent & SGD:\n",
    "Gradient descent updates weights iteratively using:\n",
    "\n",
    "$$\n",
    "x_{k+1} = x_k - s_k \\nabla L(x_k)\n",
    "$$\n",
    "SGD speeds up training and improves generalisation by computing gradients on small random mini-batches instead of the entire data set.\n",
    "\n",
    "3. Practical training stability;\n",
    "SGD avoids overfitting in practice and benefits from techniques such as momentum, adaptive learning rates (e.g. Adam) and early stopping to converge faster and maintain good performance on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4HhHgGB2Qm92",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "4HhHgGB2Qm92",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b35857f155cdbc702863f6d1deb33e6a",
     "grade": false,
     "grade_id": "cell-322b985138185d8e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Steps of SGD\n",
    "\n",
    "1. Mini-batches: random subsets of data are selected in each iteration to compute a noisy estimate of the gradient.\n",
    "\n",
    "2. Forward pass: compute predictions using current weights.\n",
    "\n",
    "3. Loss calculation: measure difference between predictions and true labels.\n",
    "\n",
    "4. Backward pass: use automatic differentiation to find gradients.\n",
    "\n",
    "5. Weight update: adjust weights incrementally in the opposite direction of gradients.\n",
    "\n",
    "    Repeat: this process iterates multiple times, progressively reducing the loss and improving model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bf17a51",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9bf17a51",
    "outputId": "dec1a570-589a-44a5-a422-12a54824d6db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.28246793150901794 [-0.81711113  1.2332046 ]\n",
      "10 0.32800012826919556 [-0.77026129  1.24072444]\n",
      "20 0.25425708293914795 [-0.73008639  1.25254464]\n",
      "30 0.015801629051566124 [-0.69650716  1.25042665]\n",
      "40 0.36563342809677124 [-0.6607812   1.24708831]\n",
      "50 0.1696881800889969 [-0.63260621  1.24352777]\n",
      "60 0.18371504545211792 [-0.60611552  1.23942494]\n",
      "70 0.14574940502643585 [-0.58907056  1.22756052]\n",
      "80 0.07515283674001694 [-0.56214428  1.2258997 ]\n",
      "90 0.18182028830051422 [-0.53988492  1.22007561]\n",
      "100 0.1958242505788803 [-0.51229918  1.20963466]\n",
      "110 0.13647663593292236 [-0.48752689  1.20617306]\n",
      "120 0.09315693378448486 [-0.47454092  1.19189417]\n",
      "130 0.08985738456249237 [-0.4555679  1.1838727]\n",
      "140 0.015101850964128971 [-0.44640636  1.17761028]\n",
      "150 0.06805810332298279 [-0.42687866  1.17529058]\n",
      "160 0.14071322977542877 [-0.40481341  1.16979659]\n",
      "170 0.022882983088493347 [-0.38662228  1.15833724]\n",
      "180 0.08650193363428116 [-0.37365228  1.1498785 ]\n",
      "190 0.1384269744157791 [-0.3633486   1.13890707]\n",
      "200 0.2201676219701767 [-0.34735438  1.1246686 ]\n",
      "210 0.058584630489349365 [-0.335843   1.1186564]\n",
      "220 0.06407057493925095 [-0.32213432  1.11228716]\n",
      "230 0.052951112389564514 [-0.30548999  1.10119319]\n",
      "240 0.12152612954378128 [-0.29619169  1.09002078]\n",
      "250 0.08234313875436783 [-0.28646997  1.07924378]\n",
      "260 0.052052371203899384 [-0.27713275  1.06217515]\n",
      "270 0.07432971894741058 [-0.26288891  1.05065107]\n",
      "280 0.08854128420352936 [-0.24796623  1.04516399]\n",
      "290 0.044299982488155365 [-0.23843376  1.03289008]\n",
      "300 0.047428082674741745 [-0.2310634   1.02203417]\n",
      "310 0.03719450533390045 [-0.22278126  1.01596069]\n",
      "320 0.0603448785841465 [-0.21332476  1.01030016]\n",
      "330 0.0560731440782547 [-0.20277777  1.00255775]\n",
      "340 0.04071374982595444 [-0.19125471  1.00026524]\n",
      "350 0.04175814613699913 [-0.18264446  0.99129283]\n",
      "360 0.06847836077213287 [-0.17357625  0.98371828]\n",
      "370 0.048474062234163284 [-0.16579431  0.97275329]\n",
      "380 0.01129129808396101 [-0.16613677  0.96390206]\n",
      "390 0.010913649573922157 [-0.15832882  0.9586705 ]\n",
      "400 0.08562911301851273 [-0.16018821  0.94369495]\n",
      "410 0.017140822485089302 [-0.15146735  0.92772746]\n",
      "420 0.024638788774609566 [-0.13775116  0.92576206]\n",
      "430 0.08746793121099472 [-0.1333012   0.91467792]\n",
      "440 0.006953065283596516 [-0.13191554  0.90835738]\n",
      "450 0.030325762927532196 [-0.12011765  0.90275222]\n",
      "460 0.015519684180617332 [-0.1140737   0.89646977]\n",
      "470 0.022222153842449188 [-0.10490347  0.890526  ]\n",
      "480 0.03694097697734833 [-0.09494229  0.88474923]\n",
      "490 0.027378518134355545 [-0.08216846  0.88241255]\n",
      "final weights: tensor([[-0.0782],\n",
      "        [ 0.8762]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "B, N, D_in, D_out = 4, 20, 2, 1\n",
    "xrange = yrange = np.arange(0.0, 1.0, 0.1)\n",
    "g = np.meshgrid(xrange, yrange, sparse=False, indexing='ij')\n",
    "_x = np.vstack(tup=tuple(map(np.ravel, g))).T\n",
    "_w = np.array((0.4, 0.2)).reshape(1, -1).T\n",
    "eps = 1e-2  #Define eps here\n",
    "_y = _x.dot(_w) + eps * np.random.rand(_x.shape[0], 1)\n",
    "\n",
    "#Select a small sample of the data\n",
    "\n",
    "np.random.seed(42)\n",
    "idx = np.random.randint(0, 100, N)\n",
    "x_np = _x[idx]\n",
    "y_np = _y[idx]\n",
    "\n",
    "#Create random tensors to hold inputs and outputs.\n",
    "\n",
    "x = Variable(torch.Tensor(x_np))\n",
    "y = Variable(torch.Tensor(y_np))\n",
    "\n",
    "#Create random tensors for weights.\n",
    "#Setting requires_grad=True indicates that you want to compute gradients with respect to these tensors during the backward pass.\n",
    "w = torch.randn(D_in, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-2\n",
    "epochs = 500\n",
    "weights = np.empty((epochs//10, 2))\n",
    "losses = np.empty(epochs//10)\n",
    "for t in range(epochs):\n",
    "    sample = np.random.randint(0, 20, B)\n",
    "    x_B, y_B = x[sample], y[sample]\n",
    "    # Forward pass: compute predicted y using operations on Tensors; \n",
    "    y_pred = x_B.mm(w)\n",
    "\n",
    "    #Compute and print loss using operations on tensors.\n",
    "    #Now, loss is a tensor of shape (1,)\n",
    "    #loss.item() gets the scalar value held in the loss.\n",
    "    loss = (y_pred - y_B).pow(2).mean()\n",
    "\n",
    "    #Use autograd to compute the backward pass. \n",
    "    loss.backward()\n",
    "\n",
    "    #Manually update weights using gradient descent. Wrap in torch.no_grad().\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad #This is the gradient of loss with respect to w\n",
    "\n",
    "        #Manually zero the gradients after updating weights\n",
    "        w.grad.zero_()\n",
    "\n",
    "    if t % 10 == 0:\n",
    "        ind = int(t/10)\n",
    "        losses[ind] = loss.item()\n",
    "        weights[ind, :] = w.data.view(1, -1).numpy()[0]\n",
    "        print(t, losses[ind], weights[ind, :])\n",
    "\n",
    "#Compare this with the initial weights you had set up in your data.\n",
    "print('final weights:', w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fbf4d6-4913-460a-9af1-5c364a45d6b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
