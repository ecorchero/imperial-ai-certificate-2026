{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "vwVQqeA6yFeb",
   "metadata": {
    "id": "vwVQqeA6yFeb"
   },
   "source": [
    "# Self-study try-it activity 16.1: Jupyter Notebook on using PyTorch\n",
    "\n",
    "To compute with neural networks, you will be using the Python library `torch`.\n",
    "\n",
    "Ensure that the libraries `aima3` and `torchviz` are already installed. If not, please use `pip install aima3` and `pip install torchviz` to install these before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb90ydZwlWz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8cb90ydZwlWz",
    "outputId": "7de80417-050c-4246-f2b7-7c6d9eaf3453"
   },
   "outputs": [],
   "source": [
    "pip install aima3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e10042",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "65e10042",
    "outputId": "b3b5ab1c-8917-4ac8-ffdb-a757ffe40dd7"
   },
   "outputs": [],
   "source": [
    "!pip install torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ea8cb2",
   "metadata": {
    "id": "66ea8cb2"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from aima3 import learning\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PxHzc8sjz7FH",
   "metadata": {
    "id": "PxHzc8sjz7FH"
   },
   "source": [
    " `from aima3 import learning` imports the machine learning module from the aima3 package, which implements algorithms and concepts from the book \"Artificial Intelligence: A Modern Approach.\" This allows you to access various AI learning functionalities provided by the package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62148d4",
   "metadata": {
    "id": "c62148d4"
   },
   "source": [
    "### 1. Using PyTorch for neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7308e7a",
   "metadata": {
    "id": "a7308e7a"
   },
   "source": [
    "To compute with neural networks, you will be using the Python library `torch`:\n",
    "```python\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "x = torch.rand(5, 3) #Example usage\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0926508b",
   "metadata": {
    "id": "0926508b"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f03ba0",
   "metadata": {
    "id": "71f03ba0"
   },
   "source": [
    "This code demonstrates how to define a linear layer in PyTorch using the `torch.nn` module.\n",
    "\n",
    "- The line `import torch.nn as nn` imports PyTorch's neural network module, which provides tools to build neural networks.\n",
    "- `F_1 = nn.Linear(in_features=2, out_features=2, bias=True)` creates a linear layer with two input features and two output features. The `bias=True` argument means the layer includes a bias term in its calculations.\n",
    "- This linear layer performs a linear transformation on the input data, defined mathematically as \\( y = xW^T + b \\), where \\(x\\) is the input, \\(W\\) is the weight matrix and \\(b\\) is the bias vector.\n",
    "- The `print` statement displays the randomly initialised weights and bias parameters of the layer.\n",
    "\n",
    "This layer is typically a building block in neural networks, representing a fully connected (dense) layer where each input feature is connected to each output feature via learned weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b046ca9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6b046ca9",
    "outputId": "833441ee-a465-4243-fd0c-fc4f84437acf"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "\n",
    "#In PyTorch, you have to start defining the linear functions of each layer\n",
    "F_1 = nn.Linear(in_features=2, out_features=2, bias=True) #Bias = True adds the bias term for input - layer 1 weights\n",
    "print('A_2:', F_1.weight, '\\n', 'b_2:', F_1.bias) #Weights are assigned randomly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b729fcd",
   "metadata": {
    "id": "6b729fcd"
   },
   "source": [
    "From `F_2` above, you can define a function in Python using either the `lambda` or the `def` keywords:\n",
    "\n",
    "```python\n",
    "def F_2(x):\n",
    "    return F_2.forward(x) #Or just F_2(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afd0566",
   "metadata": {
    "id": "1afd0566"
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "  return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e1c036",
   "metadata": {
    "id": "39e1c036"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lp065lGt336E",
   "metadata": {
    "id": "lp065lGt336E"
   },
   "source": [
    "1. An input tensor in PyTorch is created by converting a NumPy array with specific values and data type.\n",
    "\n",
    "2. A linear layer in PyTorch is defined, which maps the input features to a specified number of output features and includes a bias term.\n",
    "\n",
    "3. A function `F_2` is defined, which applies the linear layer `L_2` followed by a ReLU activation function to the input.\n",
    "\n",
    "4. Finally, evaluate the input tensor by passing it through the defined linear layer and ReLU activation function to get the output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becf51d0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "becf51d0",
    "outputId": "c6eb7d1a-26be-4666-9411-b9e594b769e6"
   },
   "outputs": [],
   "source": [
    "x = torch.from_numpy(np.array([1., 2.], dtype=np.float64)).float() #Use numpy commands – example input to Layer 2\n",
    "L_2 = nn.Linear(in_features=2, out_features=3, bias=True) #Define the second step -> 2 - to 3 + the bias term\n",
    "F_2 = lambda x: func.relu(L_2.forward(x)) #F_2 applies the ReLU activation on the L_2 evaluation.\n",
    "F_2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5526e081",
   "metadata": {
    "id": "5526e081"
   },
   "source": [
    "You can apply the above steps by simple algebra, so what `torch` applies is not a mystery:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5HgUvMzK5B7y",
   "metadata": {
    "id": "5HgUvMzK5B7y"
   },
   "source": [
    "- Next, extract the weights and biases from the PyTorch linear layer and convert them to NumPy arrays for matrix operations.\n",
    "\n",
    "- Perform the affine transformation on the input by multiplying the weight matrix with the input vector and adding the bias, then apply the ReLU activation function to get the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc6db8e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0cc6db8e",
    "outputId": "25270514-0895-49a7-bbb4-16929a0efd50"
   },
   "outputs": [],
   "source": [
    "weights = L_2.weight.detach().numpy() #The weights L2 uses are taken\n",
    "x_vect = x.detach().numpy().reshape(2,1) #You take the input as a NumPy array\n",
    "input_of_L_2 = weights.dot(x_vect) + L_2.bias.detach().numpy().reshape(3,1) #Now, apply the affine transformation\n",
    "output_of_L2 = relu(input_of_L_2) #Composition with the ReLu function (activate the input)\n",
    "output_of_L2 #This is what F_2 returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb408fb",
   "metadata": {
    "id": "ebb408fb"
   },
   "source": [
    "Finally, you apply the last layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159ef22c",
   "metadata": {
    "id": "159ef22c"
   },
   "outputs": [],
   "source": [
    "L_3 = nn.Linear(in_features=3, out_features=2, bias=True)\n",
    "F_3 = lambda x: L_3.forward(x) #This is just evaluation; here, you do not apply ReLU (or this is \"identity activation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282f6e83",
   "metadata": {
    "id": "282f6e83"
   },
   "source": [
    "You defined all layers. Now, to make a prediction, you can simply compose the layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7fa4cd",
   "metadata": {
    "id": "cb7fa4cd"
   },
   "outputs": [],
   "source": [
    "F = lambda x: F_3(F_2(F_1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f585573b",
   "metadata": {
    "id": "f585573b"
   },
   "source": [
    "And finally, giving an input to `F` will return an output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaf6afd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5eaf6afd",
    "outputId": "07abc1e3-d6a4-429f-e4c6-19534eba0070"
   },
   "outputs": [],
   "source": [
    "x = torch.from_numpy(np.array([1., 2.], dtype=np.float64)).float()\n",
    "y = F(x)\n",
    "print(\"The output is given by\" ,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2hV5A0A-5gyp",
   "metadata": {
    "id": "2hV5A0A-5gyp"
   },
   "source": [
    "#To-do:\n",
    "For the input [-2, 5], use the torch network created and compute the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9092de4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b9092de4",
    "outputId": "b7e76314-ec74-49cf-d2f8-5907b719814d"
   },
   "outputs": [],
   "source": [
    "x = torch.from_numpy(np.array([-2, 5], dtype=np.float64)).float()\n",
    "y = F(x)\n",
    "print(\"The output is given by\" ,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1986d423",
   "metadata": {
    "id": "1986d423"
   },
   "source": [
    "Note that `F` is just a function with fixed weights since `torch` automatically assigns initial weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add16242",
   "metadata": {
    "id": "add16242"
   },
   "source": [
    "#### 2. Visualising a network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Aibcic_b7fjg",
   "metadata": {
    "id": "Aibcic_b7fjg"
   },
   "source": [
    "## Neural network model using `nn.Sequential` container and visualisation using `torchviz`\n",
    "\n",
    "This code constructs a small neural network model using PyTorch’s `nn.Sequential` container, runs a random input through it and visualises the computation graph with `torchviz`.\n",
    "\n",
    "- **Importing modules**:\n",
    "  - `Variable` from `torch.autograd` is used to wrap tensors for automatic differentiation (though in newer PyTorch versions, tensor itself has this capability).\n",
    "  - `make_dot` from `torchviz` is used to create a visual graph of the computations for the model’s output tensor.\n",
    "\n",
    "- **Defining the model with `nn.Sequential`**:\n",
    "  - `nn.Sequential()` is a sequential container where layers and functions are added in order.\n",
    "  - `model.add_module('W1', nn.Linear(2, 2))` adds the first linear layer with two input features and two output features.\n",
    "  - `model.add_module('W2', nn.Linear(2, 3))` adds the second linear layer mapping from two to three features.\n",
    "  - `model.add_module('relu', nn.ReLU())` adds a ReLU activation function to introduce non-linearity.\n",
    "  - `model.add_module('W3', nn.Linear(3, 2))` adds a final linear layer mapping from three to two output features.\n",
    "\n",
    "- **Creating input and computing output**:\n",
    "  - `x = Variable(torch.randn(1, 2))` creates a single random input tensor with two features wrapped as a variable to track operations for automatic differentiation.\n",
    "  - `y = model(x)` forwards the input `x` through the model, producing an output tensor `y`.\n",
    "\n",
    "- **Visualising the computation graph**:\n",
    "  - `make_dot(y, params=dict(model.named_parameters()))` generates a graphical visualisation of the computation graph starting from output `y`.\n",
    "  - This graph illustrates the flow of data and operations through the layers, including weights and biases from each layer in the model.\n",
    "  \n",
    "This code is useful for defining a simple feedforward neural network, executing a forward pass on random data and visually inspecting the computation graph to understand how tensors and operations connect across the network layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6c10a2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 660
    },
    "id": "0e6c10a2",
    "outputId": "d8b0f5bf-095f-4823-987c-caa7eb40f9ad"
   },
   "outputs": [],
   "source": [
    "\n",
    "from torch.autograd import Variable\n",
    "from torchviz import make_dot\n",
    "model = nn.Sequential()\n",
    "model.add_module('W1', nn.Linear(2,2)) #Add a layer with linear transformation as before\n",
    "model.add_module('W2', nn.Linear(2,3)) #Add another layer similarly\n",
    "model.add_module('relu', nn.ReLU()) #Add a ReLU activation\n",
    "model.add_module('W3', nn.Linear(3, 2)) #Add a final linear layer\n",
    "x = Variable(torch.randn(1, 2)) #Input variable\n",
    "y = model(x)\n",
    "make_dot(y, params=dict(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bede946b",
   "metadata": {
    "id": "bede946b"
   },
   "source": [
    "You can change the activation functions and number of layers for different purposes. For example, you can represent logistic regression with the architecture shown in the beginning of this notebook. You can visualise the architecture by using `torchviz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf94afff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "cf94afff",
    "outputId": "1684588c-9eb8-4191-9736-e15caff85d4f"
   },
   "outputs": [],
   "source": [
    "#This is how a simple logistic regression would look like\n",
    "model = nn.Sequential()\n",
    "model.add_module('W0', nn.Linear(2, 2))\n",
    "model.add_module('logit', nn.Sigmoid()) #Sigmoid activation\n",
    "x = Variable(torch.randn(1, 2))\n",
    "y = model(x)\n",
    "make_dot(y.max(), params=dict(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c786ac",
   "metadata": {
    "id": "71c786ac"
   },
   "source": [
    "### Demonstration of logistic regression via a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3353c66",
   "metadata": {
    "id": "e3353c66"
   },
   "source": [
    "For a binary classification problem, using logistic regression with two input features $$x_1$$ and $$x_2$$, the probability of an instance belonging to the positive class \\(y = +1\\) is given by:\n",
    "$$ \\mathbb{P}[y = 1 | x_1, x_2] = \\dfrac{1}{ 1 + \\exp( - w_0 - w_1 x_1 -  w_2 x_2)} = s(w_0 + w_1x_1 + w_2x_2).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf215b84",
   "metadata": {
    "id": "cf215b84"
   },
   "source": [
    "### To-do:\n",
    "Find the probability $\\mathbb{P}[y = 1 | x_1, x_2]$ where $w_0 = 1, w_1 = 0.3, w_2 = -0.1$ for the input $x = (x_1 = 1, x_2 = -2)$.\n",
    "\n",
    "Try this using simple `sigmoid()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa995aea",
   "metadata": {
    "id": "fa995aea"
   },
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426a5817",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "426a5817",
    "outputId": "3612166e-f073-480c-f46e-37ec59db3a87",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w0,w1,w2 = 1, 0.3, -0.1\n",
    "x1,x2 = 1, -2\n",
    "prob = sigmoid(w0 + w1*x1 + w2*x2)\n",
    "round(prob,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b97881",
   "metadata": {
    "id": "e8b97881"
   },
   "source": [
    "### To-do:\n",
    "Model the same with a simple neural network. Report the probability of the same input belonging to class +1 when the weights are fixed accordingly.\n",
    "*(Hint: The weights within two layers are automatically being initialised via `torch`; however, you may change it by ```L_1.weight.data = ...``` and ```L_1.bias.data = ...```)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25514617",
   "metadata": {
    "id": "25514617"
   },
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75aa45bf",
   "metadata": {
    "id": "75aa45bf"
   },
   "outputs": [],
   "source": [
    "L_1 = nn.Linear(in_features=2, out_features=1, bias=True) #Bias = True adds the bias term for input - layer 1 weights\n",
    "L_1.weight.data = torch.from_numpy(np.array([[0.3, -0.1]], dtype=np.float64)).float() #Give the transformation weights\n",
    "L_1.bias.data = torch.from_numpy(np.array([1], dtype=np.float64)).float() #Give the bias weight\n",
    "F_1 = lambda x: func.sigmoid(L_1.forward(x)) #F_2 applies the ReLU activation on the L_2 evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce44074",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ce44074",
    "outputId": "566aaf5e-8e20-4eda-e001-e26640a4bece"
   },
   "outputs": [],
   "source": [
    "print('Transformation weights', L_1.weight, '\\n', 'bias weight:', L_1.bias) #Weights fixed as specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9ac95e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0b9ac95e",
    "outputId": "0f6493d6-cf9f-40a1-d689-c7a19564a8a4"
   },
   "outputs": [],
   "source": [
    "x = torch.from_numpy(np.array([1,-2], dtype=np.float64)).float() #Input\n",
    "y = F_1(x)\n",
    "prob_nn = y.item() #Extract the estimated probability\n",
    "round(prob_nn,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef9edca-d1a6-4587-b012-162c8cd3cad3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
