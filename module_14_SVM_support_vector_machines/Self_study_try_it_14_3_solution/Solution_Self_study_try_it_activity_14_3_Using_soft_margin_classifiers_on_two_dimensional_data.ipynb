{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "HXsZF3VsyRV2",
   "metadata": {
    "id": "HXsZF3VsyRV2"
   },
   "source": [
    "# Self-study try-it activity 14.3: Using soft-margin classifiers on two-dimensional data\n",
    "\n",
    "A soft-margin classifier is a type of SVM that allows some misclassifications or margin violations to achieve a better balance between fitting the training data and generalising to unseen data. Unlike a hard-margin SVM, which requires that all data points be perfectly separated by the decision boundary (only suitable for linearly separable data), a soft-margin SVM introduces slack variables to permit some data points to fall within the margin or even on the wrong side of the hyperplane.\n",
    "\n",
    "In this notebook, you will\n",
    "- Train two soft-margin classifiers\n",
    "- Derive the confusion matrix\n",
    "- Visualise the decision boundary\n",
    "- Compare the number of support vectors in each of the two soft-margin classifiers and estimate the classifier with more support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6c78d6",
   "metadata": {
    "id": "cd6c78d6"
   },
   "outputs": [],
   "source": [
    "#Import the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "import math\n",
    "import warnings\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "# Use the following to plot SVMs\n",
    "from mlxtend.plotting import plot_decision_regions # On terminal, install: pip.install mlxtend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e75a5e",
   "metadata": {
    "id": "d9e75a5e"
   },
   "source": [
    "#### Task 1: SVM-based classifier on a non-separable data set\n",
    "**Load the two-dimensional data `Case2linear/X.npy` that has 20 rows and two columns, and the corresponding target `Case2linear/y.np`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97393fb",
   "metadata": {
    "id": "c97393fb"
   },
   "outputs": [],
   "source": [
    "n = 20 # n points in each group\n",
    "X = np.load('data/Case2linear/X.npy')\n",
    "y = np.load('data/Case2linear/y.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3f1621",
   "metadata": {
    "id": "1f3f1621"
   },
   "source": [
    "**Plot the two-dimensional points and colour them so that points with the same class have the same colour.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff791d63",
   "metadata": {
    "id": "ff791d63",
    "outputId": "8336aef2-4c81-4bc7-81c9-3c67cf011186"
   },
   "outputs": [],
   "source": [
    "color = np.concatenate((np.repeat(\"red\", n), np.repeat(\"blue\",n)), axis=0) #y is split half-half\n",
    "plt.scatter(X[:,0], X[:,1], c = color, alpha= .5)\n",
    "plt.xlabel(\"$x_1$\", size = 15)\n",
    "plt.ylabel(\"$x_2$\", size = 15)\n",
    "plt.title(\"Two classes that are linearly separable\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebfc29f",
   "metadata": {
    "id": "0ebfc29f"
   },
   "source": [
    "## Question 1:\n",
    "\n",
    "**Why might a linear hard-margin Support Vector Classifier (SVC) not be suitable for this data set?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e056ca97",
   "metadata": {
    "id": "e056ca97"
   },
   "source": [
    "- Answer: The data set is not linearly separable, hence there is no perfect classifier. Thus, the hard-margin SVC will fail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f70821a",
   "metadata": {
    "id": "9f70821a"
   },
   "source": [
    "**Train a soft-margin linear SVC with associated cost $=1$, derive the confusion matrix, and visualise the decision boundary.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b458e0",
   "metadata": {
    "id": "f1b458e0"
   },
   "outputs": [],
   "source": [
    "\n",
    "clf_c1 = svm.SVC(kernel = 'linear', C = 1)\n",
    "clf_c1.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9879b9b7",
   "metadata": {
    "id": "9879b9b7",
    "outputId": "ff1b4748-e0d4-46a1-9a65-a9c133d67807"
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y, clf_c1.predict(X))\n",
    "cmd = ConfusionMatrixDisplay(cm, display_labels=np.unique(y))\n",
    "cmd.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf2caa2",
   "metadata": {
    "id": "ddf2caa2",
    "outputId": "5336f77a-7e75-4c7c-b541-1422732a4ee4"
   },
   "outputs": [],
   "source": [
    "with warnings.catch_warnings(): #Otherwise, the package might throw an error that there is no \"boundary\" when everything is classified the same.\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    plot_decision_regions(X, y, clf=clf_c1, legend=2, colors = \"red,blue\", markers= \"o\");\n",
    "    ax=plt.gca();\n",
    "    plt.title(\"Linear decision boundary\")\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5181c96",
   "metadata": {
    "id": "a5181c96"
   },
   "source": [
    "**Train a soft-margin linear SVC with associated cost $=0.01$, derive the confusion matrix, and visualise the decision boundary.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524b3fde",
   "metadata": {
    "id": "524b3fde"
   },
   "outputs": [],
   "source": [
    "\n",
    "clf_c001 = svm.SVC(kernel = 'linear', C = 0.01)\n",
    "clf_c001.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edb591f",
   "metadata": {
    "id": "2edb591f",
    "outputId": "7de59560-110e-4464-a5af-b7454199004f"
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y, clf_c001.predict(X))\n",
    "cmd = ConfusionMatrixDisplay(cm, display_labels=np.unique(y))\n",
    "cmd.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64abb8a3",
   "metadata": {
    "id": "64abb8a3",
    "outputId": "3eb7b333-955e-4598-d5b5-0b072c10c8be",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with warnings.catch_warnings(): #Otherwise, the package might throw an error that there is no \"boundary\" when everything is classified the same.\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    plot_decision_regions(X, y, clf=clf_c001, legend=2, colors = \"red,blue\", markers= \"o\");\n",
    "    ax=plt.gca();\n",
    "    plt.title(\"Linear decision boundary\")\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bc56d3",
   "metadata": {
    "id": "d6bc56d3"
   },
   "source": [
    "**Compare the number of support vectors of the previous two points**\n",
    "\n",
    "Intuitively, support vectors are the points whose removal would cause the decision boundary to change. Recalling the bias–variance trade-off, you would expect a model with smaller variance to have more support vectors. Otherwise, if only a small number of points determine the boundary, removing just a few could change its shape significantly — making the model highly biased toward the training set.\n",
    "\n",
    "Moreover, in soft-margin classification, increasing the associated cost will penalise misclassification more, hence the SVC will be forced to be biased. You would therefore expect the number of support vectors to decrease with increasing cost. Use `clf.support_vectors_` attribute of `sklearn.svm` to compare the numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a05eac3",
   "metadata": {
    "id": "1a05eac3",
    "outputId": "72aac861-3d6d-4bd2-fa42-bc5a7f574285"
   },
   "outputs": [],
   "source": [
    "support_vectors_c1=np.shape(clf_c1.support_vectors_)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44ad6ea",
   "metadata": {
    "id": "c44ad6ea",
    "outputId": "a42d08bf-05bf-45bd-d9cf-57ebc5a6b4e1"
   },
   "outputs": [],
   "source": [
    "support_vectors_c2=np.shape(clf_c001.support_vectors_)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QO6rUdDJ3On0",
   "metadata": {
    "id": "QO6rUdDJ3On0"
   },
   "source": [
    "## Question 2:\n",
    "\n",
    "Based on the number of support vectors, which of the classifiers might be a more suitable option?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3z89G-mJ3Qho",
   "metadata": {
    "id": "3z89G-mJ3Qho"
   },
   "source": [
    "**Answer:**\n",
    "\n",
    "When comparing two soft classifiers (such as soft-margin SVMs), the number of support vectors can provide insight into the model's performance and generalisation ability, but it is not a direct measure of which classifier is \"better\" without additional context.\n",
    "\n",
    "If both classifiers have similar performance metrics (e.g., accuracy, precision, recall), then the classifier with fewer support vectors is generally preferred due to its simplicity and likely better generalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lZAvdwoJ3eFN",
   "metadata": {
    "id": "lZAvdwoJ3eFN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
