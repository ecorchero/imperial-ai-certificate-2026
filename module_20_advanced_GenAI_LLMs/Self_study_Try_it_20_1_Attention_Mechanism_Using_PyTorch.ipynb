{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9rpnS9FOi7wM"
   },
   "source": [
    "# Self-study Try-it 20.1: Build an attention mechanism using PyTorch\n",
    "\n",
    "Attention mechanisms enable models to focus on the most relevant parts of input data, improving understanding and prediction in tasks like translation and speech recognition. By assigning varying importance to tokens, attention enhances context-awareness. Variable-length sequences are managed through masking, which prevents padded or future tokens from influencing the output. Together, attention and masking form a core part of transformer architectures.\n",
    "\n",
    "\n",
    " The basic attention mechanism is the foundational building block used in transformer models, enabling them to weigh the importance of different parts of the input sequence dynamically and effectively capture contextual relationships. This mechanism, often extended with multi-head attention, is central to the power and success of modern transformer-based AI systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IrbIvo33arVt"
   },
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IdwIvFJE-a_e"
   },
   "source": [
    "In the code below, using PyTorch, a simple attention mechanism is defined that computes attention scores by taking the dot product of query and\n",
    "key tensors, scales these scores by the square root of the key dimension to maintain numerical stability, and applies a softmax\n",
    "to obtain attention weights. These weights are then used to compute a weighted sum of the value tensor, producing an output that highlights the most relevant information in the input sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VBjAronjkU23",
    "outputId": "4d1946da-29ac-4141-c1cb-93866ab832f8"
   },
   "outputs": [],
   "source": [
    "def simple_attention(query, key, value):\n",
    "    # Calculate raw attention scores by dot product of query and key transpose\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1))\n",
    "    # Scale scores by square root of key dimension\n",
    "    d_k = query.size(-1)\n",
    "    scaled_scores = scores / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "    # Apply softmax to get attention weights (probabilities)\n",
    "    attn_weights = F.softmax(scaled_scores, dim=-1)\n",
    "    # Compute weighted sum of values according to attention weights\n",
    "    output = torch.matmul(attn_weights, value)\n",
    "    return output, attn_weights\n",
    "\n",
    "# Example input: batch size 1, sequence length 3, embedding dimension 2\n",
    "query = torch.tensor([[[1.0, 0.0],\n",
    "                       [0.0, 1.0],\n",
    "                       [1.0, 1.0]]])  # shape (1, 3, 2)\n",
    "\n",
    "key = torch.tensor([[[1.0, 0.0],\n",
    "                     [0.0, 1.0],\n",
    "                     [1.0, 1.0]]])    # shape (1, 3, 2)\n",
    "\n",
    "value = torch.tensor([[[1.0, 10.0],\n",
    "                       [10.0, 1.0],\n",
    "                       [5.0, 5.0]]])   # shape (1, 3, 2)\n",
    "\n",
    "output, attn_weights = simple_attention(query, key, value)\n",
    "\n",
    "print(\"Attention output:\\n\", output)\n",
    "print(\"Attention weights:\\n\", attn_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbQHKNt6bK4Q"
   },
   "source": [
    "### Try-it: 1. Modify Input Tensors:\n",
    "Change the query or key vectors and see how the attention weights and output change. For example, make one query vector more similar to a specific key vector and observe the effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cD36Pl1RbL4M",
    "outputId": "aeb49c4b-6027-4981-dbe3-34c8b7390f45"
   },
   "outputs": [],
   "source": [
    "# Original inputs\n",
    "query = torch.tensor([[[1.0, 0.0],\n",
    "                       [0.0, 1.0],\n",
    "                       [1.0, 1.0]]])  # shape (1, 3, 2)\n",
    "\n",
    "key = torch.tensor([[[1.0, 0.0],\n",
    "                     [0.0, 1.0],\n",
    "                     [1.0, 1.0]]])    # shape (1, 3, 2)\n",
    "\n",
    "value = torch.tensor([[[1.0, 10.0],\n",
    "                       [10.0, 1.0],\n",
    "                       [5.0, 5.0]]])   # shape (1, 3, 2)\n",
    "\n",
    "output, attn_weights = simple_attention(query, key, value)\n",
    "print(\"Original attention weights:\\n\", attn_weights)\n",
    "print(\"Original attention output:\\n\", output)\n",
    "\n",
    "# Modified query: move first query vector closer to second key vector\n",
    "modified_query = torch.tensor([[[0.1, 0.9],\n",
    "                                [0.0, 1.0],\n",
    "                                [1.0, 1.0]]])\n",
    "\n",
    "output_mod, attn_weights_mod = simple_attention(modified_query, key, value)\n",
    "print(\"\\nModified attention weights:\\n\", attn_weights_mod)\n",
    "print(\"Modified attention output:\\n\", output_mod)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6XfSkyUbmv8"
   },
   "source": [
    "### Try-it: 2. Implement Masking\n",
    "\n",
    "Add a mask to the attention scores before softmax to simulate attention only on allowed tokens (e.g., for sequence padding).\n",
    "\n",
    "Here, if a mask value is provided, mask is applied with a value of 0 for allowed positions and -inf for masked positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DjzaVzx9bqNE",
    "outputId": "09ce4764-becf-4c56-df58-433ec95aee02"
   },
   "outputs": [],
   "source": [
    "def simple_attention_with_mask(query, key, value, mask=None):\n",
    "    # Calculate raw attention scores\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1))\n",
    "    d_k = query.size(-1)\n",
    "    scaled_scores = scores / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "\n",
    "    # Apply mask if provided (mask with 0 for allowed and -inf for masked positions)\n",
    "    if mask is not None:\n",
    "        scaled_scores = scaled_scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "    # Softmax to get attention weights\n",
    "    attn_weights = F.softmax(scaled_scores, dim=-1)\n",
    "    # Weighted sum of values\n",
    "    output = torch.matmul(attn_weights, value)\n",
    "    return output, attn_weights\n",
    "\n",
    "# Example inputs (batch_size=1, seq_len=3, embedding_dim=2)\n",
    "query = torch.tensor([[[1.0, 0.0],\n",
    "                       [0.0, 1.0],\n",
    "                       [1.0, 1.0]]])\n",
    "key = torch.tensor([[[1.0, 0.0],\n",
    "                     [0.0, 1.0],\n",
    "                     [1.0, 1.0]]])\n",
    "value = torch.tensor([[[1.0, 10.0],\n",
    "                       [10.0, 1.0],\n",
    "                       [5.0, 5.0]]])\n",
    "\n",
    "# Create a mask that allows attending only to the first two keys (mask shape must broadcast with scaled_scores)\n",
    "mask = torch.tensor([[[1, 1, 0],   # Allow attention on keys 0 and 1, mask out key 2\n",
    "                      [1, 1, 0],\n",
    "                      [1, 1, 0]]])  # shape (1, 3, 3)\n",
    "\n",
    "output, attn_weights = simple_attention_with_mask(query, key, value, mask=mask)\n",
    "\n",
    "print(\"Attention weights with mask:\\n\", attn_weights)\n",
    "print(\"Attention output with mask:\\n\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UORNRv_dcNNn"
   },
   "source": [
    "### Try-it: 3. Compare Different Scaling Factors\n",
    "Remove the scaling factor 1/Square root(dk)  and observe effects on the softmax outputs. Redefine the simple_attention with `scale=True` in addition to other attributes. Observe the difference with `scale=True` and `scale=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o8kUvjDBc5nH",
    "outputId": "37eb4eba-d7d2-4b88-bdc0-507eec66ac70"
   },
   "outputs": [],
   "source": [
    "def simple_attention(query, key, value, scale=True):\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1))\n",
    "    d_k = query.size(-1)\n",
    "\n",
    "    # Apply scaling if scale=True\n",
    "    if scale:\n",
    "        scores = scores / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "\n",
    "    attn_weights = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attn_weights, value)\n",
    "    return output, attn_weights\n",
    "\n",
    "# Example inputs (batch_size=1, seq_len=3, embedding_dim=2)\n",
    "query = torch.tensor([[[1.0, 0.0],\n",
    "                       [0.0, 1.0],\n",
    "                       [1.0, 1.0]]])\n",
    "key = torch.tensor([[[1.0, 0.0],\n",
    "                     [0.0, 1.0],\n",
    "                     [1.0, 1.0]]])\n",
    "value = torch.tensor([[[1.0, 10.0],\n",
    "                       [10.0, 1.0],\n",
    "                       [5.0, 5.0]]])\n",
    "\n",
    "\n",
    "# Attention with scaling factor 1/sqrt(d_k)\n",
    "output_scaled, attn_weights_scaled = simple_attention(query, key, value, scale=True)\n",
    "\n",
    "# Attention without scaling factor\n",
    "output_unscaled, attn_weights_unscaled = simple_attention(query, key, value, scale=False)\n",
    "\n",
    "print(\"Attention weights with scaling factor:\\n\", attn_weights_scaled)\n",
    "print(\"Attention output with scaling factor:\\n\", output_scaled)\n",
    "\n",
    "print(\"\\nAttention weights without scaling factor:\\n\", attn_weights_unscaled)\n",
    "print(\"Attention output without scaling factor:\\n\", output_unscaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tPKffYN_hR0v"
   },
   "source": [
    "### Try-it: 4. Add multi-head attention\n",
    "Here, we implement multi-head attention through the following steps:\n",
    "- Add number of heads and head dimensions\n",
    "- Add linear layers projecting input into queries, keys, and values for all heads\n",
    "- Split queries, keys, and values into multiple heads and rearrange dimensions\n",
    "- Perform scaled dot-product attention separately for each head\n",
    "- Concatenate the heads and transform back to original embedding dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rqOkGz_KhSMi",
    "outputId": "903ef15f-34bb-4bfb-e2e8-1c25fc463fe4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class SimpleMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # Linear layers to project input to queries, keys, and values\n",
    "        self.q_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        # Final linear layer to combine heads\n",
    "        self.out_linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embed_dim = x.size()\n",
    "\n",
    "        # Project inputs to queries, keys, values\n",
    "        Q = self.q_linear(x)  # (batch, seq_len, embed_dim)\n",
    "        K = self.k_linear(x)\n",
    "        V = self.v_linear(x)\n",
    "\n",
    "        # Split embeddings into multiple heads and transpose for attention calculation\n",
    "        # New shape: (batch, num_heads, seq_len, head_dim)\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_output = torch.matmul(attn_weights, V)  # (batch, num_heads, seq_len, head_dim)\n",
    "\n",
    "        # Concatenate heads and put through final linear layer\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim)\n",
    "        output = self.out_linear(attn_output)\n",
    "\n",
    "        return output, attn_weights\n",
    "\n",
    "# Example usage\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "embed_dim = 8\n",
    "num_heads = 2\n",
    "\n",
    "# Random input tensor representing a batch of sequences of embeddings\n",
    "x = torch.randn(batch_size, seq_len, embed_dim)\n",
    "\n",
    "model = SimpleMultiHeadAttention(embed_dim, num_heads)\n",
    "output, attn_weights = model(x)\n",
    "\n",
    "print(\"Output shape:\", output.shape)          # Expected: (batch_size, seq_len, embed_dim)\n",
    "print(\"Attention weights shape:\", attn_weights.shape)  # Expected: (batch_size, num_heads, seq_len, seq_len)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
