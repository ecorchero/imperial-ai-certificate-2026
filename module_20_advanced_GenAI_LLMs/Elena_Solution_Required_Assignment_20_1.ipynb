{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vP82YKb7Flnt"
   },
   "source": [
    "# Required Assignment 20.1-Refining a simple transformer in PyTorch\n",
    "\n",
    "In this assignment, you will explore the key steps involved in building a simple transformer model for text summarization. The Transformer architecture is one of the most powerful models in natural language processing (NLP), widely used in translation, summarization, and question answering.\n",
    "\n",
    "This exercise aims to train a state-of-the-art model and understand and implement the core building blocks of PyTorch. You will learn how raw text is converted into tokens, how positional information is added, how attention mechanisms work, and finally, how the model generates a summary through greedy decoding.\n",
    "\n",
    "In this assignment, you will complete the following steps:\n",
    "\n",
    "- Tokenization – Convert raw text into numerical indices using a predefined vocabulary.\n",
    "\n",
    "- Positional Encoding – Add position information to word embeddings so the model can understand word order.\n",
    "\n",
    "- Multi-Head Attention – Explore how the model learns relationships between words through the attention mechanism.\n",
    "\n",
    "- Greedy Decoding – Use the transformer model to generate a simple summary from the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "R4gbYvEPARAQ"
   },
   "outputs": [],
   "source": [
    "### import necessary libraries.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09UTJHPxJszP"
   },
   "source": [
    "The TokenEmbedding class defines a simple embedding layer for converting token indices into dense vector representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "1-YI6NnvAV-Q"
   },
   "outputs": [],
   "source": [
    "# --- Transformer components as previously defined ---\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3SbncdfYOnJk"
   },
   "source": [
    "The `PositionalEncoding` class implements the positional encoding mechanism used in Transformer models to inject information about the relative or absolute position of tokens in a sequence. Since Transformer architectures do not have recurrence or convolution, positional encodings provide a way to give the model a sense of token order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "02fbI1UvAZpL"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_size, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embed_size)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * (-math.log(10000.0) / embed_size))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8hTDagyQD5gc"
   },
   "source": [
    "### Question 1: Positional Encoding Check\n",
    "Create a tensor of random values with shape `(1, 5, embed_size)` and pass it through the `PositionalEncoding` class.  \n",
    "Print the shape of the output and confirm it matches the input shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "rz9-5nxFD92J",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cb20aa77ff1e98046a8b77e0e3688d64",
     "grade": false,
     "grade_id": "cell-506e05af908514a7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "19ac0a1c-1c40-4e6a-dd74-707a56527539"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 16])\n"
     ]
    }
   ],
   "source": [
    "### GRADED CELL\n",
    "def test_positional_encoding(embed_size):\n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "\n",
    "    # create input tensor: (batch_size=1, seq_len=5, embed_size)\n",
    "    x = torch.randn(1, 5, embed_size)\n",
    "\n",
    "    # instantiate positional encoding\n",
    "    pe = PositionalEncoding(embed_size, max_len=5)\n",
    "\n",
    "    # pass through positional encoding\n",
    "    out = pe(x)\n",
    "\n",
    "    # return the shape so the visible test can print it\n",
    "    return out.shape\n",
    "    \n",
    "# Visible test\n",
    "print(test_positional_encoding(16))  # Expected: torch.Size([1, 5, 16])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "HpQfb9WCEFxf",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b768a280d4b392e6a8de1530c01895e8",
     "grade": true,
     "grade_id": "cell-5f9937d7fcfc0b70",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "84d38193-bdba-4927-f167-7986643995e5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "w9JwYF_XAfPR"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "        assert self.head_dim * heads == embed_size, \"Embed size must be divisible by heads\"\n",
    "        self.query_linear = nn.Linear(embed_size, embed_size)\n",
    "        self.key_linear = nn.Linear(embed_size, embed_size)\n",
    "        self.value_linear = nn.Linear(embed_size, embed_size)\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        N = query.shape[0]\n",
    "        query_len, key_len, value_len = query.shape[1], key.shape[1], value.shape[1]\n",
    "        queries = self.query_linear(query).view(N, query_len, self.heads, self.head_dim)\n",
    "        keys = self.key_linear(key).view(N, key_len, self.heads, self.head_dim)\n",
    "        values = self.value_linear(value).view(N, value_len, self.heads, self.head_dim)\n",
    "        queries = queries.permute(0, 2, 1, 3)\n",
    "        keys = keys.permute(0, 2, 1, 3)\n",
    "        values = values.permute(0, 2, 1, 3)\n",
    "        energy = torch.matmul(queries, keys.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-inf\"))\n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "        out = torch.matmul(attention, values)\n",
    "        out = out.permute(0, 2, 1, 3).contiguous()\n",
    "        out = out.view(N, query_len, self.embed_size)\n",
    "        out = self.fc_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UDdki9UxEXj1"
   },
   "source": [
    "### Question 2: Exploration of Attention Weights\n",
    "Create a random input tensor of shape `(1, 5, embed_size)` and pass it through the `MultiHeadAttention` class.  \n",
    "Print the shape of the attention output and confirm it matches the input shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "1EbTm_C4Eed7",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "79a4d95c2b7dacc5302a8640c463311d",
     "grade": false,
     "grade_id": "cell-6d7c7fc0d6c6f930",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "dced8123-22f8-443c-e0a6-99445c02aa74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 16])\n"
     ]
    }
   ],
   "source": [
    "### GRADED CELL\n",
    "def test_attention(embed_size, heads):\n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "\n",
    "    # random input: (batch_size=1, seq_len=5, embed_size)\n",
    "    x = torch.randn(1, 5, embed_size)\n",
    "\n",
    "    # instantiate multi-head attention\n",
    "    attn = MultiHeadAttention(embed_size, heads)\n",
    "\n",
    "    # self-attention: query, key, and value are the same input\n",
    "    out = attn(x, x, x)\n",
    "\n",
    "    # return the shape to confirm it matches the input shape\n",
    "    return out.shape\n",
    "\n",
    "# Visible test\n",
    "print(test_attention(16, 4))  # Expected: torch.Size([1, 5, 16])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "WzEKJEDFEh7A",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "157cb4f566cc0118ed6c7080a4c85748",
     "grade": true,
     "grade_id": "cell-37986575932fa510",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "e661bbcd-9a52-4f72-b925-5216c9d316fc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1GL0w6MPeto"
   },
   "source": [
    "The `FeedForward` class implements a simple two-layer fully connected feedforward network, commonly used within transformer architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "VA_eoG2gAkVp"
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_size, forward_expansion):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(embed_size, forward_expansion * embed_size)\n",
    "        self.fc2 = nn.Linear(forward_expansion * embed_size, embed_size)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3gQM37GPuCC"
   },
   "source": [
    "The `EncoderLayer` class represents a single layer of a transformer encoder, combining multi-head self-attention and a feedforward network with residual connections and normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "4EfGdTRIAm0P"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(embed_size, heads)\n",
    "        self.ff = FeedForward(embed_size, forward_expansion)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x, mask):\n",
    "        attn_out = self.mha(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_out))\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + self.dropout(ff_out))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWzSrTY2P63I"
   },
   "source": [
    "The `DecoderLayer` class represents a single layer of a transformer decoder, integrating self-attention, encoder-decoder cross-attention, and a feedforward network with residual connections and normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "bFyy1grUAqWy"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(embed_size, heads)\n",
    "        self.cross_attn = MultiHeadAttention(embed_size, heads)\n",
    "        self.ff = FeedForward(embed_size, forward_expansion)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.norm3 = nn.LayerNorm(embed_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x, enc_out, src_mask, tgt_mask):\n",
    "        self_attn_out = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(self_attn_out))\n",
    "        cross_attn_out = self.cross_attn(x, enc_out, enc_out, src_mask)\n",
    "        x = self.norm2(x + self.dropout(cross_attn_out))\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm3(x + self.dropout(ff_out))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Zylb8QhQLJO"
   },
   "source": [
    "The `TransformerSummarizer` class implements a complete sequence-to-sequence transformer model for text summarization. It combines token embeddings, positional encodings, stacked encoder and decoder layers, and a final linear layer to generate output tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "HspvdhfNAt_c"
   },
   "outputs": [],
   "source": [
    "class TransformerSummarizer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size=256, num_layers=2, heads=4, forward_expansion=4, dropout=0.1, max_len=100):\n",
    "        super().__init__()\n",
    "        self.token_emb = TokenEmbedding(vocab_size, embed_size)\n",
    "        self.pos_enc = PositionalEncoding(embed_size, max_len)\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(embed_size, heads, forward_expansion, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(embed_size, heads, forward_expansion, dropout) for _ in range(num_layers)])\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        return (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "    def make_tgt_mask(self, tgt):\n",
    "        N, tgt_len = tgt.shape\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(2)\n",
    "        subsequent_mask = torch.tril(torch.ones((tgt_len, tgt_len), device=tgt.device)).bool()\n",
    "        tgt_mask = tgt_mask & subsequent_mask\n",
    "        return tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        tgt_mask = self.make_tgt_mask(tgt)\n",
    "\n",
    "        enc_out = self.token_emb(src)\n",
    "        enc_out = self.pos_enc(enc_out)\n",
    "        enc_out = self.dropout(enc_out)\n",
    "\n",
    "        for layer in self.encoder_layers:\n",
    "            enc_out = layer(enc_out, src_mask)\n",
    "\n",
    "        dec_out = self.token_emb(tgt)\n",
    "        dec_out = self.pos_enc(dec_out)\n",
    "        dec_out = self.dropout(dec_out)\n",
    "\n",
    "        for layer in self.decoder_layers:\n",
    "            dec_out = layer(dec_out, enc_out, src_mask, tgt_mask)\n",
    "\n",
    "        final_out = self.fc_out(dec_out)\n",
    "        return final_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "638pg5FZQaAl"
   },
   "source": [
    "This setup is commonly used in sequence-to-sequence models to convert text into numerical representations that can be fed into neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "1ezMGzhkA8Iw"
   },
   "outputs": [],
   "source": [
    "# -- Simple Tokenizer and Vocabulary --\n",
    "# Tokens: [PAD]=0, [SOS]=1, [EOS]=2, and some words for demo\n",
    "word2idx = {\n",
    "    '[PAD]': 0, '[SOS]': 1, '[EOS]': 2,\n",
    "    'the': 3, 'cat': 4, 'sat': 5, 'on': 6, 'mat': 7,\n",
    "    'a': 8, 'dog': 9, 'is': 10, 'here': 11,\n",
    "}\n",
    "idx2word = {v:k for k,v in word2idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dU5EyQUIA_SJ"
   },
   "source": [
    "### Question 3: Tokenization Practice\n",
    "Write a function `my_tokenize` that converts a given sentence into a list of indices using the provided `word2idx` dictionary.  \n",
    "Test it on the sentence: `\"The cat sat on the mat\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "id": "7VrIO0A2Bkfg",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "21eebf7d079726552c4a9f339ad39bdb",
     "grade": false,
     "grade_id": "cell-62b7edfcdb7dba0b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "###GRADED CELL\n",
    "def my_tokenize(text):\n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "\n",
    "    tokens = []\n",
    "    for word in text.split():\n",
    "        tokens.append(word2idx.get(word.lower(), word2idx['[PAD]']))  # default to PAD if not found\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "2L2KnPsTDZaR",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "db16d771e473ca6268e7d8aa00942ef1",
     "grade": true,
     "grade_id": "cell-17d4a95cce111924",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "dbe9ba2f-99ad-4f2a-a9ca-0481173e483b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXl86NA1Qyst"
   },
   "source": [
    "This workflow covers text preprocessing and generating summaries using a Transformer model:\n",
    "\n",
    "- Tokenization: Converts sentences into sequences of integer token IDs using a predefined vocabulary. Unknown words are mapped to a default token.\n",
    "\n",
    "- Detokenization: Converts sequences of token IDs back into readable text, stopping at the end-of-sequence token.\n",
    "\n",
    "- Greedy Decoding: Generates summaries by iteratively selecting the most probable next token until the end-of-sequence token is reached or a maximum length is met.\n",
    "\n",
    "This approach allows models to transform raw text into numerical representations, process it with neural networks, and produce human-readable summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "X22pVlq-LGM7"
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return [word2idx.get(word, 0) for word in text.lower().split()]\n",
    "\n",
    "def detokenize(indices):\n",
    "    words = []\n",
    "    for idx in indices:\n",
    "        if idx == word2idx['[EOS]']:\n",
    "            break\n",
    "        words.append(idx2word.get(idx, '[UNK]'))\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Greedy decoding for summary generation\n",
    "def generate_summary(model, src_sentence, max_len=10, device='cpu'):\n",
    "    model.eval()\n",
    "    src_tokens = torch.tensor([tokenize(src_sentence)], dtype=torch.long, device=device)\n",
    "    # Assume [PAD]=0, no padding needed here for single sentence\n",
    "    tgt_tokens = torch.tensor([[word2idx['[SOS]']]], dtype=torch.long, device=device)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            output = model(src_tokens, tgt_tokens)\n",
    "        next_token_logits = output[:, -1, :]\n",
    "        next_token = next_token_logits.argmax(dim=-1).unsqueeze(1)\n",
    "        tgt_tokens = torch.cat((tgt_tokens, next_token), dim=1)\n",
    "        #Stop if EOS predicted, otherwise decoding ends at max_len.\n",
    "        if next_token.item() == word2idx['[EOS]']:\n",
    "            break\n",
    "    summary = tgt_tokens[0,1:].cpu().tolist()  # remove [SOS]\n",
    "    return detokenize(summary)\n",
    "\n",
    "# Initialize model with small vocab for demo\n",
    "vocab_size = len(word2idx)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = TransformerSummarizer(vocab_size, embed_size=64, num_layers=2, heads=4)\n",
    "model = model.to(device)\n",
    "\n",
    "# For demo, load dummy trained weights or keep random (won't generate meaningful summary)\n",
    "# Normally you would train the model here on summarization dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zLWc_9U_E-dJ"
   },
   "source": [
    "### Question 4: Greedy Decoding Demonstration\n",
    "Use the provided `generate_summary` function to generate a summary for the input sentence:  \n",
    "\n",
    "`\"My name is Anushka. I am working on a project in AI.\"`  \n",
    "\n",
    "Return the generated summary as a string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "WxoYNK17FKyL",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "94c8e74c19c56ba4d1558d04dd739ca3",
     "grade": false,
     "grade_id": "cell-969d266eedeb38a6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "8eb75570-bb70-4221-dcc5-c7082bddb450"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on\n"
     ]
    }
   ],
   "source": [
    "### GRADED CELL\n",
    "def run_summary():\n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "\n",
    "    input_sentence = \"My name is Anushka. I am working on a project in AI.\"\n",
    "    summary = generate_summary(model, input_sentence, max_len=5, device=device)\n",
    "    return summary\n",
    "\n",
    "# Visible test\n",
    "print(run_summary())  # Output will be random (model is untrained), but should be a string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "HGW32baFFNyM",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d097532978b54b4fa0ff9ece92ecaf44",
     "grade": true,
     "grade_id": "cell-9659126c17d28b4e",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "3239589e-f534-4d51-f3ad-cef0b1d55409"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E4AfflvQE6ue",
    "outputId": "d3c75e82-f695-4641-fe5f-741c299a6748"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: My name is Anushka. I am working on a project in AI. I love working on this project and am happy to see the wonders happening because of AI. AI helps solve real-world problems and makes processes faster and smarter. Through this project, I am learning how machines can analyze data and make predictions. It excites me to see how AI can be applied in healthcare, education, and business. I enjoy experimenting with different models and improving their accuracy. This journey motivates me to explore deeper into the field of AI.\n",
      "Number of words in input: 91\n",
      "Generated summary: on\n",
      "Number of words in summary: 1\n"
     ]
    }
   ],
   "source": [
    "# Example input sentence\n",
    "input_sentence = \"My name is Anushka. I am working on a project in AI. I love working on this project and am happy to see the wonders happening because of AI. AI helps solve real-world problems and makes processes faster and smarter. Through this project, I am learning how machines can analyze data and make predictions. It excites me to see how AI can be applied in healthcare, education, and business. I enjoy experimenting with different models and improving their accuracy. This journey motivates me to explore deeper into the field of AI.\"\n",
    "summary = generate_summary(model, input_sentence, max_len=5, device=device)\n",
    "input_word_count = len(input_sentence.split())\n",
    "summary_word_count = len(summary.split())\n",
    "\n",
    "\n",
    "print(\"Input:\", input_sentence)\n",
    "print(\"Number of words in input:\", input_word_count)\n",
    "print(\"Generated summary:\", summary)\n",
    "print(\"Number of words in summary:\", summary_word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NgOYof_sQ7J1"
   },
   "source": [
    "This workflow demonstrates how to perform abstractive text summarization using the pretrained T5 (Text-to-Text Transfer Transformer) model from Hugging Face. The T5 model treats every NLP task as a text-to-text problem, making it highly flexible for tasks like summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2wjaC5OUMC5H",
    "outputId": "498fec6e-16dc-455e-afa3-c03c0d7f5a5a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7ec8d798e93410680b0d26597c42d5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a0d0971b8c44520a6eeeca76d71680f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff86c6f951dc44ec940f18510e841bb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ImportError",
     "evalue": "\n requires the protobuf library but it was not found in your environment. Check out the instructions on the\ninstallation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2315\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2314\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2315\u001b[39m     tokenizer = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2316\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/models/t5/tokenization_t5.py:184\u001b[39m, in \u001b[36mT5Tokenizer.__init__\u001b[39m\u001b[34m(self, vocab_file, eos_token, unk_token, pad_token, extra_ids, additional_special_tokens, sp_model_kwargs, legacy, add_prefix_space, **kwargs)\u001b[39m\n\u001b[32m    183\u001b[39m \u001b[38;5;28mself\u001b[39m.legacy = legacy\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m \u001b[38;5;28mself\u001b[39m.sp_model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_spm_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrom_slow\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[38;5;28mself\u001b[39m.vocab_file = vocab_file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/models/t5/tokenization_t5.py:210\u001b[39m, in \u001b[36mT5Tokenizer.get_spm_processor\u001b[39m\u001b[34m(self, from_slow)\u001b[39m\n\u001b[32m    209\u001b[39m sp_model = f.read()\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m model_pb2 = \u001b[43mimport_protobuf\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mThe new behaviour of \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m (with `self.legacy = False`)\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    211\u001b[39m model = model_pb2.ModelProto.FromString(sp_model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:37\u001b[39m, in \u001b[36mimport_protobuf\u001b[39m\u001b[34m(error_message)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available():\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentencepiece\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sentencepiece_model_pb2\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m sentencepiece_model_pb2\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/sentencepiece/sentencepiece_model_pb2.py:5\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[33;03m\"\"\"Generated protocol buffer code.\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotobuf\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m builder \u001b[38;5;28;01mas\u001b[39;00m _builder\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotobuf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m descriptor \u001b[38;5;28;01mas\u001b[39;00m _descriptor\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Load pretrained T5 tokenizer and model\u001b[39;00m\n\u001b[32m      7\u001b[39m model_name = \u001b[33m\"\u001b[39m\u001b[33mt5-small\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m tokenizer = \u001b[43mT5Tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlegacy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m model = T5ForConditionalGeneration.from_pretrained(model_name)\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msummarize\u001b[39m(text):\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# Prepend the task prefix for T5\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2069\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2066\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2067\u001b[39m         logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2069\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2072\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2073\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2074\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2076\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2077\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2078\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2079\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2080\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2081\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2316\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2314\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   2315\u001b[39m     tokenizer = \u001b[38;5;28mcls\u001b[39m(*init_inputs, **init_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m2316\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[43mimport_protobuf_decode_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   2317\u001b[39m     logger.info(\n\u001b[32m   2318\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2319\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2320\u001b[39m     )\n\u001b[32m   2321\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:87\u001b[39m, in \u001b[36mimport_protobuf_decode_error\u001b[39m\u001b[34m(error_message)\u001b[39m\n\u001b[32m     85\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DecodeError\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(PROTOBUF_IMPORT_ERROR.format(error_message))\n",
      "\u001b[31mImportError\u001b[39m: \n requires the protobuf library but it was not found in your environment. Check out the instructions on the\ninstallation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries if not already installed\n",
    "# !pip install transformers torch\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load pretrained T5 tokenizer and model\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name, legacy=False)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "def summarize(text):\n",
    "    # Prepend the task prefix for T5\n",
    "    input_text = \"summarize: \" + text\n",
    "    # Tokenize input text and generate input IDs\n",
    "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    # Generate summary ids using beam search for better quality\n",
    "    summary_ids = model.generate(\n",
    "        inputs,\n",
    "        max_length=150,\n",
    "        min_length=40,\n",
    "        length_penalty=2.0,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    # Decode the generated summary ids to string\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# Example input text\n",
    "input_text = (\n",
    "    \"My name is Anushka. I am working on a project in AI. I love working on this project and am happy to see the wonders happening because of AI. AI helps solve real-world problems and makes processes faster and smarter. Through this project, I am learning how machines can analyze data and make predictions. It excites me to see how AI can be applied in healthcare, education, and business. I enjoy experimenting with different models and improving their accuracy. This journey motivates me to explore deeper into the field of AI. I am also learning how important data quality is for building reliable AI systems. Collaborating with my peers on this project gives me new ideas and perspectives. I hope to apply the knowledge I gain here to create solutions that benefit society. AI has a bright future, and being part of this field makes me feel inspired. My long-term goal is to keep researching and contributing to meaningful innovations in AI.\"\n",
    ")\n",
    "\n",
    "summary = summarize(input_text)\n",
    "# Count words in input and summary\n",
    "input_word_count = len(input_text.split())\n",
    "summary_word_count = len(summary.split())\n",
    "\n",
    "print(\"Input text:\", input_text)\n",
    "print(\"Number of words in input:\", input_word_count)\n",
    "print(\"\\nGenerated summary:\", summary)\n",
    "print(\"Number of words in summary:\", summary_word_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q6rrzBIpRVXO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
