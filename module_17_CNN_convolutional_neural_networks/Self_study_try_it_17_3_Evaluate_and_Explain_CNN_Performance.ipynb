{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CE64ry5_CMS6"
   },
   "source": [
    "# Self-study try-it 17.3: Evaluate and Explain CNN Performance\n",
    "\n",
    "In this activity you will evaluate the CNN Performance using confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VkIzJaTJdz81"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VFKwK9o2dz81"
   },
   "source": [
    "## Training an image classifier\n",
    "----------------------------\n",
    "\n",
    "In continuation with our previous Self-study Try-it activities 17.1, 17.2, we will first train the image classifier using the CIFAR10 dataset and then evaluate the CNN's performance using a confusion matrix.\n",
    "1. Load and normalise the CIFAR10 training and test data sets using\n",
    "   ``torchvision``.\n",
    "2. Define a convolutional neural network.\n",
    "3. Define a loss function.\n",
    "4. Train the network on the training data.\n",
    "5. Test the network on the test data.\n",
    "\n",
    "### Step 1: Loading and normalising CIFAR10\n",
    "\n",
    "\n",
    "Using ``torchvision``, it’s extremely easy to load CIFAR10.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gTJTxk-1dz83"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0a54M9xldz83"
   },
   "source": [
    "The output of torchvision data sets are PILImage images of range [0, 1].\n",
    "We transform them to Tensors of normalised range [-1, 1].\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uxqb8XFMdz84"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDd_z_QSdz85"
   },
   "source": [
    "Let us show some of the training images (just for fun!).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "id": "tA849bBpegTu",
    "outputId": "8630bdfc-184b-4998-b336-67f1a16eb80a"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter) # Changed dataiter.next() to next(dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRjppo8ddz85"
   },
   "source": [
    "### Step 2: Define a convolutional neural network\n",
    "\n",
    "\n",
    "Copy the neural network from the neural networks section before and modify it to\n",
    "take 3-channel images (instead of 1-channel images as it was defined).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "al1KP7aFdz85",
    "outputId": "35785c22-e436-4adf-ea93-f13e7dd56e8b"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super(Net, self).__init__()\n",
    "        if name:\n",
    "            self.name = name\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "        # compute the total number of parameters\n",
    "        total_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        print(self.name + ': total params:', total_params)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net(name='LetNet5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s26Th4I4dz85"
   },
   "source": [
    "### Step 3: Define a loss function and optimiser\n",
    "\n",
    "Let's use a Classification Cross-Entropy loss and SGD with momentum.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gyK4Hubtdz85"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6NquOrhdz85"
   },
   "source": [
    "### Step 4: Train the network\n",
    "\n",
    "\n",
    "This is when things start to get interesting.\n",
    "We simply have to loop over our data iterator and feed the inputs to the\n",
    "network and optimise. We will also use the `time` package to get the training time of the network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "4timUy7Adz85",
    "outputId": "1e7fc3f7-42bd-49e1-a26d-74efc745f9cb"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "end = time.time()\n",
    "print('training time ', end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqvYDpU6dz86"
   },
   "source": [
    "### Step 5: Test the network on the test data\n",
    "\n",
    "\n",
    "We have trained the network for 2 passes over the training data set, but we need to check if the network has learnt anything at all.\n",
    "\n",
    "We will check this by predicting the class label that the neural network\n",
    "outputs and then comparing it against the ground-truth. If the prediction is\n",
    "correct, we add the sample to the list of correct predictions.\n",
    "\n",
    "First step: Let us display an image from the test set to get familiar.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "id": "MHbAo3qffZPi",
    "outputId": "fd2e4cc9-66c9-44f0-bb18-62ee4325728e"
   },
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter) # Use next(dataiter) instead of dataiter.next()\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ao20e1xzdz86"
   },
   "source": [
    "Now let us see what the neural network thinks these examples are.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8vLanmkqdz86"
   },
   "outputs": [],
   "source": [
    "outputs = net(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hI3DuRMwdz86"
   },
   "source": [
    "The outputs are energies for the 10 classes.\n",
    "The higher the energy for a class, the more the network\n",
    "thinks that the image is of the particular class.\n",
    "So, let's get the index of the highest energy:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "OfJxF_Zhdz86",
    "outputId": "6b0e0037-8169-4b7d-c4e8-6b03973eb5e0"
   },
   "outputs": [],
   "source": [
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hFe4khCdz86"
   },
   "source": [
    "The results seem pretty good.\n",
    "\n",
    "Let us look at how the network performs on the whole data set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "PU7FMV5Gdz86",
    "outputId": "525d24e2-d4cc-45a2-9239-a59e8dcecc78"
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMNGp1Aidz86"
   },
   "source": [
    "That looks way better than chance, which is 10% accuracy (randomly picking\n",
    "a class out of 10 classes).\n",
    "It seems like the network learnt something!\n",
    "\n",
    "Now let's answer the following question: What are the classes that performed well and the classes that did\n",
    "not?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "MCdEAjP1dz86",
    "outputId": "8ca30c9c-7cba-4e6b-c763-6eaad607c5e7"
   },
   "outputs": [],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tteNahfC16cZ"
   },
   "source": [
    "### Step 6: Evaluate with Confusion Matrix and Per-Class Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 704
    },
    "id": "V71upFUL2Cf-",
    "outputId": "4f819570-339b-48ee-d134-9f2adcd40ecd"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate_model(net, dataloader, classes):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "    disp.plot(cmap='Blues', xticks_rotation='vertical')\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    # Per-class accuracy\n",
    "    cm_df = pd.DataFrame(cm, index=classes, columns=classes)\n",
    "    correct_per_class = cm_df.values.diagonal()\n",
    "    total_per_class = cm_df.sum(axis=1).values\n",
    "    accuracy_per_class = correct_per_class / total_per_class\n",
    "\n",
    "    print(\"\\nPer-class Accuracy:\")\n",
    "    for i, acc in enumerate(accuracy_per_class):\n",
    "        print(f\"{classes[i]:>10}: {acc*100:.2f}%\")\n",
    "\n",
    "# Run evaluation\n",
    "evaluate_model(net, testloader, classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pd2HHmNR2O8j"
   },
   "source": [
    "### Step 7: Visualize Misclassified Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 259
    },
    "id": "MErLMsQK2R5-",
    "outputId": "8d2a5fec-632e-4f30-a5e9-c4c1bf9e19b1"
   },
   "outputs": [],
   "source": [
    "def show_misclassified(net, dataloader, classes, num_images=5):\n",
    "    misclassified = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            for i in range(len(labels)):\n",
    "                if predicted[i] != labels[i]:\n",
    "                    misclassified.append((images[i], predicted[i], labels[i]))\n",
    "                if len(misclassified) >= num_images:\n",
    "                    break\n",
    "            if len(misclassified) >= num_images:\n",
    "                break\n",
    "\n",
    "    # Display\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(15, 3))\n",
    "    for idx, (img, pred, label) in enumerate(misclassified):\n",
    "        img = img / 2 + 0.5  # unnormalize\n",
    "        npimg = img.numpy()\n",
    "        axes[idx].imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "        axes[idx].set_title(f\"Pred: {classes[pred]}\\nTrue: {classes[label]}\")\n",
    "        axes[idx].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show misclassified samples\n",
    "show_misclassified(net, testloader, classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1zDSbZ62ZmB"
   },
   "source": [
    "###  Reflection Prompts\n",
    "\n",
    "- Which classes did the model struggle with most, and what might explain these errors?\n",
    "- How could you modify the network architecture or training strategy to improve performance?\n",
    "- What insights do the misclassified images offer about the limitations of your model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uk7Z59dpQGl9"
   },
   "source": [
    "### Question 1: Which classes did the model struggle with most, and what might explain these errors?\n",
    "\n",
    "The lowest per class accuracies are Cat, Deer, Bird with accuracies of 30.5%, 35.4%, 38.7% respectively.\n",
    "\n",
    "This can be because of **visual ambiguity**, because they often share similar textures, or shapes at low resolution(32x32) making them harder to distinguish.\n",
    "\n",
    "**Intra-class variability**: Classes like “cat” and “bird” have high diversity in poses, colors, and backgrounds, which the model may not generalize well across.\n",
    "\n",
    "**Limited depth**: The current CNN (LetNet5 variant) is relatively shallow and may not extract sufficiently abstract features for nuanced distinctions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sX51LVeUScbn"
   },
   "source": [
    "### Question 2: How could you modify the network architecture or training strategy to improve performance?\n",
    "\n",
    "Here are a few targeted improvements:\n",
    "\n",
    "**Architectural Enhancements:**\n",
    "\n",
    "- Add more convolutional layers: Deeper networks like ResNet or VGG can capture more complex patterns.\n",
    "\n",
    "- Use Batch Normalization: Helps stabilize and accelerate training.\n",
    "\n",
    "- Replace fully connected layers with Global Average Pooling: Reduces overfitting and improves spatial generalization.\n",
    "\n",
    "**Training Strategy Tweaks:**\n",
    "\n",
    "- Data Augmentation: Apply random crops, flips, rotations, and color jitter to improve robustness.\n",
    "\n",
    "- Train longer or with learning rate scheduling: Two epochs is minimal—try 20–50 with a decaying learning rate.\n",
    "\n",
    "- Use Adam optimizer: Often converges faster and more reliably than SGD with momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ceq9OCAOTHA6"
   },
   "source": [
    "### Question 3: What insights do the misclassified images offer about the limitations of your model?\n",
    "\n",
    "- **Semantic confusion**: The model confuses animals with similar shapes or contexts (e.g., dog vs. cat), indicating limited feature abstraction.\n",
    "\n",
    "- **Background bias**: CIFAR-10 images often contain cluttered backgrounds, which may mislead the model if it overfits to context rather than object.\n",
    "\n",
    "- **Resolution bottleneck:** At 32×32, fine-grained details are lost—this limits the model’s ability to distinguish subtle features like ears or wings."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
