{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHQNECtREzME"
   },
   "source": [
    "# Self-Study Try-it 17.2: CNN Modifications and Preprocessing\n",
    "\n",
    "This activity extends the existing LeNet-style CIFAR-10 baseline code by incorporating dropout, variations in filter size and padding, data augmentation, and a deeper network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EpcKT706CPQg"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lj8tcTvxH2XW"
   },
   "source": [
    "### Step 1: Add Dropout for Regularization\n",
    "Dropout helps prevent overfitting by randomly \"dropping\" units during training, forcing the model to learn more robust features.\n",
    "\n",
    "Modify your LeNet class to include dropout layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EFSLVczVCTiC"
   },
   "outputs": [],
   "source": [
    "class LeNetDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNetDropout, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, kernel_size=5, padding=2)\n",
    "        self.pool = nn.AvgPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.5)  # Dropout with 50% rate after fc1\n",
    "        # The input size for fc1 will be determined dynamically\n",
    "        self.fc1 = nn.Linear(16 * 8 * 8, 120) # Adjusted based on CIFAR-10 input size and pooling\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.sigmoid(self.conv1(x)))\n",
    "        x = self.pool(self.sigmoid(self.conv2(x)))\n",
    "        # print(x.shape)  # Uncomment to check the shape before flattening\n",
    "        x = x.view(-1, 16 * 8 * 8) # Adjusted based on CIFAR-10 input size and pooling\n",
    "        x = self.sigmoid(self.fc1(x))\n",
    "        x = self.dropout(x)  # Apply dropout here\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57vGEf0_ITB3"
   },
   "source": [
    "### Step 2: Experiment with Filter Size and Padding\n",
    "You can try different kernel sizes or add padding to control the size of feature maps after convolution.\n",
    "\n",
    "Example: Change the second convolution to a 3x3 kernel with a padding of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sRxAOLjtIhH1"
   },
   "outputs": [],
   "source": [
    "class LeNetDropout1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNetDropout1, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, kernel_size=5, padding=2)\n",
    "        self.pool = nn.AvgPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=3, padding=1)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.75)  # Dropout with 50% rate after fc1\n",
    "        # The input size for fc1 will be determined dynamically\n",
    "        self.fc1 = nn.Linear(16 * 8 * 8, 120) # Adjusted based on CIFAR-10 input size and pooling\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.sigmoid(self.conv1(x)))\n",
    "        x = self.pool(self.sigmoid(self.conv2(x)))\n",
    "        # print(x.shape)  # Uncomment to check the shape before flattening\n",
    "        x = x.view(-1, 16 * 8 * 8) # Adjusted based on CIFAR-10 input size and pooling\n",
    "        x = self.sigmoid(self.fc1(x))\n",
    "        x = self.dropout(x)  # Apply dropout here\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TBqH85NHJApW"
   },
   "source": [
    "### Step 3: Enhance Data Augmentation\n",
    "To help the model generalize better add, additional transforms to increase the variation within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BI7xxQwhCYJl"
   },
   "outputs": [],
   "source": [
    "# Data preprocessing and augmentation\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),  # Randomly crop with padding\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.247, 0.243, 0.261))\n",
    "])\n",
    "\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-KR1hfSJhsa"
   },
   "source": [
    "### Step 4: Create a Deeper CNN Variant\n",
    "Add more convolutional layers and increase the feature map depth. Below is an example of a deeper model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xEliLkePJmi5"
   },
   "outputs": [],
   "source": [
    "class DeepLeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepLeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)  # 3->16 channels\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # Using max pooling here\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 4 * 4, 256)        # Adjust for final size\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = self.pool(self.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 64 * 4 * 4)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.dropout(self.relu(self.fc2(x)))\n",
    "        # print(x.shape) # Add this line to check the shape before the final layer\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KKUAqxxlCbM7",
    "outputId": "c8fd2821-8c7a-42a7-9643-8d425f2fcd42"
   },
   "outputs": [],
   "source": [
    "# Load CIFAR-10 dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SSBln-hOCdwz"
   },
   "outputs": [],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = LeNetDropout1() # Create an instance of the model\n",
    "net = net.to(device) # Move the model instance to the device\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bwombwUoCgqK",
    "outputId": "cd091601-8328-458c-8c62-733d94f0d269"
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(10):  # 20 epochs\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in trainloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(trainloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s4hAeAs1psYJ",
    "outputId": "e53cfb25-6da4-4aba-c023-fa653af0aa0c"
   },
   "outputs": [],
   "source": [
    "# Evaluation on test set\n",
    "net.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in testloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy on CIFAR-10 test set: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Mj-3dLwqQKR"
   },
   "source": [
    "### Further Learning\n",
    "\n",
    "In this section, we'll try changing some of the features of the model and explore how these changes affect the model's performance.\n",
    "\n",
    "- Increase the `dropout` to 0.75. What is the impact on the accuracy?\n",
    "- What is the effect of increasing the padding from 2 to 3?\n",
    "- Increase the number of epochs to 25 and check the impact on loss and overall accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjFyVFlLvmo9"
   },
   "source": [
    "##Increase the `dropout` to 0.75. What is the impact on the accuracy?\n",
    "\n",
    "Dropout randomly disables a fraction of neurons during training to prevent overfitting. A rate of p=0.75 means 75% of the neurons are dropped during each forward pass in training.\n",
    "### Potential Benefits:\n",
    "**Stronger regularization**: Forces the network to learn more robust features.\n",
    "\n",
    "**Better generalization**: May reduce overfitting on small or noisy datasets.\n",
    "\n",
    "### Potential Drawbacks:\n",
    "**Underfitting risk**: With 75% of neurons dropped, the model may struggle to learn effectively, especially if the dataset is complex (like CIFAR-10).\n",
    "\n",
    "**Slower convergence**: Fewer active neurons means weaker gradient signals, which can slow learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6mTlsQbC_We"
   },
   "source": [
    "## What is the effect of increasing the padding from 2 to 3?\n",
    "\n",
    "Increasing padding from 2 to 3 adds more zero pixels around the input, which increases the output spatial dimensions.\n",
    "\n",
    "With kernel_size=5 and padding=2, the output size remains the same as input (e.g., 32×32).\n",
    "\n",
    "With padding=3, the output becomes larger:\n",
    "\n",
    "$$\n",
    "\\text{Output size} = \\left\\lfloor \\frac{32 + 2 \\times 3 - 5}{1} \\right\\rfloor + 1 = 33\n",
    "$$\n",
    "\n",
    "So the feature map becomes 33×33 instead of 32×32."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkS_KafME4PY"
   },
   "source": [
    "## Increase the number of epochs to 25 and check the impact on loss and overall accuracy.\n",
    "\n",
    "Will more epochs, there will a decrease in loss and an improvement in overall accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4sDAYyDvC2xJ"
   },
   "source": [
    "##"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
